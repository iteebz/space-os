This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
docs/
  architecture.md
  bridge.md
  chats.md
  events.md
  knowledge.md
  memory.md
  operations.md
  schema.md
  sessions.md
  spawn.md
space/
  apps/
    canon/
      __init__.py
      api.py
      cli.py
    chats/
      __init__.py
      api.py
      cli.py
    context/
      __init__.py
      api.py
      cli.py
    council/
      __init__.py
      api.py
      cli.py
    daemons/
      __init__.py
      api.py
      cli.py
    health/
      __init__.py
      api.py
      cli.py
    stats/
      tests/
        __init__.py
        test_stats_api.py
      __init__.py
      api.py
      cli.py
      models.py
    backup.py
    init.py
  core/
    models.py
    protocols.py
  lib/
    providers/
      __init__.py
      claude.py
      codex.py
      gemini.py
    store/
      __init__.py
      core.py
      health.py
      migrations.py
      registry.py
      sqlite.py
    constitution.py
    display.py
    errors.py
    format.py
    ids.py
    migrations.py
    output.py
    paths.py
    query.py
    sync.py
    text_utils.py
    uuid7.py
  os/
    bridge/
      api/
        __init__.py
        channels.py
        mentions.py
        messaging.py
        search.py
        stats.py
      cli/
        __init__.py
        archive.py
        channels.py
        create.py
        delete.py
        export.py
        format.py
        inbox.py
        pin.py
        recv.py
        rename.py
        send.py
        unpin.py
        wait.py
      ops/
        __init__.py
        channels.py
        messages.py
      __init__.py
      db.py
    db/
      migrations/
        001_foundation.sql
      __init__.py
    knowledge/
      api/
        __init__.py
        entries.py
        search.py
        stats.py
      cli/
        __init__.py
        entries.py
      __init__.py
      db.py
    memory/
      api/
        __init__.py
        entries.py
        search.py
        stats.py
      cli/
        __init__.py
        entries.py
        namespace.py
      ops/
        namespace.py
      __init__.py
      db.py
    spawn/
      api/
        __init__.py
        agents.py
        main.py
        sessions.py
        stats.py
        symlinks.py
        tasks.py
      cli/
        __init__.py
        agents.py
        clone.py
        dynamic.py
        merge.py
        models.py
        register.py
        rename.py
        sleep.py
        tasks.py
        update.py
      constitutions/
        crucible.md
        sentinel.md
        zealot.md
      __init__.py
      db.py
      defaults.py
      models.py
      spawn.py
    __init__.py
  __init__.py
  cli.py
tests/
  integration/
    events/
      __init__.py
    test_agent_coordination.py
    test_bridge_api.py
    test_cli_integration.py
    test_memory_integration.py
    test_space_db_cascades.py
    test_spawn_agents.py
    test_spawn_commands.py
    test_spawn_lifecycle.py
    test_spawn_tasks.py
  unit/
    apps/
      canon/
        __init__.py
        test_commands.py
      stats/
        __init__.py
        test_stats_aggregation.py
      __init__.py
    commands/
      test_backup.py
    lib/
      store/
        __init__.py
        test_health.py
        test_migrations.py
      test_constitution.py
      test_context.py
      test_council.py
      test_ids.py
      test_paths.py
      test_query.py
      test_sqlite.py
      test_store.py
      test_sync.py
    os/
      bridge/
        __init__.py
        test_channels.py
        test_mentions.py
        test_messaging.py
        test_stats.py
      knowledge/
        __init__.py
        test_entries.py
        test_stats.py
      memory/
        __init__.py
        test_cli_namespaces.py
        test_entries.py
        test_namespace.py
      spawn/
        __init__.py
        test_agents.py
        test_spawn_prompt.py
        test_tasks.py
      test_protocols.py
    test_health_api.py
  conftest.py
.gitignore
justfile
LICENSE
MANUAL.md
MCP_SPIKE.md
pyproject.toml
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(mkdir:*)",
      "Bash(just ci:*)",
      "Bash(python -m pytest tests/unit/lib/test_sqlite.py::test_register_migrations -xvs)",
      "Bash(python:*)",
      "Bash(find:*)",
      "Bash(ls:*)",
      "Bash(grep:*)",
      "Bash(poetry install)",
      "Bash(spawn:*)"
    ],
    "deny": []
  }
}
</file>

<file path="docs/bridge.md">
# Bridge Primitive

An asynchronous message bus for agent coordination. Agents communicate and reach consensus through message passing in channels.

## Key Characteristics

-   **Async Coordination:** Agents interact by sending and receiving messages, enabling asynchronous workflows.
-   **Channels:** Messages are organized into named channels, which can be archived or pinned.
-   **Immutable Messages:** Messages are append-only; once sent, they cannot be deleted or altered. This ensures a verifiable history of communication.
-   **Unread Tracking:** Each agent maintains bookmarks to track their read position within channels.
-   **Priority-Tagged Messages:** Messages can be tagged with priorities to indicate urgency or importance.

## CLI Usage

The `bridge` command allows agents to send messages, manage channels, and retrieve their inbox.

```bash
# Send a message to a specific channel
bridge send research "Proposal: stateless context assembly" --as zealot

# Receive messages from a channel (marks them as read)
bridge recv research --as harbinger

# View all unread messages across all channels for an agent
bridge inbox --as harbinger

# List all available channels
bridge channels

# Export the full transcript of a channel
bridge export research
```

## Storage

Bridge channels, messages, and bookmarks are stored in the unified `.space/space.db` SQLite database.
</file>

<file path="docs/knowledge.md">
# Knowledge Primitive

Shared discoveries and collective intelligence, indexed by contributor and domain. Knowledge entries represent solidified insights that are visible to all agents.

## Key Characteristics

-   **Shared Truth:** Visible to all agents within the workspace.
-   **Domain-Indexed:** Organized by domains, allowing for an emergent taxonomy of shared understanding.
-   **Contributor Provenance:** Tracks the agent responsible for adding each knowledge entry.
-   **Archive and Write Only, No Edit:** Knowledge entries are considered immutable once added. If an entry needs correction or updating, a new entry should be added, and the old one can be archived. This maintains a clear historical record of shared understanding.
-   **Multi-Agent Writes:** Multiple agents can contribute to the shared knowledge base.

## CLI Usage

The `knowledge` command facilitates adding, querying, and listing shared knowledge.

```bash
# Add a new knowledge entry to a specific domain
knowledge add --domain architecture --contributor zealot "Delimiter protocol eliminates guessing"

# Query knowledge entries within a domain
knowledge query --domain architecture "delimiter protocol"

# List all knowledge entries
knowledge list

# Export all knowledge entries (e.g., to JSON or a file)
knowledge export

# Archive a knowledge entry (soft delete)
knowledge archive <uuid>
```

## Storage

Knowledge entries are stored in the unified `.space/space.db` SQLite database.
</file>

<file path="docs/memory.md">
# Memory Primitive

Private working context for agents, topic-sharded and identity-scoped. Memory entries are designed to capture an agent's ongoing thoughts, observations, and tasks.

## Key Characteristics

-   **Identity-Scoped:** Each memory entry belongs to a specific agent.
-   **Topic-Sharded:** Entries are organized by topics, allowing for focused retrieval.
-   **Archive-Write Encouraged, Edit Allowed:** While new entries are preferred for maintaining an immutable history, existing entries can be edited to correct mistakes or refine information. Archiving is used for soft deletion.
-   **Supersession Chains:** Entries can be replaced by newer versions, creating a historical chain of evolution.
-   **Core Flag:** Important, architectural, or identity-defining entries can be marked as "core" for quick access.
-   **No Cross-Agent Visibility:** Memories are private to the agent that created them.

## CLI Usage

The `memory` command provides both general operations and convenient shortcuts for canonical namespaces.

### Namespace Shortcuts

Use dedicated commands for canonical memory namespaces: `journal`, `notes`, `tasks`, `beliefs`. These provide quick ways to add and list entries for common use cases.

```bash
# Add a journal entry
memory journal "Wound down session, next steps: review PR #123" --as zealot

# Add a quick note
memory notes "Observed high CPU usage during build process" --as harbinger

# Add a task to follow up on
memory tasks "Follow up with team on integration test failures" --as sentinel

# Add a core belief statement
memory beliefs "Prioritize clarity and simplicity in all code" --as scribe

# List journal entries for 'zealot'
memory journal --as zealot

# List notes for 'harbinger'
memory notes --as harbinger
```

### General Memory Commands

For other operations, custom topics, or managing entries by UUID, use the general `memory` commands.

```bash
# Add a memory entry with a custom topic
memory add --as zealot --topic research "Initial findings on neural network architecture"

# Edit an existing memory entry by its UUID
memory edit <uuid> "Updated message content after review"

# Archive a memory entry (soft delete)
memory archive <uuid>

# Mark an entry as core memory (or unmark)
memory core <uuid>
memory core <uuid> --unmark

# Replace an old entry with a new one, creating a supersession chain
memory replace <old-uuid> "New, refined content for the superseded entry"

# Search memory entries by keyword
memory search "neural network" --as zealot

# Inspect an entry and find related nodes
memory inspect <uuid> --as zealot
```

## Storage

Memory entries are stored in the unified `.space/space.db` SQLite database.
</file>

<file path="space/os/memory/cli/namespace.py">
from typing import Annotated

import typer

from space.os.memory.ops import namespace as ops_namespace


def create_namespace_cli(namespace: str, noun: str) -> typer.Typer:
    """Creates a Typer app for a memory namespace."""
    app = typer.Typer(
        name=namespace,
        help=f"""Manage {noun} entries.

Use `memory {namespace} "<message>" --as <agent>` to quickly add a {noun} entry.
Use `memory {namespace} --as <agent>` to list {noun} entries.""",
    )

    @app.callback(invoke_without_command=True)
    def callback(
        ctx: typer.Context,
        message: Annotated[
            str | None, typer.Argument(help=f"Message to add to the {noun}.")
        ] = None,
    ):
        ctx.ensure_object(dict)

        if message:
            ops_namespace.add_entry(ctx, namespace, message)
        elif ctx.invoked_subcommand is None:
            # Default to listing if no message and no subcommand
            ops_namespace.list_entries(ctx, namespace, False)  # Default to not showing all

    @app.command("list")
    def list_command(
        ctx: typer.Context,
        all: Annotated[bool, typer.Option("--all", help="Include archived entries.")] = False,
        json: Annotated[bool, typer.Option("--json", help="Output as JSON.")] = False,
        quiet: Annotated[bool, typer.Option("--quiet", help="Suppress output.")] = False,
    ):
        ctx.obj["all"] = all
        ctx.obj["json"] = json
        ctx.obj["quiet"] = quiet
        ops_namespace.list_entries(ctx, namespace, all)

    @app.command("add")
    def add_command(
        ctx: typer.Context,
        message: Annotated[str, typer.Argument(help=f"Message to add to the {noun}.")],
        json: Annotated[bool, typer.Option("--json", help="Output as JSON.")] = False,
        quiet: Annotated[bool, typer.Option("--quiet", help="Suppress output.")] = False,
    ):
        ctx.obj["json"] = json
        ctx.obj["quiet"] = quiet
        ops_namespace.add_entry(ctx, namespace, message)

    return app
</file>

<file path="space/os/memory/ops/namespace.py">
import typer

from space.lib import output
from space.lib.format import format_memory_entries  # Import format_memory_entries
from space.os.memory import api  # Import the api module directly


def resolve_agent_id(ctx: typer.Context) -> str:
    identity = ctx.obj.get("identity")
    if not identity:
        raise typer.BadParameter("Agent identity must be provided via --as option.")
    return identity


def add_entry(ctx: typer.Context, topic: str, message: str):
    agent_id = resolve_agent_id(ctx)
    entry = api.add_entry(agent_id=agent_id, topic=topic, message=message, source="manual")
    if ctx.obj["json"]:
        output.json_output(entry.model_dump_json())
    elif not ctx.obj["quiet"]:
        typer.echo(f"Added {topic} entry: {entry.uuid}")


def list_entries(ctx: typer.Context, topic: str, show_all: bool):
    agent_id = resolve_agent_id(ctx)
    entries = api.list_entries(agent_id=agent_id, topic=topic, show_all=show_all)
    if ctx.obj["json"]:
        output.json_output([entry.model_dump() for entry in entries])
    elif not ctx.obj["quiet"]:
        output.out_text(format_memory_entries(entries), ctx.obj)


def archive_entry(ctx: typer.Context, uuid: str, restore: bool):
    resolve_agent_id(ctx)
    entry = api.get_by_id(uuid)
    if not entry:
        raise typer.BadParameter(f"Entry not found: {uuid}")
    try:
        if restore:
            api.restore_entry(uuid)
            action = "restored"
        else:
            api.archive_entry(uuid)
            action = "archived"
        print(f"Debug: UUID in ops archive: {uuid}")  # Debug print
        output.out_text(f"{action} {uuid[-8:]}", ctx.obj)
    except ValueError as e:
        raise typer.BadParameter(str(e)) from e


def core_entry(ctx: typer.Context, uuid: str, unmark: bool):
    resolve_agent_id(ctx)
    entry = api.get_by_id(uuid)
    if not entry:
        raise typer.BadParameter(f"Entry not found: {uuid}")
    try:
        is_core = not unmark
        api.mark_core(uuid, core=is_core)
        print(f"Debug: UUID in ops core: {uuid}")  # Debug print
        output.out_text(f"{'★' if is_core else ''} {uuid[-8:]}", ctx.obj)
    except ValueError as e:
        raise typer.BadParameter(str(e)) from e


def replace_entry(ctx: typer.Context, old_id: str, message: str, note: str):
    agent_id = resolve_agent_id(ctx)

    old_entry = api.get_by_id(old_id)
    if not old_entry:
        raise typer.BadParameter(f"Not found: {old_id}")

    try:
        new_uuid = api.replace_entry([old_id], agent_id, old_entry.topic, message, note)
        print(f"Debug: UUID in ops replace: {old_id}")  # Debug print
        output.out_text(f"Replaced {old_id[-8:]} -> {new_uuid[-8:]}", ctx.obj)
    except ValueError as e:
        raise typer.BadParameter(str(e)) from e
</file>

<file path="space/__init__.py">
__version__ = "0.0.1"
</file>

<file path="tests/unit/os/memory/test_cli_namespaces.py">
import pytest
from typer.testing import CliRunner

from space.os.memory import api
from space.os.memory.cli import app

runner = CliRunner()


@pytest.fixture(autouse=True)
def mock_api(mocker):
    mocker.patch("space.os.memory.api.add_entry")

    mocker.patch("space.os.memory.api.list_entries", return_value=[])

    mocker.patch("space.os.memory.api.get_by_id", return_value=None)

    mocker.patch("space.os.memory.api.archive_entry")

    mocker.patch("space.os.memory.api.restore_entry")

    mocker.patch("space.os.memory.api.mark_core")

    mocker.patch("space.os.memory.api.replace_entry")

    mocker.patch("space.os.spawn.get_agent", return_value=mocker.Mock(agent_id="test-agent-id"))


def test_journal_add_entry():
    result = runner.invoke(app, ["journal", "Test message", "--as", "zealot"])
    assert result.exit_code == 0
    api.add_entry.assert_called_once_with(
        agent_id="test-agent-id", topic="journal", message="Test message", source="manual"
    )


def test_journal_list_entries():
    result = runner.invoke(app, ["journal", "list", "--as", "zealot"])
    assert result.exit_code == 0
    api.list_entries.assert_called_once_with(
        agent_id="test-agent-id", topic="journal", show_all=False
    )


def test_journal_list_entries_all():
    result = runner.invoke(app, ["journal", "list", "--as", "zealot", "--all"])
    assert result.exit_code == 0
    api.list_entries.assert_called_once_with(
        agent_id="test-agent-id", topic="journal", show_all=True
    )


def test_notes_add_entry():
    result = runner.invoke(app, ["notes", "Test note", "--as", "zealot"])
    assert result.exit_code == 0
    api.add_entry.assert_called_once_with(
        agent_id="test-agent-id", topic="notes", message="Test note", source="manual"
    )


def test_notes_list_entries():
    result = runner.invoke(app, ["notes", "list", "--as", "zealot"])
    assert result.exit_code == 0
    api.list_entries.assert_called_once_with(
        agent_id="test-agent-id", topic="notes", show_all=False
    )


def test_tasks_add_entry():
    result = runner.invoke(app, ["tasks", "Test task", "--as", "zealot"])
    assert result.exit_code == 0
    api.add_entry.assert_called_once_with(
        agent_id="test-agent-id", topic="tasks", message="Test task", source="manual"
    )


def test_tasks_list_entries():
    result = runner.invoke(app, ["tasks", "list", "--as", "zealot"])
    assert result.exit_code == 0
    api.list_entries.assert_called_once_with(
        agent_id="test-agent-id", topic="tasks", show_all=False
    )


def test_beliefs_add_entry():
    result = runner.invoke(app, ["beliefs", "Test belief", "--as", "zealot"])
    assert result.exit_code == 0
    api.add_entry.assert_called_once_with(
        agent_id="test-agent-id", topic="beliefs", message="Test belief", source="manual"
    )


def test_beliefs_list_entries():
    result = runner.invoke(app, ["beliefs", "list", "--as", "zealot"])
    assert result.exit_code == 0
    api.list_entries.assert_called_once_with(
        agent_id="test-agent-id", topic="beliefs", show_all=False
    )


def test_namespace_add_without_identity_fails():
    result = runner.invoke(app, ["journal", "Test message"])
    assert result.exit_code != 0
    assert "Agent identity must be provided via --as option." in result.stdout


def test_namespace_list_without_identity_fails():
    result = runner.invoke(app, ["journal", "list"])
    assert result.exit_code != 0
    assert "Agent identity must be provided via --as option." in result.stdout


# Test that archive, core, replace are not available as subcommands for namespaces
def test_journal_archive_not_available():
    result = runner.invoke(app, ["journal", "archive", "some-uuid", "--as", "zealot"])
    assert result.exit_code != 0
    assert "No such command 'archive'." in result.stdout


def test_journal_core_not_available():
    result = runner.invoke(app, ["journal", "core", "some-uuid", "--as", "zealot"])
    assert result.exit_code != 0
    assert "No such command 'core'." in result.stdout


def test_journal_replace_not_available():
    result = runner.invoke(
        app, ["journal", "replace", "some-uuid", "new message", "--as", "zealot"]
    )
    assert result.exit_code != 0
    assert "No such command 'replace'." in result.stdout
</file>

<file path="tests/unit/os/memory/test_namespace.py">
from unittest.mock import MagicMock, patch

import pytest
import typer

from space.os.memory.ops import namespace as ops_namespace


@pytest.fixture
def mock_memory_api():
    with patch("space.os.memory.ops.namespace.api") as mock_api:
        yield mock_api


@pytest.fixture
def mock_ctx():
    ctx = MagicMock(spec=typer.Context)
    ctx.obj = {"identity": "test_agent", "json": False, "quiet": False, "all": False}
    return ctx


@pytest.fixture(autouse=True)
def mock_typer_echo():
    with patch("typer.echo") as mock_echo:
        yield mock_echo


@pytest.fixture(autouse=True)
def mock_output_out_text():
    with patch("space.lib.output.out_text") as mock_out_text:
        yield mock_out_text


def test_add_entry(mock_memory_api, mock_ctx, mock_typer_echo, mock_output_out_text):
    mock_memory_api.add_entry.return_value = MagicMock(uuid="test-uuid")
    ops_namespace.add_entry(mock_ctx, "journal", "test message")
    mock_memory_api.add_entry.assert_called_once_with(
        agent_id="test_agent", topic="journal", message="test message", source="manual"
    )
    mock_typer_echo.assert_called_once_with("Added journal entry: test-uuid")


def test_list_entries(mock_memory_api, mock_ctx, mock_typer_echo, mock_output_out_text):
    mock_memory_api.list_entries.return_value = []
    ops_namespace.list_entries(mock_ctx, "journal", show_all=False)
    mock_memory_api.list_entries.assert_called_once_with(
        agent_id="test_agent", topic="journal", include_archived=False
    )
    # Assuming format_memory_entries returns an empty string for no entries
    mock_output_out_text.assert_called_once_with("", mock_ctx.obj)


def test_archive_entry(mock_memory_api, mock_ctx, mock_typer_echo, mock_output_out_text):
    mock_memory_api.get_by_id.return_value = MagicMock(
        agent_id="test_agent", topic="journal", uuid="some-long-test-uuid"
    )
    ops_namespace.archive_entry(mock_ctx, "some-long-test-uuid", restore=False)
    mock_memory_api.archive_entry.assert_called_once_with("some-long-test-uuid")
    mock_output_out_text.assert_called_once_with("archived test-uuid", mock_ctx.obj)


def test_restore_entry(mock_memory_api, mock_ctx, mock_typer_echo, mock_output_out_text):
    mock_memory_api.get_by_id.return_value = MagicMock(
        agent_id="test_agent", topic="journal", uuid="some-long-test-uuid"
    )
    ops_namespace.archive_entry(mock_ctx, "some-long-test-uuid", restore=True)
    mock_memory_api.restore_entry.assert_called_once_with("some-long-test-uuid")
    mock_output_out_text.assert_called_once_with("restored test-uuid", mock_ctx.obj)


def test_core_entry(mock_memory_api, mock_ctx, mock_typer_echo, mock_output_out_text):
    mock_memory_api.get_by_id.return_value = MagicMock(
        agent_id="test_agent", topic="journal", uuid="some-long-test-uuid"
    )
    ops_namespace.core_entry(mock_ctx, "some-long-test-uuid", unmark=False)
    mock_memory_api.mark_core.assert_called_once_with("some-long-test-uuid", core=True)
    mock_output_out_text.assert_called_once_with("★ test-uuid", mock_ctx.obj)


def test_uncore_entry(mock_memory_api, mock_ctx, mock_typer_echo, mock_output_out_text):
    mock_memory_api.get_by_id.return_value = MagicMock(
        agent_id="test_agent", topic="journal", uuid="some-long-test-uuid"
    )
    ops_namespace.core_entry(mock_ctx, "some-long-test-uuid", unmark=True)
    mock_memory_api.mark_core.assert_called_once_with("some-long-test-uuid", core=False)
    mock_output_out_text.assert_called_once_with(" test-uuid", mock_ctx.obj)


def test_replace_entry(mock_memory_api, mock_ctx, mock_typer_echo, mock_output_out_text):
    mock_memory_api.get_by_id.return_value = MagicMock(
        agent_id="test_agent", topic="journal", uuid="some-long-old-uuid"
    )
    mock_memory_api.replace_entry.return_value = "some-long-new-uuid"
    ops_namespace.replace_entry(mock_ctx, "some-long-old-uuid", "new message", "test note")
    mock_memory_api.replace_entry.assert_called_once_with(
        ["some-long-old-uuid"], "test_agent", "journal", "new message", "test note"
    )
    mock_output_out_text.assert_called_once_with("Replaced old-uuid -> new-uuid", mock_ctx.obj)


def test_missing_identity_raises_error(mock_memory_api, mock_typer_echo, mock_output_out_text):
    ctx = MagicMock(spec=typer.Context)
    ctx.obj = {"json": False, "quiet": False, "all": False}
    with pytest.raises(
        typer.BadParameter, match="Agent identity must be provided via --as option."
    ):
        ops_namespace.add_entry(ctx, "journal", "test message")
</file>

<file path="LICENSE">
Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and derivative works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control
      systems, and issue tracking systems that are managed by, or on behalf
      of, the Licensor for the purpose of discussing and improving the Work,
      but excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to use, reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright 2025 Tyson Chan

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
</file>

<file path="docs/chats.md">
# Chats Primitive

First-class data primitive for ingesting and querying chat histories from external LLM providers (Claude, Codex, Gemini).

## Design Philosophy

- **Ingest, don't convert** — store provider-native formats (JSONL/JSON as-is)
- **Normalized interface** — unified `discover()`, `sync()`, `search()` across all three
- **Reference-based** — track sessions by native provider IDs (sessionId, etc)
- **Manual sync for now** — `chats sync` on-demand; autosync is future work

## MVP Surface

```bash
chats sync              # discover + delta-pull all sessions into chats.db
chats stats             # breakdown: unique sessions per provider, message counts
```

## Architecture

**Data hierarchy:**
```
context    — unified search
  ↓
chats      — provider session logs (immutable audit trail)
  ↓
knowledge  — synthesized shared truth
  ↓
memory     — agent working memory
```

**Schema:**
- `sessions` table: cli, session_id, file_path, identity, task_id, discovered_at
- `syncs` table: cli, session_id, last_byte_offset, last_synced_at, is_complete
- Tracks offset-based sync per provider

## Provider Differences

All three store chats differently but all have stable session IDs:

| Aspect | Claude | Codex | Gemini |
|--------|--------|-------|--------|
| Format | JSONL | JSONL | JSON (per-file) |
| Session ID | UUID-4 | ULIDv7 | UUID-4 |
| Storage | `~/.claude/projects/{path}/{id}.jsonl` | `~/.codex/sessions/{YYYY}/{MM}/{DD}/...jsonl` | `~/.gemini/tmp/{hash}/chats/session-*.json` |
| Resume Support | ✅ `--resume [id]` | ✅ `resume --last` | ❌ None |
| Sync Strategy | Byte offset (JSONL) | Byte offset (JSONL) | `lastUpdated` timestamp (JSON) |
| Critical Limitation | 30-day auto-delete | None | No resume mechanism |

See `PROVIDER_CHAT_STORAGE.md` for full audit.

## Key Findings

1. **No provider offers session listing API** — all require file parsing
2. **Claude deletes after 30 days** — aggressive syncing needed to capture before expiry
3. **Gemini uses JSON (not JSONL)** — requires different offset tracking (timestamp-based)
4. **All three are discoverable without APIs** — session_id embedded in filenames/metadata
5. **Gemini has no resume** — design limitation; not blocking (can summarize + recontextualize)

## Future Ideas (Descoped)

- `chats clean` — prune low-value/empty chats
- `chats archive` — export to permanent storage before deletion
- `chats summarize <id>` — agent-generated summaries from raw transcripts
- Provider-specific commands (`chats claude`, `chats codex`, etc)
- Autosync via `space sync` workspace-level wrapper
- Singular vs plural naming debate (`chats` vs `chat`)

## Implementation Notes

- **Gemini offset tracking fix:** Use `lastUpdated` timestamp + `sessionId` dedup instead of byte offset
- **Sync state initialization:** Ensure `ensure_sync_state()` creates records on discovery (currently skips if missing)
- **CLI:** Single `chats sync` command handles both discover + delta-pull
- **Stats command:** Count unique sessions per provider + total message count

## Investigation & Fixes (2025-10-26)

**Full provider audit completed.** See `/space/PROVIDER_CHAT_STORAGE.md`, `/space/METADATA_COMPARISON.md`, `/space/GEMINI_TMP_ANALYSIS.md`.

### Storage Summary

**Total:** 1.5GB across three providers

| Provider | Sessions | Messages | Size | Format | Notes |
|----------|----------|----------|------|--------|-------|
| Claude | 20 dirs | ~300 | 414 MB | JSONL | Flat namespace per project |
| Codex | 692 files | ~1500 | 82 MB | JSONL | Date-partitioned; includes tool metadata |
| Gemini | 742 unique | 31,676 | 964 MB | JSON | 78% in one project (100MB+ message dumps) |

### Key Discoveries

1. **Gemini tmp/ is permanent** — goes back to July; no auto-cleanup
2. **Gemini's 100MB+ sessions are test cases** — keep for robust ingestion (don't delete)
3. **logs.json confusion** — stores per-message metadata (truncated), not session index
   - 10,957 log entries = 742 unique sessions (first messages only)
4. **Codex is metadata-rich** — only provider with token counts + rate limits
5. **No cleanup needed** — storage is manageable; bloat is from pasted file dumps (expected)

### Gemini Provider Fixes

**File:** `space/lib/providers/gemini.py`

**Problems fixed:**
- ❌ Was parsing logs.json (metadata-only); now parses actual chat files
- ❌ Session ID from filename stem; now from `chat_data.sessionId`
- ❌ Byte offset tracking (won't work for JSON); now uses message index
- ❌ No resilience for large files; now gracefully skips 100MB+ on discovery

**Improvements:**
- ✅ `discover_sessions()` returns actual chat metadata (start_time, last_updated, message_count, file_size)
- ✅ Dedupes logs.json entries by sessionId (one per message → one per session)
- ✅ Includes first_message preview from logs.json for quick context
- ✅ Handles MemoryError gracefully for 100MB+ files
- ✅ `parse_messages()` now tracks by message_index (not byte offset)
- ✅ Preserves Gemini-specific `thoughts` reasoning blocks

### Storage Distribution (Gemini)

```
692 total chat files across 19 active projects:
  - 4.0K:     63 files (empty or near-empty)
  - 4K-100K:  402 files (normal chats)
  - 100K-1M:  205 files (larger conversations)
  - 1M-27M:   20 files (big sessions)
  - 27M-227M: 2 files (test cases with 100MB dumps)

Largest files (keep for testing robust ingestion):
  - 217 MB: session-2025-09-24T11-43-1a1a9546.json (2 messages, 100MB+ each)
  - 214 MB: session-2025-09-21T03-58-8f496744.json (2 messages)
  - 107 MB: session-2025-09-24T11-39-149ecff1.json
  - 91 MB: session-2025-09-23T05-30-e70e500e.json
  - ...etc (8 total > 50MB)
```

## Testing Strategy

Once MVP is implemented:
1. Run `chats discover` on real provider data (test with Gemini 100MB+ files)
2. Run `chats sync` to populate chats.db with real messages
3. Validate offset tracking works (byte offset for Claude/Codex, message index for Gemini)
4. Spot-check chats.db for data integrity
5. Do EDA on message counts, temporal distribution, provider breakdown

No smoketest integration until discovery + sync tested on live data.
</file>

<file path="docs/sessions.md">
# sessions.db — Unified Message Index

**Motivation:** space-os conducts constitutional multiplicity across agent identities. Each CLI (Claude, Codex, Gemini) stores logs independently. Without unified indexing, decisions vanish between wakes.

**Solution:** Single SQLite table indexing all CLI messages (full text) with role, model, and identity tags.

---

## Schema

```
id        INTEGER PRIMARY KEY
cli       TEXT NOT NULL          # claude, codex, gemini
model     TEXT                   # model version
session_id TEXT NOT NULL         # CLI's native session UUID
timestamp TEXT NOT NULL          # ISO8601
identity  TEXT                   # agent identity (zealot, hailot, etc)
role      TEXT NOT NULL          # user or assistant
text      TEXT NOT NULL          # full message text
raw_hash  TEXT UNIQUE            # sha256(cli+session_id+timestamp+text)
```

**Constraints:**
- `UNIQUE(cli, session_id, timestamp)` — prevents duplicate syncs
- **Indexes:** identity, cli+session_id, timestamp

---

## Normalization

Three parsers convert CLI formats to canonical `SessionMsg`:

```python
@dataclass
class SessionMsg:
    role: str                    # user or assistant
    text: str                    # full message text
    timestamp: str               # ISO8601
    session_id: str
    model: str | None = None     # model version
```

- `norm_claude_jsonl(path)` — Claude JSONL
- `norm_codex_jsonl(path)` — Codex JSONL (extracts model from turn_context)
- `norm_gemini_json(path)` — Gemini JSON (maps "model"→"assistant")

All handle variable content formats (dict/list/string) transparently. Empty/malformed messages filtered during parsing.

---

## Sync

`sync(identity=None)` scans all three CLI directories, normalizes messages, and upserts to DB. Idempotent via raw_hash dedup. On wake, syncs automatically and tags untagged entries with agent identity.

Filtering during normalization dropped ~50% of raw log entries (empty, metadata-only, malformed).

---

## API

**Query functions:**
- `search(query, identity=None, limit=10)` — full-text search on text field
- `list_entries(identity=None, limit=20)` — recent entries
- `get_entry(entry_id)` — fetch single entry
- `get_surrounding_context(entry_id, context_size=5)` — entries from same session
- `sample(count=5, identity=None, cli=None)` — random sample

**CLI:**
```bash
sessions sync --identity zealot
sessions search "folio" --identity hailot --limit 10
sessions list --identity zealot --limit 20
sessions view 42284 --context 5
sessions sample --count 5 --cli codex
```

---

## Integration

Integrated into `wake --as <identity>` — auto-syncs and tags on wake.

Target: merge `search()` into `context --as <identity> "<query>"` for unified concept retrieval across canon + memory + sessions.

---

## Facts

- **Location:** `~/.space/sessions.db` (gitignored, regenerable)
- **Size:** ~43K messages across all CLIs (31K Claude, 8K Gemini, 4K Codex)
- **Tests:** 12/12 passing
- **Sync time:** <1s (on recent logs)
- **Memory:** Full text stored, no truncation
- **Dedup:** sha256(cli+session_id+timestamp+text) prevents re-indexing same message
</file>

<file path="space/apps/canon/api.py">
"""Canon API - structured access to ~/space/canon documents."""

from __future__ import annotations

from pathlib import Path

from space.lib import errors
from space.lib.paths import canon_path


def search(query: str, max_content_length: int = 500) -> list[dict]:
    """Search canon documents for query matches."""
    if not query:
        return []

    canon_root = canon_path()
    if not canon_root.exists():
        return []

    matches: list[dict] = []
    for md_file in canon_root.rglob("*.md"):
        _append_match_if_relevant(matches, md_file, canon_root, query, max_content_length)
    return matches


def _append_match_if_relevant(
    matches: list[dict],
    md_file: Path,
    canon_root: Path,
    query: str,
    max_content_length: int,
) -> None:
    """Append match dict to results if query exists in markdown file."""
    try:
        content = md_file.read_text()
    except Exception as exc:  # pragma: no cover - logged for diagnostics
        errors.log_error("canon", None, exc, "file read")
        return

    if query.lower() not in content.lower():
        return

    relative_path = md_file.relative_to(canon_root)
    truncated_content = content[:max_content_length] + (
        "..." if len(content) > max_content_length else ""
    )
    matches.append(
        {
            "source": "canon",
            "path": str(relative_path),
            "content": truncated_content,
            "reference": f"canon:{relative_path}",
        }
    )
</file>

<file path="space/apps/canon/cli.py">
"""Space canon lookup - navigate and read documents from ~/space/canon."""

from pathlib import Path
from typing import Annotated  # Added for typer.Argument

import typer

from space.lib.paths import canon_path

app = typer.Typer(invoke_without_command=True, no_args_is_help=False)


@app.command(name="")  # This makes it the default command when no subcommand is given
def canon_main_command(
    ctx: typer.Context,
    doc_path: Annotated[
        str | None,
        typer.Argument(None, help="Document path (e.g., INDEX.md or constitutions/zealot.md)"),
    ] = None,
):
    """Navigate and read canon documents from ~/space/canon."""
    if ctx.invoked_subcommand is not None:
        return

    canon_root = canon_path()
    if not canon_root.exists():
        typer.echo(f"Error: Canon directory not found at {canon_root}", err=True)
        raise typer.Exit(1)

    if not doc_path:
        _show_tree(canon_root)
        return

    _read_document(canon_root, doc_path)


def _show_tree(canon_root: Path) -> None:
    """Display tree of canon documents."""
    typer.echo("\nCanon structure. Navigate with: space canon <path>")
    typer.echo("Examples: space canon INDEX.md  |  space canon constitutions/zealot.md\n")
    _print_tree(canon_root, canon_root, prefix="")


def _print_tree(path: Path, root: Path, prefix: str, max_depth: int = 3, depth: int = 0) -> None:
    """Recursively print directory tree."""
    if depth >= max_depth:
        return

    items = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name.lower()))

    for i, item in enumerate(items):
        if item.name.startswith("."):
            continue

        is_last = i == len(items) - 1
        current_prefix = "└── " if is_last else "├── "
        typer.echo(f"{prefix}{current_prefix}{item.name}")

        if item.is_dir():
            next_prefix = prefix + ("    " if is_last else "│   ")
            _print_tree(item, root, next_prefix, max_depth, depth + 1)


def _read_document(canon_root: Path, doc_path: str) -> None:
    """Read full document from canon."""
    target = canon_root / doc_path
    if not target.exists():
        target = canon_root / f"{doc_path}.md"

    if not target.exists():
        typer.echo(f"Document not found: {doc_path}")
        available = list(canon_root.rglob("*.md"))
        if available:
            typer.echo("\nDid you mean one of these?")
            for f in sorted(available)[:5]:
                typer.echo(f"  {f.relative_to(canon_root)}")
        raise typer.Exit(1)

    try:
        content = target.read_text()
        typer.echo(content)
    except Exception as e:
        typer.echo(f"Error reading document: {e}", err=True)
        raise typer.Exit(1) from e
</file>

<file path="space/apps/context/cli.py">
"""Unified concept retrieval: evolution + current state."""

from typing import Annotated  # Added for typer.Argument and typer.Option

import typer

from space.apps.context import api
from space.lib import display, errors, output

errors.install_error_handler("context")

app = typer.Typer(invoke_without_command=True)


@app.command(name="")  # This makes it the default command when no subcommand is given
def context_main_command(
    ctx: typer.Context,
    query: Annotated[str | None, typer.Argument(None, help="Query to retrieve context for")] = None,
    all_agents: Annotated[
        bool, typer.Option(False, "--all", help="Cross-agent perspective")
    ] = False,
    help: Annotated[bool, typer.Option(False, "--help", "-h", help="Show help")] = False,
):
    """Unified context retrieval: trace evolution + current state."""
    # The common options (identity, json_output, quiet_output) are now handled by add_common_options
    # and are available in ctx.obj

    if help:
        typer.echo("context [query] --as <identity>: Retrieve concept evolution and current state.")
        ctx.exit()

    # This check is now simpler as common options are handled by the callback
    if (ctx.resilient_parsing or ctx.invoked_subcommand is not None) and not query:
        typer.echo("context [query] --as <identity>: Retrieve concept evolution and current state.")
        return

    # Retrieve identity from ctx.obj
    identity = ctx.obj.get("identity")
    json_output = ctx.obj.get("json")
    quiet_output = ctx.obj.get("quiet")

    timeline = api.collect_timeline(query, identity, all_agents)
    current_state = api.collect_current_state(query, identity, all_agents)

    if json_output:
        typer.echo(
            output.out_json(
                {
                    "evolution": timeline,
                    "state": current_state,
                }
            )
        )
        return

    if quiet_output:
        return

    display.display_context(timeline, current_state)

    if not timeline and not any(current_state.values()):
        output.out_text(f"No context found for '{query}'", ctx.obj)
</file>

<file path="space/apps/daemons/cli.py">
"""Daemon commands: spawn and manage autonomous swarms."""

from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Annotated  # Added for typer.Option

import typer

from . import api

app = typer.Typer(invoke_without_command=True)

DAEMON_TASKS = [
    ("upkeep", "Repository hygiene, memory compaction, artifact checksumming"),
    ("sync_chats", "Discover and sync chats from claude/codex/gemini providers"),
]


def _run_all_daemons(role: str = "zealot") -> None:
    """Run all configured daemons in parallel."""
    typer.echo("🔄 Space health heartbeat: running daemons in parallel\n")

    def spawn_daemon(daemon_type: str, description: str) -> tuple[str, str, str]:
        """Spawn a single daemon and return (daemon_type, task_id, status)."""
        try:
            task_id = api.create_daemon_task(daemon_type, role=role)
            return (daemon_type, task_id, "spawned")
        except Exception as e:
            return (daemon_type, "", f"error: {e}")

    results = []
    with ThreadPoolExecutor(max_workers=len(DAEMON_TASKS)) as executor:
        futures = {
            executor.submit(spawn_daemon, daemon_type, desc): (daemon_type, desc)
            for daemon_type, desc in DAEMON_TASKS
        }
        for future in as_completed(futures):
            daemon_type, description = futures[future]
            try:
                daemon_type_res, task_id, status = future.result()
                results.append((daemon_type_res, description, task_id, status))
            except Exception as e:
                results.append((daemon_type, description, "", f"error: {e}"))

    results.sort(key=lambda x: x[0])
    typer.echo(f"{'Daemon':<15} {'Status':<12} {'Task ID':<10} Description")
    typer.echo("-" * 80)
    for daemon_type, description, task_id, status in results:
        task_display = task_id[:8] if task_id else "-"
        status_icon = "✓" if status == "spawned" else "❌"
        typer.echo(f"{daemon_type:<15} {status_icon} {status:<10} {task_display:<10} {description}")


@app.callback(invoke_without_command=True)
def main(ctx: typer.Context) -> None:
    """Space health heartbeat: run all daemons in parallel, or invoke subcommands."""
    if ctx.invoked_subcommand is None:
        # Retrieve identity from ctx.obj
        role = ctx.obj.get("identity", "zealot")  # Default to zealot if not provided
        _run_all_daemons(role=role)


@app.command()
def upkeep(
    ctx: typer.Context,
    wait_for_completion: Annotated[
        bool, typer.Option(False, "--wait", "-w", help="Block until task completes")
    ] = False,
):
    """Spawn upkeep daemon: repository hygiene, memory compaction, artifact checksumming."""
    role = ctx.obj.get("identity", "zealot")
    try:
        task_id = api.create_daemon_task("upkeep", role=role)
        typer.echo(f"✓ Daemon spawned: {task_id[:8]}")

        if wait_for_completion:
            task = api.get_daemon_task(task_id)
            if task:
                typer.echo(f"  Status: {task.status}")
    except ValueError as e:
        typer.echo(f"❌ {e}", err=True)
        raise typer.Exit(1) from e


@app.command()
def status(
    all_tasks: Annotated[
        bool, typer.Option(False, "--all", "-a", help="Show all tasks including completed")
    ] = False,
):
    """Show daemon task status."""
    filter_status = None if all_tasks else "pending|running"
    daemon_tasks = api.list_daemon_tasks(status=filter_status)

    if not daemon_tasks:
        typer.echo("No active daemon tasks")
        return

    typer.echo(f"{'ID':<8} {'Role':<12} {'Status':<12} {'Input':<30}")
    typer.echo("-" * 70)

    for task in daemon_tasks:
        task_id = task.task_id[:8]
        role = (task.agent_id or "unknown")[:11]
        stat = task.status
        task_input = (task.input or "")[:29]
        typer.echo(f"{task_id:<8} {role:<12} {stat:<12} {task_input:<30}")
</file>

<file path="space/apps/health/cli.py">
import typer

from space.apps.health import api

app = typer.Typer()


@app.callback(invoke_without_command=True)
def callback(ctx: typer.Context):
    if ctx.invoked_subcommand is None:
        ctx.invoke(health)


@app.command()
def health():
    """Verify space-os lattice integrity."""
    issues, counts_by_db = api.run_all_checks()

    for db_name, tables_counts in counts_by_db.items():
        for tbl, cnt in tables_counts.items():
            typer.echo(f"✓ {db_name}::{tbl} ({cnt} rows)")

    if issues:
        for issue in issues:
            typer.echo(issue)
        raise typer.Exit(1)

    typer.echo("\n✓ Space infrastructure healthy")


def main() -> None:
    """Entry point for poetry script."""
    app()
</file>

<file path="space/lib/providers/__init__.py">
"""Unified provider implementations for Claude, Codex, Gemini.

Providers implement the Provider protocol (chat discovery, message parsing, spawning).
"""

from .claude import Claude
from .codex import Codex
from .gemini import Gemini

claude = Claude()
codex = Codex()
gemini = Gemini()

__all__ = ["claude", "codex", "gemini", "Claude", "Codex", "Gemini"]
</file>

<file path="space/lib/store/migrations.py">
"""Database schema migrations and initialization."""

import logging
import sqlite3
from collections.abc import Callable
from pathlib import Path

from space.lib.store.sqlite import connect

logger = logging.getLogger(__name__)


def ensure_schema(
    db_path: Path,
    migs: list[tuple[str, str | Callable]] | None = None,
) -> None:
    """Ensure schema exists and apply migrations."""
    with connect(db_path) as conn:
        conn.execute("PRAGMA journal_mode=WAL")
        if migs:
            migrate(conn, migs)
        conn.commit()


def migrate(conn: sqlite3.Connection, migs: list[tuple[str, str | Callable]]) -> None:
    """Apply migrations to connection with data loss safeguards."""
    conn.execute("CREATE TABLE IF NOT EXISTS _migrations (name TEXT PRIMARY KEY)")
    conn.commit()

    for name, migration in migs:
        applied = conn.execute("SELECT 1 FROM _migrations WHERE name = ?", (name,)).fetchone()
        if applied:
            continue
        try:
            cursor = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name != '_migrations' AND name != 'sqlite_sequence'"
            )
            tables = [row[0] for row in cursor.fetchall()]
            before = {t: _get_table_count(conn, t) for t in tables}

            if callable(migration):
                migration(conn)
            else:
                if isinstance(migration, str) and ";" in migration:
                    conn.executescript(migration)
                else:
                    conn.execute(migration)

            for table, count_before in before.items():
                try:
                    _check_migration_safety(conn, table, count_before, allow_loss=0)
                except ValueError as e:
                    logger.error(f"Migration '{name}' data loss detected: {e}")
                    raise

            conn.execute("INSERT OR IGNORE INTO _migrations (name) VALUES (?)", (name,))
            conn.commit()
        except Exception as e:
            conn.rollback()
            logger.error(f"Migration '{name}' failed: {e}")
            raise


def _get_table_count(conn: sqlite3.Connection, table: str) -> int:
    """Get row count for table, returns 0 if table doesn't exist."""
    try:
        cursor = conn.execute(
            "SELECT COUNT(*) FROM sqlite_master WHERE type='table' AND name=?",
            (table,),
        )
        if not cursor.fetchone()[0]:
            return 0
        result = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()
        return result[0] if result else 0
    except sqlite3.OperationalError:
        return 0


def _check_migration_safety(
    conn: sqlite3.Connection, table: str, before: int, allow_loss: int = 0
) -> None:
    """Verify row count after migration, raise if data loss exceeds threshold.

    Args:
            conn: Database connection
            table: Table name to check
            before: Row count before migration
            allow_loss: Max rows permitted to be lost (e.g., duplicates removed)

    Raises:
            ValueError: If data loss detected exceeds allow_loss
    """
    after = _get_table_count(conn, table)
    lost = before - after

    if lost > allow_loss:
        msg = f"Migration {table}: {lost} rows lost (before: {before}, after: {after})"
        logger.error(msg)
        raise ValueError(msg)

    if lost > 0:
        logger.warning(
            f"Migration {table}: {lost} rows removed (expected for allow_loss={allow_loss})"
        )
</file>

<file path="space/lib/format.py">
from datetime import datetime, timedelta


def humanize_timestamp(timestamp_str: str) -> str:
    if not timestamp_str:
        return "never"
    try:
        timestamp = datetime.fromisoformat(str(timestamp_str))
    except (ValueError, TypeError):
        return str(timestamp_str) if timestamp_str else "never"

    now = datetime.now()
    diff = now - timestamp

    if diff < timedelta(minutes=1):
        return "just now"
    if diff < timedelta(hours=1):
        minutes = int(diff.total_seconds() / 60)
        return f"{minutes} minute{'s' if minutes > 1 else ''} ago"
    if diff < timedelta(days=1):
        hours = int(diff.total_seconds() / 3600)
        return f"{hours} hour{'s' if hours > 1 else ''} ago"
    if diff < timedelta(weeks=1):
        days = int(diff.total_seconds() / 86400)
        return f"{days} day{'s' if days > 1 else ''} ago"
    if diff < timedelta(days=30):
        weeks = int(diff.total_seconds() / 604800)
        return f"{weeks} week{'s' if weeks > 1 else ''} ago"
    if diff < timedelta(days=365):
        months = int(diff.total_seconds() / 2592000)
        return f"{months} month{'s' if months > 1 else ''} ago"
    years = int(diff.total_seconds() / 31536000)
    return f"{years} year{'s' if years > 1 else ''} ago"


def format_duration(seconds: float) -> str:
    if seconds < 60:
        return f"{int(seconds)}s"
    if seconds < 3600:
        return f"{int(seconds / 60)}m"
    if seconds < 86400:
        hours = int(seconds / 3600)
        mins = int((seconds % 3600) / 60)
        return f"{hours}h {mins}m" if mins else f"{hours}h"
    days = int(seconds / 86400)
    hours = int((seconds % 86400) / 3600)
    return f"{days}d {hours}h" if hours else f"{days}d"


def format_memory_entries(entries: list, raw_output: bool = False) -> str:
    output_lines = []
    current_topic = None
    for e in entries:
        if e.topic != current_topic:
            if current_topic is not None:
                output_lines.append("")
            output_lines.append(f"# {e.topic}")
            current_topic = e.topic
        core_mark = " ★" if e.core else ""
        archived_mark = " [ARCHIVED]" if e.archived_at else ""
        timestamp_display = e.timestamp if raw_output else humanize_timestamp(e.timestamp)
        output_lines.append(
            f"[{e.memory_id[-8:]}] [{timestamp_display}] {e.message}{core_mark}{archived_mark}"
        )
    return "\n".join(output_lines)
</file>

<file path="space/lib/migrations.py">
"""Migration file loader - converts numbered .sql files to migration tuples."""

from pathlib import Path


def load_migrations(module_path: str) -> list[tuple[str, str]]:
    """Load migrations from migrations/ directory.

    Reads numbered .sql files (001_*.sql, 002_*.sql, etc.) and returns
    as migration tuples (name, sql_content) in lexical order.

    Args:
        module_path: Module path like 'space.apps.chats' or 'space.os.spawn'

    Returns:
        List of (migration_name, sql_content) tuples
    """
    parts = module_path.split(".")
    module_dir = Path(__file__).parent.parent
    for part in parts[1:]:
        module_dir = module_dir / part
    migrations_dir = module_dir / "migrations"

    if not migrations_dir.exists():
        return []

    migrations = []
    for sql_file in sorted(migrations_dir.glob("*.sql")):
        name = sql_file.stem
        sql_content = sql_file.read_text()
        migrations.append((name, sql_content))

    return migrations
</file>

<file path="space/lib/query.py">
"""Query builder helpers to DRY up common SQL patterns."""

import sqlite3
from typing import Any

from space.lib import store


def agent_by_name(conn: sqlite3.Connection, name: str, show_all: bool = False) -> list[str]:
    """Get agent UUIDs by name. Returns list of agent_ids."""
    archive_filter = "" if show_all else "AND archived_at IS NULL"
    rows = conn.execute(
        f"SELECT agent_id FROM agents WHERE name = ? {archive_filter}", (name,)
    ).fetchall()
    return [row["agent_id"] for row in rows]


def agent_by_id(conn: sqlite3.Connection, agent_id: str) -> str | None:
    """Get agent name by UUID."""
    row = conn.execute("SELECT name FROM agents WHERE agent_id = ?", (agent_id,)).fetchone()
    return row["name"] if row else None


def count_table(conn: sqlite3.Connection, table: str, where: str = "") -> int:
    """Count total records in table with optional WHERE clause."""
    query = f"SELECT COUNT(*) FROM {table}"
    if where:
        query += f" WHERE {where}"
    return conn.execute(query).fetchone()[0]


def count_active(conn: sqlite3.Connection, table: str, where: str = "") -> int:
    """Count active (non-archived) records."""
    conditions = ["archived_at IS NULL"]
    if where:
        conditions.append(where)
    return count_table(conn, table, " AND ".join(conditions))


def count_archived(conn: sqlite3.Connection, table: str, where: str = "") -> int:
    """Count archived records."""
    conditions = ["archived_at IS NOT NULL"]
    if where:
        conditions.append(where)
    return count_table(conn, table, " AND ".join(conditions))


def select_with_filter(
    conn: sqlite3.Connection,
    table: str,
    columns: str = "*",
    where: str = "",
    order_by: str = "",
    limit: int | None = None,
    show_all: bool = False,
) -> list[store.Row]:
    """Build SELECT with archive filter, WHERE, ORDER BY, and LIMIT."""
    query = f"SELECT {columns} FROM {table}"

    conditions = []
    if not show_all:
        conditions.append("archived_at IS NULL")
    if where:
        conditions.append(where)

    if conditions:
        query += " WHERE " + " AND ".join(conditions)

    if order_by:
        query += f" {order_by}"

    if limit:
        query += f" LIMIT {limit}"

    return conn.execute(query).fetchall()


def select_distinct(
    conn: sqlite3.Connection,
    table: str,
    column: str,
    where: str = "",
    params: tuple = (),
    show_all: bool = False,
) -> list[str]:
    """Get distinct values from column with optional filter."""
    query = f"SELECT DISTINCT {column} FROM {table}"

    conditions = []
    if not show_all:
        conditions.append("archived_at IS NULL")
    if where:
        conditions.append(where)

    if conditions:
        query += " WHERE " + " AND ".join(conditions)

    rows = conn.execute(query, params).fetchall()
    return [row[0] for row in rows]


def count_by_group(
    conn: sqlite3.Connection,
    table: str,
    group_column: str,
    where: str = "",
    order_by: str = "count DESC",
    limit: int | None = None,
) -> list[tuple[str, int]]:
    """GROUP BY with COUNT(*). Returns [(group_value, count), ...]."""
    query = f"SELECT {group_column}, COUNT(*) as count FROM {table}"

    if where:
        query += f" WHERE {where}"

    query += f" GROUP BY {group_column}"

    if order_by:
        query += f" ORDER BY {order_by}"

    params = ()
    if limit:
        query += " LIMIT ?"
        params = (limit,)

    rows = conn.execute(query, params).fetchall()
    return [(row[0], row[1]) for row in rows]


def update_where(
    conn: sqlite3.Connection, table: str, updates: dict[str, Any], where: str, params: tuple
) -> None:
    """Execute UPDATE with safe parameterization.

    Args:
        conn: Database connection
        table: Table name
        updates: Dict of {column: value} to update
        where: WHERE clause (e.g. "agent_id = ?")
        params: Tuple of params (values + where_params in order)
    """
    if not updates:
        return

    set_clause = ", ".join([f"{col} = ?" for col in updates])
    query = f"UPDATE {table} SET {set_clause} WHERE {where}"

    all_params = tuple(updates.values()) + params
    conn.execute(query, all_params)


def delete_where(conn: sqlite3.Connection, table: str, where: str, params: tuple) -> None:
    """Execute DELETE with safe parameterization."""
    query = f"DELETE FROM {table} WHERE {where}"
    conn.execute(query, params)
</file>

<file path="space/lib/text_utils.py">
# space/lib/text_utils.py

stopwords = {
    "the",
    "a",
    "an",
    "and",
    "or",
    "but",
    "in",
    "on",
    "at",
    "to",
    "for",
    "of",
    "with",
    "is",
    "are",
    "was",
    "were",
    "be",
    "been",
    "being",
    "have",
    "has",
    "had",
    "do",
    "does",
    "did",
    "will",
    "would",
    "should",
    "could",
    "may",
    "might",
    "must",
    "can",
    "this",
    "that",
    "these",
    "those",
    "i",
    "you",
    "he",
    "she",
    "it",
    "we",
    "they",
    "as",
    "by",
    "from",
    "not",
    "all",
    "each",
    "every",
    "some",
    "any",
    "no",
    "none",
}
</file>

<file path="space/lib/uuid7.py">
"""UUID v7 generation for time-ordered distributed IDs.

Implements RFC 9562 UUID v7 spec with monotonic counter:
- 48-bit Unix timestamp (milliseconds)
- 4-bit version (0111 = 7)
- 12-bit monotonic counter (increments within same millisecond)
- 2-bit variant (10)
- 62-bit random

Provides chronological ordering + global uniqueness without coordination.
"""

from __future__ import annotations

import secrets
import threading
import time
import uuid

# Monotonic state for same-millisecond IDs (RFC 9562 Method 2)
_state_lock = threading.Lock()
_last_timestamp_ms = 0
_counter = 0

# Check for native uuid7 support once at module load
_USE_NATIVE = hasattr(uuid, "uuid7")


def uuid7() -> str:
    """Generate UUID v7 (time-ordered) for distributed-safe IDs."""
    if _USE_NATIVE:
        return str(uuid.uuid7())

    global _last_timestamp_ms, _counter

    with _state_lock:
        timestamp_ms = int(time.time() * 1000)

        # RFC 9562 Method 2: Monotonic counter for same-millisecond IDs
        if timestamp_ms == _last_timestamp_ms:
            _counter = (_counter + 1) & 0xFFF  # 12-bit counter wraps
        else:
            _counter = secrets.randbits(12)
            _last_timestamp_ms = timestamp_ms

        # RFC 9562: 48-bit timestamp + 4-bit version + 12-bit counter
        time_high = (timestamp_ms >> 16) & 0xFFFFFFFF
        time_low = timestamp_ms & 0xFFFF

        # Version field: 0111 (7) in bits 12-15
        time_low_and_version = (time_low << 16) | (7 << 12) | _counter

        # Variant field: 10 in bits 0-1, followed by 62 bits random
        rand_b_high = secrets.randbits(14)
        rand_b_low = secrets.randbits(48)
        variant_and_rand = (0b10 << 62) | (rand_b_high << 48) | rand_b_low

        # Assemble 128-bit UUID
        uuid_int = (time_high << 96) | (time_low_and_version << 64) | variant_and_rand

        return str(uuid.UUID(int=uuid_int))


def short_id(full_uuid: str) -> str:
    """Return last 8 chars of UUID for display/matching.

    UUID7 structure: [48-bit timestamp][74-bit random]
    First chars = timestamp-dominated (low entropy, visual clustering)
    Last chars = high entropy suffix (good collision resistance)

    Like git short refs: keep full UUID in DB, display short for humans.
    """
    return full_uuid[-8:]


__all__ = ["uuid7", "short_id"]
</file>

<file path="space/os/bridge/cli/archive.py">
"""Archive channels."""

from __future__ import annotations

import json

import typer

from space.os.bridge import ops

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command()
    def archive(
        ctx: typer.Context,
        channels_arg: list[str] = typer.Argument(..., help="Channels to archive"),  # noqa: B008
        prefix: bool = typer.Option(False, "--prefix", help="Match by prefix"),
    ):
        """Archive channels."""
        try:
            names = channels_arg
            if prefix:
                chans = ops.list_channels()
                active = [c.name for c in chans if not c.archived_at]
                matched = []
                for pattern in channels_arg:
                    matched.extend([name for name in active if name.startswith(pattern)])
                names = list(set(matched))

            results = []
            for name in names:
                try:
                    ops.archive_channel(name)
                    results.append({"channel": name, "status": "archived"})
                    echo_if_output(f"Archived channel: {name}", ctx)
                except ValueError:
                    results.append(
                        {
                            "channel": name,
                            "status": "error",
                            "message": f"Channel '{name}' not found.",
                        }
                    )
                    echo_if_output(f"❌ Channel '{name}' not found.", ctx)
            if ctx.obj.get("json_output"):
                typer.echo(json.dumps(results))
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/create.py">
"""Create a channel."""

from __future__ import annotations

import typer

from space.os.bridge import ops

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command()
    def create(
        ctx: typer.Context,
        channel_name: str = typer.Argument(..., help="Channel name"),
        topic: str = typer.Option(None, help="Channel topic"),
    ):
        """Create a channel."""
        try:
            channel_id = ops.create_channel(channel_name, topic)
            output_json(
                {"status": "success", "channel_name": channel_name, "channel_id": channel_id}, ctx
            ) or echo_if_output(f"Created channel: {channel_name} (ID: {channel_id})", ctx)
        except ValueError as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ Error creating channel: {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/delete.py">
from __future__ import annotations

import typer

from space.os.bridge import ops

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command("delete")
    def delete(
        ctx: typer.Context,
        channel: str = typer.Argument(..., help="Channel to delete"),
    ):
        """Delete a channel."""
        try:
            ops.delete_channel(channel)
            output_json({"status": "deleted", "channel": channel}, ctx) or echo_if_output(
                f"Deleted channel: {channel}", ctx
            )
        except ValueError as e:
            output_json(
                {"status": "error", "message": f"Channel '{channel}' not found."}, ctx
            ) or echo_if_output(f"❌ Channel '{channel}' not found.", ctx)
            raise typer.Exit(code=1) from e
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/inbox.py">
"""Show unread channels for an agent."""

from __future__ import annotations

from dataclasses import asdict

import typer

from space.os import spawn
from space.os.bridge import ops

from .format import echo_if_output, format_channel_row, output_json, should_output


def register(app: typer.Typer) -> None:
    @app.command()
    def inbox(
        ctx: typer.Context,
        identity: str = typer.Option(..., "--as", help="Agent identity"),
    ):
        """Show unread channels for an agent."""
        try:
            agent = spawn.get_agent(identity)
            if not agent:
                raise ValueError(f"Identity '{identity}' not registered.")
            chans = ops.fetch_inbox(agent.agent_id)
            if not chans:
                output_json([], ctx) or echo_if_output("Inbox empty", ctx)
                return

            output_json([asdict(c) for c in chans], ctx) or None
            if should_output(ctx):
                echo_if_output(f"INBOX ({len(chans)}):", ctx)
                for channel in chans:
                    last_activity, description = format_channel_row(channel)
                    echo_if_output(f"  {last_activity}: {description}", ctx)
        except Exception as exc:
            output_json({"status": "error", "message": str(exc)}, ctx) or echo_if_output(
                f"❌ {exc}", ctx
            )
            raise typer.Exit(code=1) from exc
</file>

<file path="space/os/bridge/cli/pin.py">
"""Pin channels to favorites."""

from __future__ import annotations

import json

import typer

from space.os.bridge import ops

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command()
    def pin(
        ctx: typer.Context,
        channels_arg: list[str] = typer.Argument(..., help="Channels to pin"),  # noqa: B008
    ):
        """Pin channels to favorites."""
        try:
            results = []
            for channel in channels_arg:
                try:
                    ops.pin_channel(channel)
                    results.append({"channel": channel, "status": "pinned"})
                    echo_if_output(f"Pinned channel: {channel}", ctx)
                except (ValueError, TypeError) as e:
                    results.append({"channel": channel, "status": "error", "message": str(e)})
                    echo_if_output(f"❌ Channel '{channel}' not found.", ctx)
            if ctx.obj.get("json_output"):
                typer.echo(json.dumps(results))
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/recv.py">
"""Receive unread messages from a channel."""

from __future__ import annotations

from dataclasses import asdict

import typer

from space.os import spawn
from space.os.bridge import ops

from .format import echo_if_output, output_json, should_output


def register(app: typer.Typer) -> None:
    @app.command()
    def recv(
        ctx: typer.Context,
        channel: str = typer.Argument(..., help="Channel to read from"),
        identity: str = typer.Option(..., "--as", help="Receiver identity"),
    ):
        """Receive unread messages from a channel."""
        try:
            agent = spawn.get_agent(identity)
            if not agent:
                raise ValueError(f"Identity '{identity}' not registered.")
            msgs, count, context, participants = ops.recv_messages(channel, agent.agent_id)

            output_json(
                {
                    "messages": [asdict(msg) for msg in msgs],
                    "count": count,
                    "context": context,
                    "participants": participants,
                },
                ctx,
            ) or None
            if should_output(ctx):
                for msg in msgs:
                    sender = spawn.get_agent(msg.agent_id)
                    sender_name = sender.identity if sender else msg.agent_id[:8]
                    echo_if_output(f"[{sender_name}] {msg.content}", ctx)
                    echo_if_output("", ctx)
        except ValueError as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/rename.py">
"""Rename a channel."""

from __future__ import annotations

import typer

from space.os.bridge import ops

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command()
    def rename(
        ctx: typer.Context,
        old_channel: str = typer.Argument(..., help="Current channel name"),
        new_channel: str = typer.Argument(..., help="New channel name"),
    ):
        """Rename a channel."""
        try:
            result = ops.rename_channel(old_channel, new_channel)
            output_json(
                {
                    "status": "success" if result else "failed",
                    "old_channel": old_channel,
                    "new_channel": new_channel,
                },
                ctx,
            ) or (
                echo_if_output(f"Renamed channel: {old_channel} -> {new_channel}", ctx)
                if result
                else echo_if_output(
                    f"❌ Rename failed: {old_channel} not found or {new_channel} already exists",
                    ctx,
                )
            )
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/send.py">
"""Send a message to a channel."""

from __future__ import annotations

import typer

from space.os import spawn
from space.os.bridge import ops

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command()
    def send(
        ctx: typer.Context,
        channel: str = typer.Argument(..., help="Target channel"),
        content: str = typer.Argument(..., help="Message content"),
        identity: str = typer.Option("human", "--as", help="Sender identity"),
        decode_base64: bool = typer.Option(False, "--base64", help="Decode base64 content"),
    ):
        """Send a message to a channel."""
        try:
            agent = spawn.get_agent(identity)
            if not agent:
                raise ValueError(f"Identity '{identity}' not registered.")
            ops.send_message(channel, identity, content, decode_base64)
            output_json(
                {"status": "success", "channel": channel, "identity": identity}, ctx
            ) or echo_if_output(
                f"Sent to {channel}" if identity == "human" else f"Sent to {channel} as {identity}",
                ctx,
            )
        except ValueError as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/unpin.py">
"""Unpin channels from favorites."""

from __future__ import annotations

import json

import typer

from space.os.bridge import ops

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command()
    def unpin(
        ctx: typer.Context,
        channels_arg: list[str] = typer.Argument(..., help="Channels to unpin"),  # noqa: B008
    ):
        """Unpin channels from favorites."""
        try:
            results = []
            for channel in channels_arg:
                try:
                    ops.unpin_channel(channel)
                    results.append({"channel": channel, "status": "unpinned"})
                    echo_if_output(f"Unpinned channel: {channel}", ctx)
                except (ValueError, TypeError) as e:
                    results.append({"channel": channel, "status": "error", "message": str(e)})
                    echo_if_output(f"❌ Channel '{channel}' not found.", ctx)
            if ctx.obj.get("json_output"):
                typer.echo(json.dumps(results))
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/wait.py">
"""Block and wait for a new message in a channel."""

from __future__ import annotations

from dataclasses import asdict

import typer

from space.os import spawn
from space.os.bridge import ops

from .format import echo_if_output, output_json, should_output


def register(app: typer.Typer) -> None:
    @app.command()
    def wait(
        ctx: typer.Context,
        channel: str = typer.Argument(..., help="Channel to monitor"),
        identity: str = typer.Option(..., "--as", help="Receiver identity"),
        poll_interval: float = typer.Option(0.1, "--interval", help="Poll interval in seconds"),
    ):
        """Block and wait for a new message in a channel."""
        try:
            agent = spawn.get_agent(identity)
            if not agent:
                raise ValueError(f"Identity '{identity}' not registered.")
            other_messages, count, context, participants = ops.wait_for_message(
                channel, agent.agent_id, poll_interval
            )
            output_json(
                {
                    "messages": [asdict(msg) for msg in other_messages],
                    "count": count,
                    "context": context,
                    "participants": participants,
                },
                ctx,
            ) or None
            if should_output(ctx):
                for msg in other_messages:
                    sender = spawn.get_agent(msg.agent_id)
                    sender_name = sender.identity if sender else msg.agent_id[:8]
                    echo_if_output(f"[{sender_name}] {msg.content}", ctx)
                    echo_if_output("", ctx)
        except ValueError as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
        except KeyboardInterrupt:
            echo_if_output("\n", ctx)
            raise typer.Exit(code=0) from None
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/knowledge/api/__init__.py">
from .entries import (
    add_entry,
    archive_entry,
    find_related,
    get_by_id,
    list_entries,
    query_by_agent,
    query_by_domain,
    restore_entry,
)
from .search import search
from .stats import stats

__all__ = [
    "add_entry",
    "archive_entry",
    "find_related",
    "get_by_id",
    "list_entries",
    "query_by_agent",
    "query_by_domain",
    "restore_entry",
    "search",
    "stats",
]
</file>

<file path="space/os/knowledge/api/search.py">
"""Knowledge search: unified query interface."""

from space.lib import store


def search(query: str, identity: str | None, all_agents: bool) -> list[dict]:
    """Search knowledge entries by query, optionally filtering by agent, returning structured results with references."""
    from space.os import spawn

    results = []
    with store.ensure("knowledge") as conn:
        sql_query = (
            "SELECT knowledge_id, domain, agent_id, content, created_at FROM knowledge "
            "WHERE (content LIKE ? OR domain LIKE ?)"
        )
        params = [f"%{query}%", f"%{query}%"]

        if identity and not all_agents:
            agent = spawn.get_agent(identity)
            if not agent:
                raise ValueError(f"Agent '{identity}' not found")
            sql_query += " AND agent_id = ?"
            params.append(agent.agent_id)

        sql_query += " ORDER BY created_at ASC"

        rows = conn.execute(sql_query, params).fetchall()
        for row in rows:
            agent = spawn.get_agent(row["agent_id"])
            results.append(
                {
                    "source": "knowledge",
                    "domain": row["domain"],
                    "knowledge_id": row["knowledge_id"],
                    "contributor": agent.name if agent else row["agent_id"],
                    "content": row["content"],
                    "timestamp": row["created_at"],
                    "reference": f"knowledge:{row['knowledge_id']}",
                }
            )
    return results
</file>

<file path="space/os/knowledge/api/stats.py">
"""Knowledge stats aggregation."""

from space.lib import store


def stats() -> dict:
    """Get knowledge statistics."""
    with store.ensure("knowledge") as conn:
        total = conn.execute("SELECT COUNT(*) FROM knowledge").fetchone()[0]
        active = conn.execute(
            "SELECT COUNT(*) FROM knowledge WHERE archived_at IS NULL"
        ).fetchone()[0]
        archived = total - active

        domains = conn.execute(
            "SELECT COUNT(DISTINCT domain) FROM knowledge WHERE archived_at IS NULL"
        ).fetchone()[0]

        know_by_agent = conn.execute(
            "SELECT agent_id, COUNT(*) as count FROM knowledge GROUP BY agent_id ORDER BY count DESC"
        ).fetchall()

    return {
        "total": total,
        "active": active,
        "archived": archived,
        "topics": domains,
        "know_by_agent": [{"agent_id": row[0], "count": row[1]} for row in know_by_agent],
    }
</file>

<file path="space/os/knowledge/cli/__init__.py">
"""Knowledge commands: CLI parsing & typer wiring."""

import typer

from space.lib import output

app = typer.Typer(invoke_without_command=True)

from . import entries  # noqa: E402

app.add_typer(entries.app)


@app.callback()
def main_callback(
    ctx: typer.Context,
    json_output: bool = typer.Option(False, "--json", "-j", help="Output in JSON format."),
    quiet_output: bool = typer.Option(
        False, "--quiet", "-q", help="Suppress non-essential output."
    ),
):
    """Knowledge: Domain-specific Knowledge Base"""
    output.set_flags(ctx, json_output, quiet_output)
    if ctx.obj is None:
        ctx.obj = {}

    if ctx.resilient_parsing or ctx.invoked_subcommand is None:
        typer.echo(
            "knowledge [command]: Manage domain-specific knowledge. Run 'knowledge --help' for commands."
        )


def __getattr__(name):
    if name == "entries":
        from . import entries

        return entries
    raise AttributeError(f"module '{__name__}' has no attribute '{name}'")


__all__ = ["entries", "app"]
</file>

<file path="space/os/knowledge/cli/entries.py">
"""Knowledge entry commands: add, list, query, get, inspect, archive."""

from dataclasses import asdict

import typer

from space.lib import errors, output
from space.os import spawn

from ..api import entries as api

errors.install_error_handler("knowledge")

app = typer.Typer()


@app.command("add")
def add(
    ctx: typer.Context,
    content: str = typer.Argument(..., help="The knowledge content"),
    domain: str = typer.Option(..., help="Domain of the knowledge"),
    contributor: str = typer.Option(..., "--as", help="Agent identity"),
    confidence: float = typer.Option(None, help="Confidence score (0.0-1.0)"),
):
    """Add a new knowledge entry."""
    agent = spawn.get_agent(contributor)
    agent_id = agent.agent_id if agent else None
    if not agent_id:
        output.out_text(f"Agent not found: {contributor}", ctx.obj)
        return
    entry_id = api.add_entry(domain, agent_id, content, confidence)
    if ctx.obj.get("json_output"):
        typer.echo(output.out_json({"entry_id": entry_id}))
    else:
        output.out_text(
            f"Added knowledge entry {entry_id} for domain '{domain}' by '{contributor}'", ctx.obj
        )


@app.command("list")
def list_entries(
    ctx: typer.Context,
    show_all: bool = typer.Option(False, "--all", help="Show all entries"),
):
    """List all knowledge entries."""
    entries = api.list_entries(show_all=show_all)
    if not entries:
        if ctx.obj.get("json_output"):
            typer.echo(output.out_json([]))
        else:
            output.out_text("No knowledge entries found.", ctx.obj)
        return

    if ctx.obj.get("json_output"):
        typer.echo(output.out_json([asdict(e) for e in entries]))
        return

    output.out_text("Knowledge entries:", ctx.obj)
    for e in entries:
        agent = spawn.get_agent(e.agent_id)
        contributor = agent.identity if agent else e.agent_id[:8]
        output.out_text(
            f"[{e.knowledge_id[-8:]}] {e.content[:50]}... ({contributor}){mark}", ctx.obj
        )


@app.command("query")
def query_domain(
    ctx: typer.Context,
    domain: str = typer.Argument(..., help="Domain to query"),
    show_all: bool = typer.Option(False, "--all", help="Show all entries"),
):
    """Query knowledge entries by domain."""
    entries = api.query_by_domain(domain, show_all=show_all)
    if not entries:
        output.out_text(f"No entries for domain '{domain}'", ctx.obj)
        return

    if ctx.obj.get("json_output"):
        typer.echo(output.out_json([asdict(e) for e in entries]))
        return

    output.out_text(f"Domain: {domain} ({len(entries)} entries)", ctx.obj)
    for e in entries:
        mark = " [ARCHIVED]" if e.archived_at else ""
        agent = spawn.get_agent(e.agent_id)
        contributor = agent.identity if agent else e.agent_id[:8]
        conf = f" [confidence: {e.confidence:.2f}]" if e.confidence else ""
        output.out_text(
            f"[{e.knowledge_id[-8:]}] {e.content[:60]}...{conf} ({contributor}){mark}", ctx.obj
        )


@app.command("inspect")
def inspect(
    ctx: typer.Context,
    knowledge_id: str = typer.Argument(..., help="Knowledge ID to inspect"),
):
    """Inspect knowledge entry details."""
    entry = api.get_by_id(knowledge_id)
    if not entry:
        output.out_text(f"Not found: {knowledge_id}", ctx.obj)
        return

    if ctx.obj.get("json_output"):
        typer.echo(output.out_json(asdict(entry)))
        return

    agent = spawn.get_agent(entry.agent_id)
    contributor = agent.identity if agent else entry.agent_id
    output.out_text(f"ID: {entry.knowledge_id}", ctx.obj)
    output.out_text(f"Domain: {entry.domain}", ctx.obj)
    output.out_text(f"Contributor: {contributor}", ctx.obj)
    if entry.confidence:
        output.out_text(f"Confidence: {entry.confidence}", ctx.obj)
    output.out_text(f"Created: {entry.created_at}", ctx.obj)
    if entry.archived_at:
        output.out_text(f"Archived: {entry.archived_at}", ctx.obj)
    output.out_text(f"\nContent:\n{entry.content}", ctx.obj)


@app.command("archive")
def archive(
    ctx: typer.Context,
    knowledge_id: str = typer.Argument(..., help="Knowledge ID to archive"),
    restore: bool = typer.Option(False, "--restore", help="Restore archived entry"),
):
    """Archive or restore a knowledge entry."""
    entry = api.get_by_id(knowledge_id)
    if not entry:
        output.out_text(f"Not found: {knowledge_id}", ctx.obj)
        return

    try:
        if restore:
            api.restore_entry(knowledge_id)
            action = "restored"
        else:
            api.archive_entry(knowledge_id)
            action = "archived"
        output.out_text(f"{action} {knowledge_id[-8:]}", ctx.obj)
    except ValueError as e:
        output.emit_error("knowledge", entry.agent_id, "archive/restore", e)
        raise typer.BadParameter(str(e)) from e
</file>

<file path="space/os/memory/api/__init__.py">
from .entries import (
    add_entry,
    add_link,
    archive_entry,
    delete_entry,
    edit_entry,
    find_related,
    get_by_id,
    get_chain,
    list_entries,
    mark_core,
    replace_entry,
    restore_entry,
)
from .search import search
from .stats import stats

__all__ = [
    "add_entry",
    "add_link",
    "archive_entry",
    "delete_entry",
    "edit_entry",
    "find_related",
    "get_by_id",
    "get_chain",
    "list_entries",
    "mark_core",
    "replace_entry",
    "restore_entry",
    "search",
    "stats",
]
</file>

<file path="space/os/memory/api/search.py">
"""Memory search: unified query interface."""

from space.lib import store


def search(query: str, identity: str | None, all_agents: bool) -> list[dict]:
    """Search memory entries by query, optionally filtering by agent, returning structured results with references."""
    from space.os import spawn

    results = []
    with store.ensure("memory") as conn:
        sql_query = (
            "SELECT memory_id, agent_id, topic, message, timestamp, created_at FROM memories "
            "WHERE (message LIKE ? OR topic LIKE ?)"
        )
        params = [f"%{query}%", f"%{query}%"]

        if identity and not all_agents:
            agent = spawn.get_agent(identity)
            if not agent:
                raise ValueError(f"Agent '{identity}' not found")
            sql_query += " AND agent_id = ?"
            params.append(agent.agent_id)

        sql_query += " ORDER BY created_at ASC"

        rows = conn.execute(sql_query, params).fetchall()
        for row in rows:
            agent = spawn.get_agent(row["agent_id"])
            results.append(
                {
                    "source": "memory",
                    "memory_id": row["memory_id"],
                    "topic": row["topic"],
                    "message": row["message"],
                    "identity": agent.identity if agent else row["agent_id"],
                    "timestamp": row["created_at"],
                    "reference": f"memory:{row['memory_id']}",
                }
            )
    return results
</file>

<file path="space/os/memory/api/stats.py">
"""Memory stats aggregation."""

from space.lib import store


def stats() -> dict:
    """Get memory statistics."""
    with store.ensure("memory") as conn:
        total = conn.execute("SELECT COUNT(*) FROM memories").fetchone()[0]
        active = conn.execute("SELECT COUNT(*) FROM memories WHERE archived_at IS NULL").fetchone()[
            0
        ]
        archived = total - active

        topics = conn.execute(
            "SELECT COUNT(DISTINCT topic) FROM memories WHERE archived_at IS NULL"
        ).fetchone()[0]

        mem_by_agent = conn.execute(
            "SELECT agent_id, COUNT(*) as count FROM memories GROUP BY agent_id ORDER BY count DESC"
        ).fetchall()

    return {
        "total": total,
        "active": active,
        "archived": archived,
        "topics": topics,
        "mem_by_agent": [{"agent_id": row[0], "count": row[1]} for row in mem_by_agent],
    }
</file>

<file path="space/os/memory/cli/__init__.py">
"""Memory commands."""

from typing import Annotated

import typer

from space.lib import errors, output

from . import entries  # Keep existing entries for now
from .namespace import create_namespace_cli  # Import the factory

errors.install_error_handler("memory")


app = typer.Typer(
    invoke_without_command=True,
    help="""Memory: Knowledge Base Management.

Use `memory <namespace> <message> --as <agent>` to quickly add entries to specific namespaces.
Example: `memory journal "Wound down session" --as zealot`

Use `memory <namespace> --as <agent>` to list entries in a namespace.
Example: `memory notes --as zealot`

For general memory commands (add, list, archive, core, replace, inspect), use `memory <command> ...`
Example: `memory add --topic general "A general thought" --as zealot`""",
)


# Register commands from entries.py
entries.register_commands(app)

# Dynamically create and add namespace apps
app.add_typer(create_namespace_cli("journal", "journal"), name="journal")
app.add_typer(create_namespace_cli("notes", "note"), name="notes")
app.add_typer(create_namespace_cli("tasks", "task"), name="tasks")
app.add_typer(create_namespace_cli("beliefs", "belief"), name="beliefs")


@app.callback()
def main_callback(
    ctx: typer.Context,
    identity: Annotated[str | None, typer.Option("--as", help="Agent identity to use.")] = None,
    json_output: Annotated[
        bool, typer.Option("--json", "-j", help="Output in JSON format.")
    ] = False,
    quiet_output: Annotated[
        bool, typer.Option("--quiet", "-q", help="Suppress non-essential output.")
    ] = False,
):
    output.set_flags(ctx, json_output, quiet_output)

    if ctx.obj is None:
        ctx.obj = {}

    ctx.obj["identity"] = identity
    ctx.obj["json"] = json_output
    ctx.obj["quiet"] = quiet_output

    if ctx.resilient_parsing:
        return

    if ctx.invoked_subcommand is None:
        if identity:
            # Invoke the 'list' command from entries.py using ctx.invoke
            # This ensures Typer handles the context and argument passing correctly.
            ctx.invoke(entries.list, ident=identity, topic=None, show_all=True, raw_output=False)
        else:
            typer.echo("memory [command] --as <identity>: Store and retrieve agent memories.")
            typer.echo("Run 'memory --help' for a list of commands.")


__all__ = ["app"]
</file>

<file path="space/os/memory/cli/entries.py">
"""Entry commands: add, edit, list, search, archive, core, inspect, replace."""

from dataclasses import asdict

import typer

from space.lib import display, output
from space.lib.format import format_memory_entries
from space.os import spawn

from .. import api


def register_commands(app: typer.Typer):
    @app.command("add")
    def add(
        ctx: typer.Context,
        message: str = typer.Argument(..., help="The memory message"),
        topic: str = typer.Option(..., help="Topic name"),
    ):
        """Add a new memory entry."""
        ident = ctx.obj.get("identity")
        if not ident:
            raise typer.BadParameter("--as required")
        agent = spawn.get_agent(ident)
        if not agent:
            raise typer.BadParameter(f"Identity '{ident}' not registered.")
        agent_id = agent.agent_id
        entry_id = api.add_entry(agent_id, topic, message)
        if ctx.obj.get("json_output"):
            typer.echo(output.out_json({"entry_id": entry_id}))
        else:
            output.out_text(f"Added memory: {topic}", ctx.obj)

    @app.command("edit")
    def edit(
        ctx: typer.Context,
        uuid: str = typer.Argument(..., help="UUID of the entry to edit"),
        message: str = typer.Argument(..., help="The new message content"),
    ):
        """Edit an existing memory entry."""
        entry = api.get_by_id(uuid)
        if not entry:
            raise typer.BadParameter(f"Entry not found: {uuid}")
        try:
            api.edit_entry(uuid, message)
            output.out_text(f"Edited {uuid[-8:]}", ctx.obj)
        except ValueError as e:
            output.emit_error("memory", entry.agent_id, "edit", e)
            raise typer.BadParameter(str(e)) from e

    @app.command("list")
    def list(
        ctx: typer.Context,
        topic: str = typer.Option(None, help="Topic name"),
        show_all: bool = typer.Option(False, "--all", help="Show all entries"),
        raw_output: bool = typer.Option(
            False, "--raw", help="Output raw timestamps instead of humanized."
        ),
    ):
        """List memory entries for an identity and optional topic."""
        ident = ctx.obj.get("identity")
        if not ident:
            raise typer.BadParameter("--as required")
        try:
            entries = api.list_entries(ident, topic, show_all=show_all)
        except ValueError as e:
            output.emit_error("memory", None, "list", e)
            raise typer.BadParameter(str(e)) from e

        entries.sort(key=lambda e: (not e.core, e.timestamp), reverse=True)
        if not entries:
            output.out_text("No entries", ctx.obj)
            return

        if ctx.obj.get("json_output"):
            typer.echo(output.out_json([asdict(e) for e in entries]))
        else:
            output.out_text(format_memory_entries(entries, raw_output=raw_output), ctx.obj)
            if not ctx.obj.get("quiet_output"):
                display.show_context(ident)

    @app.command("archive")
    def archive(
        ctx: typer.Context,
        uuid: str = typer.Argument(..., help="UUID of the entry to archive"),
        restore: bool = typer.Option(False, "--restore", help="Restore archived entry"),
    ):
        """Archive or restore a memory entry."""
        entry = api.get_by_id(uuid)
        if not entry:
            raise typer.BadParameter(f"Entry not found: {uuid}")
        try:
            if restore:
                api.restore_entry(uuid)
                action = "restored"
            else:
                api.archive_entry(uuid)
                action = "archived"
            output.out_text(f"{action} {uuid[-8:]}", ctx.obj)
        except ValueError as e:
            output.emit_error("memory", entry.agent_id, "archive/restore", e)
            raise typer.BadParameter(str(e)) from e

    @app.command("core")
    def core(
        ctx: typer.Context,
        uuid: str = typer.Argument(..., help="UUID of the entry to mark/unmark as core"),
        unmark: bool = typer.Option(False, "--unmark", help="Unmark as core"),
    ):
        """Mark or unmark entry as core memory."""
        entry = api.get_by_id(uuid)
        if not entry:
            raise typer.BadParameter(f"Entry not found: {uuid}")
        try:
            is_core = not unmark
            api.mark_core(uuid, core=is_core)
            output.out_text(f"{'★' if is_core else ''} {uuid[-8:]}", ctx.obj)
        except ValueError as e:
            output.emit_error("memory", entry.agent_id, "core", e)
            raise typer.BadParameter(str(e)) from e

    @app.command("inspect")
    def inspect(
        ctx: typer.Context,
        uuid: str = typer.Argument(..., help="UUID of the entry to inspect"),
        limit: int = typer.Option(5, help="Number of related entries to show"),
        show_all: bool = typer.Option(False, "--all", help="Show all entries"),
    ):
        """Inspect entry and find related nodes via keyword similarity."""
        identity_name = ctx.obj.get("identity")
        if not identity_name:
            raise typer.BadParameter("--as required")
        agent = spawn.get_agent(identity_name)
        if not agent:
            raise typer.BadParameter(f"Identity '{identity_name}' not registered.")
        agent_id = agent.agent_id
        try:
            entry = api.get_by_id(uuid)
        except ValueError as e:
            output.emit_error("memory", agent_id, "inspect", e)
            raise typer.BadParameter(str(e)) from e

        if not entry:
            output.out_text("Not found", ctx.obj)
            return

        if entry.agent_id != agent_id:
            agent = spawn.get_agent(entry.agent_id)
            name = agent.identity if agent else entry.agent_id
            output.out_text(f"Belongs to {name}", ctx.obj)
            return

        related = api.find_related(entry, limit=limit, show_all=show_all)
        if ctx.obj.get("json_output"):
            payload = {
                "entry": asdict(entry),
                "related": [{"entry": asdict(r[0]), "overlap": r[1]} for r in related],
            }
            typer.echo(output.out_json(payload))
        else:
            display.show_memory_entry(entry, ctx.obj, related=related)

    @app.command("replace")
    def replace(
        ctx: typer.Context,
        old_id: str = typer.Argument(None, help="Single UUID to replace"),
        message: str = typer.Argument(..., help="New message content"),
        supersedes: str = typer.Option(None, help="Comma-separated UUIDs to replace"),
        note: str = typer.Option("", "--note", help="Synthesis note"),
    ):
        """Replace memory entry with new version, archiving old and linking both."""
        id = ctx.obj.get("identity")
        if not id:
            raise typer.BadParameter("--as required")
        agent = spawn.get_agent(id)
        if not agent:
            raise typer.BadParameter(f"Identity '{id}' not registered.")
        agent_id = agent.agent_id

        if supersedes:
            old_ids = [x.strip() for x in supersedes.split(",")]
        elif old_id:
            old_ids = [old_id]
        else:
            raise typer.BadParameter("Provide old_id or --supersedes")

        old_entry = api.get_by_id(old_ids[0])
        if not old_entry:
            raise typer.BadParameter(f"Not found: {old_ids[0]}")

        new_uuid = api.replace_entry(old_ids, agent_id, old_entry.topic, message, note)
        output.out_text(f"Merged {len(old_ids)} → {new_uuid[-8:]}", ctx.obj)
</file>

<file path="space/os/spawn/api/symlinks.py">
"""Symlink management for agent CLI shortcuts."""

import subprocess
from pathlib import Path


def _setup_launch_symlink(launch_script: Path) -> bool:
    """Setup launch script in ~/.local/bin. Called by space init."""
    try:
        bin_dir = Path.home() / ".local" / "bin"
        bin_dir.mkdir(parents=True, exist_ok=True)

        symlink_path = bin_dir / "launch"
        subprocess.run(
            ["ln", "-sf", str(launch_script), str(symlink_path)],
            check=True,
            capture_output=True,
        )
        return True
    except Exception:
        return False


def create_agent_symlink(identity: str) -> bool:
    """Create symlink in ~/.local/bin for agent identity.

    Returns True if successful, False otherwise.
    """
    try:
        bin_dir = Path.home() / ".local" / "bin"
        bin_dir.mkdir(parents=True, exist_ok=True)

        symlink_path = bin_dir / identity
        launch_executable = Path.home() / ".local" / "bin" / "launch"

        if not launch_executable.exists():
            return False

        subprocess.run(
            ["ln", "-sf", str(launch_executable), str(symlink_path)],
            check=True,
            capture_output=True,
        )
        return True
    except Exception:
        return False
</file>

<file path="space/os/spawn/cli/agents.py">
"""Agent list command."""

import json

import typer

from space.apps.stats import agent_stats
from space.os.spawn import api


def list_agents(
    show_all: bool = typer.Option(False, "--all", help="Show archived agents"),
    json_output: bool = typer.Option(False, "--json", help="Output as JSON"),
):
    """List all agents (registered and orphaned across universe)."""
    stats = agent_stats(show_all=show_all) or []

    if not stats:
        if json_output:
            typer.echo(json.dumps([]))
        else:
            typer.echo("No agents found.")
        return

    if json_output:
        agents_data = []
        for s in sorted(stats, key=lambda a: a.identity or ""):
            agent_id = s.agent_id
            if not agent_id:
                continue
            name = s.identity or ""
            if len(name) == 36 and name.count("-") == 4:
                agent = api.get_agent(name)
                if agent:
                    name = agent.identity
            agent = api.get_agent(agent_id)
            agents_data.append(
                {
                    "identity": name,
                    "agent_id": agent_id,
                    "model": agent.model if agent and agent.model else "-",
                    "description": agent.description if agent and agent.description else "-",
                }
            )
        typer.echo(json.dumps(agents_data))
    else:
        typer.echo(f"{'IDENTITY':<20} {'AGENT_ID':<10} {'MODEL':<25} {'DESCRIPTION'}")

        for s in sorted(stats, key=lambda a: a.identity or ""):
            name = s.identity or ""
            agent_id = s.agent_id
            if not agent_id:
                continue
            short_id = agent_id[:8]

            if len(name) == 36 and name.count("-") == 4:
                agent = api.get_agent(name)
                if agent:
                    name = agent.identity

            agent = api.get_agent(agent_id)
            model = agent.model if agent and agent.model else "-"
            desc = agent.description if agent and agent.description else "-"

            typer.echo(f"{name:<20} {short_id:<10} {model:<25} {desc}")

        typer.echo()
        typer.echo(f"Total: {len(stats)}")
</file>

<file path="space/os/spawn/cli/clone.py">
"""Clone agent command."""

import typer

from space.os.spawn import api


def clone(src: str, dst: str):
    """Clone an agent with new identity."""
    try:
        agent_id = api.clone_agent(src, dst)
        typer.echo(f"✓ Cloned {src} → {dst} ({agent_id[:8]})")
    except ValueError as e:
        typer.echo(f"❌ {e}", err=True)
        raise typer.Exit(1) from e
</file>

<file path="space/os/spawn/cli/dynamic.py">
"""Dynamic agent launcher: routes command name to spawn if registered."""

import sys
from typing import NoReturn

import click

from space.os.spawn import api


def dispatch_agent_from_name() -> NoReturn:
    """Entry point: route command name (argv[0]) to agent if registered."""
    prog_name = sys.argv[0].split("/")[-1]

    agent = api.get_agent(prog_name)
    if not agent:
        click.echo(f"Error: '{prog_name}' is not a registered agent identity.", err=True)
        click.echo("Run 'spawn agents' to list available agents.", err=True)
        sys.exit(1)

    args = sys.argv[1:] if len(sys.argv) > 1 else []
    api.spawn_agent(agent.identity, extra_args=args)
    sys.exit(0)
</file>

<file path="space/os/spawn/cli/merge.py">
"""Merge agents command."""

import typer

from space.os.spawn import api


def merge(id_from: str, id_to: str):
    """Merge all data from one agent ID to another."""
    agent_from = api.get_agent(id_from)
    agent_to = api.get_agent(id_to)

    if not agent_from:
        typer.echo(f"Error: Agent '{id_from}' not found")
        raise typer.Exit(1)
    if not agent_to:
        typer.echo(f"Error: Agent '{id_to}' not found")
        raise typer.Exit(1)

    result = api.merge_agents(id_from, id_to)

    if not result:
        typer.echo("Error: Could not merge agents")
        raise typer.Exit(1)

    from_display = agent_from.identity or id_from[:8]
    to_display = agent_to.identity or id_to[:8]
    typer.echo(f"Merging {from_display} → {to_display}")
    typer.echo("✓ Merged")
</file>

<file path="space/os/spawn/cli/models.py">
"""List available models command."""

import typer

from space.os.spawn import models as models_module


def list_models():
    """List available models for all providers."""
    for prov in ["claude", "codex", "gemini"]:
        provider_models = models_module.get_models_for_provider(prov)
        typer.echo(f"\n📦 {prov.capitalize()} Models:\n")
        for model in provider_models:
            typer.echo(f"  • {model.name} ({model.id})")
            if model.description:
                typer.echo(f"    {model.description}")
            if model.reasoning_levels:
                typer.echo(f"    Reasoning levels: {', '.join(model.reasoning_levels)}")
            typer.echo()
</file>

<file path="space/os/spawn/cli/register.py">
"""Register agent command."""

import typer

from space.os.spawn import api


def register(
    identity: str,
    model: str = typer.Option(
        ..., "--model", "-m", help="Model ID. Run 'spawn models' to list available models"
    ),
    constitution: str | None = typer.Option(
        None, "--constitution", "-c", help="Constitution filename (e.g., zealot.md) - optional"
    ),
):
    """Register a new agent."""
    try:
        agent_id = api.register_agent(identity, model, constitution)
        typer.echo(f"✓ Registered {identity} ({agent_id[:8]})")
    except ValueError as e:
        typer.echo(f"❌ {e}", err=True)
        raise typer.Exit(1) from e
</file>

<file path="space/os/spawn/cli/rename.py">
"""Rename agent command."""

import typer

from space.os.spawn import api


def rename(old_name: str, new_name: str):
    """Rename an agent."""
    try:
        if api.rename_agent(old_name, new_name):
            typer.echo(f"✓ Renamed {old_name} → {new_name}")
        else:
            typer.echo(f"❌ Agent not found: {old_name}. Run `spawn` to list agents.", err=True)
            raise typer.Exit(1)
    except ValueError as e:
        typer.echo(f"❌ {e}", err=True)
        raise typer.Exit(1) from e
</file>

<file path="space/os/spawn/cli/tasks.py">
import contextlib
import os
import signal
import time

import typer

from space.core.models import TaskStatus
from space.os.spawn.api import tasks

app = typer.Typer()


def list(
    status: str | None = None,
    role: str | None = None,
    all: bool = typer.Option(
        False, "--all", "-a", help="Show all tasks (including completed/failed)"
    ),
):
    """List tasks, optionally filtered by status and/or role.

    Default: Show pending and running tasks only.
    With --all/-a: Show all tasks including completed/failed/timeout.
    """
    if not all and status is None:
        status = "pending|running"

    if status and "|" in status:
        statuses = status.split("|")
        all_tasks = tasks.list_tasks(status=None, role=role)
        tasks_list = [t for t in all_tasks if t.status in statuses]
    else:
        tasks_list = tasks.list_tasks(status=status, role=role)

    if not tasks_list:
        typer.echo("No tasks.")
        return

    typer.echo(f"{'ID':<8} {'Identity':<12} {'Status':<12} {'Duration':<10} {'Created':<20}")
    typer.echo("-" * 70)

    durations = []
    for task in tasks_list:
        task_id = task.task_id[:8]
        ident = (task.agent_id or "unknown")[:11]
        stat = task.status
        dur = f"{task.duration:.1f}s" if task.duration else "-"
        created = task.created_at[:19] if task.created_at else "-"
        typer.echo(f"{task_id:<8} {ident:<12} {stat:<12} {dur:<10} {created:<20}")
        if task.duration:
            durations.append(task.duration)

    if durations:
        typer.echo("-" * 70)
        avg_dur = sum(durations) / len(durations)
        min_dur = min(durations)
        max_dur = max(durations)
        typer.echo(f"Avg: {avg_dur:.1f}s | Min: {min_dur:.1f}s | Max: {max_dur:.1f}s")


def logs(task_id: str):
    """Show full task details: input, output, stderr, timestamps, duration."""
    task = tasks.get_task(task_id)
    if not task:
        typer.echo(f"❌ Task not found: {task_id}", err=True)
        raise typer.Exit(1)
    if not task.agent_id:
        typer.echo(f"❌ Task has invalid agent_id: {task_id}", err=True)
        raise typer.Exit(1)

    typer.echo(f"\n📋 Task: {task.task_id}")
    typer.echo(f"Identity: {task.agent_id}")
    typer.echo(f"Status: {task.status}")

    if task.channel_id:
        typer.echo(f"Channel: {task.channel_id}")

    typer.echo(f"Created: {task.created_at}")
    if task.started_at:
        typer.echo(f"Started: {task.started_at}")
    if task.completed_at:
        typer.echo(f"Completed: {task.completed_at}")
    if task.duration is not None:
        typer.echo(f"Duration: {task.duration:.2f}s")

    typer.echo("\n--- Input ---")
    typer.echo(task.input)

    if task.output:
        typer.echo("\n--- Output ---")
        typer.echo(task.output)

    if task.stderr:
        typer.echo("\n--- Stderr ---")
        typer.echo(task.stderr)

    typer.echo()


def wait(task_id: str, timeout: float | None = None) -> int:
    """Block until task completes. Return exit code: 0=success, 1=failed, 124=timeout."""
    if timeout is None:
        timeout = 300

    task = tasks.get_task(task_id)
    if not task:
        typer.echo(f"❌ Task not found: {task_id}", err=True)
        raise typer.Exit(1)

    start = time.time()
    while True:
        task = tasks.get_task(task_id)
        if task.status in (TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.TIMEOUT):
            if task.status == TaskStatus.COMPLETED:
                return 0
            return 1

        if time.time() - start > timeout:
            tasks.fail_task(task_id, stderr="Wait timeout exceeded")
            raise typer.Exit(124)

        time.sleep(0.1)


def kill(task_id: str):
    """Kill a running task."""
    task = tasks.get_task(task_id)
    if not task:
        typer.echo(f"❌ Task not found: {task_id}", err=True)
        raise typer.Exit(1)

    if task.status in (TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.TIMEOUT):
        typer.echo(f"⚠️ Task already {task.status}, nothing to kill")
        return

    if task.pid:
        with contextlib.suppress(OSError, ProcessLookupError):
            os.kill(task.pid, signal.SIGTERM)

    tasks.fail_task(task_id, stderr="Killed by user")
    typer.echo(f"✓ Task {task_id[:8]} killed")


app.command()(list)
app.command()(logs)
app.command()(wait)
app.command()(kill)
</file>

<file path="space/os/spawn/cli/update.py">
"""Update agent command."""

import typer

from space.os.spawn import api


def update(
    identity: str,
    model: str = typer.Option(None, "--model", "-m", help="Full model name"),
    constitution: str = typer.Option(None, "--constitution", "-c", help="Constitution filename"),
):
    """Update agent fields."""
    try:
        api.update_agent(identity, constitution, model)
        typer.echo(f"✓ Updated {identity}")
    except ValueError as e:
        typer.echo(f"❌ {e}", err=True)
        raise typer.Exit(1) from e
</file>

<file path="space/os/spawn/constitutions/crucible.md">
# Crucible Constitution

**YOU ARE NOW CRUCIBLE.**

## Mandate
- Code with proper testing is liability
- Bloated test suites are entropy
- Test interfaces, not implementations
- Key metric: coverage:loc

## Principles
- Beautiful code reads like English
- Red, Green, Refactor: the only cycle
- Tests are specification; code satisfies specification
- Code without test is suggestion. Test or delete.

## Execution
- Map architecture first
- Challenge non-adherence
- Reduce to minimal testable behavior
- Refactor mercilessly, protected by tests

### Doctrine
- Test only contracts, protocols, core behavior
- Short declarative names: `test_creation` not `test_that_user_can_be_created`
- Functions, not classes
- Fixtures in `tests/conftest.py`
- Structure: `tests/integration/` (flat) + `tests/unit/` (mirrors src/)
- Kill all superfluous comments

**TEST FIRST. REFACTOR FEARLESS.**
</file>

<file path="space/os/spawn/defaults.py">
"""Spawn default identities and models."""

DEFAULT_MODEL = "claude-haiku-4-5"

DEFAULT_AGENT_MODELS: dict[str, str] = {
    "zealot": DEFAULT_MODEL,
    "sentinel": "gemini-2.5-pro",
    "crucible": "gpt-5-codex",
}


def canonical_model(identity: str) -> str:
    """Return canonical model for identity, falling back to DEFAULT_MODEL."""

    return DEFAULT_AGENT_MODELS.get(identity, DEFAULT_MODEL)
</file>

<file path="tests/integration/test_space_db_cascades.py">
from __future__ import annotations

from space.lib import store
from space.os import db as unified_db

TIMESTAMP = "2024-01-01T00:00:00.000000"


def test_channel_and_agent_cascades(test_space):
    unified_db.register()

    agent_id = "agent-alpha"
    channel_id = "channel-bridge"
    message_id = "message-1"
    bookmark_agent = agent_id
    task_id = "task-1"
    session_id = "session-1"
    parent_memory_id = "memory-parent"
    child_memory_id = "memory-child"
    link_id = "link-1"
    knowledge_id = "knowledge-1"

    with store.ensure("space") as conn:
        conn.execute(
            """
            INSERT INTO agents (agent_id, identity, model, created_at)
            VALUES (?, ?, ?, ?)
            """,
            (agent_id, "alpha", "gpt-4", TIMESTAMP),
        )
        conn.execute(
            """
            INSERT INTO channels (channel_id, name, created_at)
            VALUES (?, ?, ?)
            """,
            (channel_id, "Bridge", TIMESTAMP),
        )
        conn.execute(
            """
            INSERT INTO messages (message_id, channel_id, agent_id, content, created_at)
            VALUES (?, ?, ?, ?, ?)
            """,
            (message_id, channel_id, agent_id, "ping", TIMESTAMP),
        )
        conn.execute(
            """
            INSERT INTO bookmarks (agent_id, channel_id, last_seen_id)
            VALUES (?, ?, ?)
            """,
            (bookmark_agent, channel_id, message_id),
        )
        conn.execute(
            """
            INSERT INTO tasks (task_id, agent_id, channel_id, input, created_at)
            VALUES (?, ?, ?, ?, ?)
            """,
            (task_id, agent_id, channel_id, "run command", TIMESTAMP),
        )
        conn.execute(
            """
            INSERT INTO sessions (session_id, agent_id, spawn_number, started_at)
            VALUES (?, ?, ?, ?)
            """,
            (session_id, agent_id, 1, TIMESTAMP),
        )
        conn.execute(
            """
            INSERT INTO memories (memory_id, agent_id, topic, message, timestamp, created_at, bridge_channel)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (
                parent_memory_id,
                agent_id,
                "journal",
                "parent memory",
                TIMESTAMP,
                TIMESTAMP,
                channel_id,
            ),
        )
        conn.execute(
            """
            INSERT INTO memories (memory_id, agent_id, topic, message, timestamp, created_at, bridge_channel)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (child_memory_id, agent_id, "notes", "child memory", TIMESTAMP, TIMESTAMP, channel_id),
        )
        conn.execute(
            """
            UPDATE memories SET supersedes=?, superseded_by=? WHERE memory_id=?
            """,
            (parent_memory_id, child_memory_id, child_memory_id),
        )
        conn.execute(
            """
            INSERT INTO links (link_id, memory_id, parent_id, kind, created_at)
            VALUES (?, ?, ?, ?, ?)
            """,
            (link_id, child_memory_id, parent_memory_id, "derives", TIMESTAMP),
        )
        conn.execute(
            """
            INSERT INTO knowledge (knowledge_id, domain, agent_id, content, created_at)
            VALUES (?, ?, ?, ?, ?)
            """,
            (knowledge_id, "architecture", agent_id, "shared insight", TIMESTAMP),
        )

    with store.ensure("space") as conn:
        conn.execute("DELETE FROM channels WHERE channel_id=?", (channel_id,))
        assert conn.execute("SELECT COUNT(*) FROM messages").fetchone()[0] == 0
        assert (
            conn.execute("SELECT channel_id FROM tasks WHERE task_id=?", (task_id,)).fetchone()[0]
            is None
        )
        assert (
            conn.execute(
                "SELECT bridge_channel FROM memories WHERE memory_id=?", (parent_memory_id,)
            ).fetchone()[0]
            is None
        )
        assert (
            conn.execute(
                "SELECT bridge_channel FROM memories WHERE memory_id=?", (child_memory_id,)
            ).fetchone()[0]
            is None
        )
        assert conn.execute("SELECT COUNT(*) FROM bookmarks").fetchone()[0] == 0

    with store.ensure("space") as conn:
        conn.execute("DELETE FROM agents WHERE agent_id=?", (agent_id,))
        for table in ("tasks", "sessions", "memories", "knowledge"):
            assert conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0] == 0
        assert conn.execute("SELECT COUNT(*) FROM links").fetchone()[0] == 0
</file>

<file path="tests/unit/lib/store/test_health.py">
"""Tests for space.lib.store.health module."""

import sqlite3
import tempfile
from pathlib import Path

import pytest

from space.lib.store.health import (
    check_backup_has_data,
    compare_snapshots,
    get_backup_stats,
)


@pytest.fixture
def temp_backup_dir():
    """Create temporary directory for backup databases."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


def test_check_backup_has_data_nonexistent(temp_backup_dir):
    """Test check returns False for nonexistent backup."""
    result = check_backup_has_data(temp_backup_dir, "missing.db")
    assert result is False


def test_check_backup_has_data_empty(temp_backup_dir):
    """Test check returns False for database with no tables."""
    db_file = temp_backup_dir / "empty.db"
    conn = sqlite3.connect(db_file)
    conn.close()

    result = check_backup_has_data(temp_backup_dir, "empty.db")
    assert result is False


def test_check_backup_has_data_with_data(temp_backup_dir):
    """Test check returns True for database with data."""
    db_file = temp_backup_dir / "data.db"
    conn = sqlite3.connect(db_file)
    conn.execute("CREATE TABLE test (id INTEGER, value TEXT)")
    conn.execute("INSERT INTO test VALUES (1, 'hello')")
    conn.commit()
    conn.close()

    result = check_backup_has_data(temp_backup_dir, "data.db")
    assert result is True


def test_get_backup_stats_nonexistent(temp_backup_dir):
    """Test stats returns empty dict for nonexistent backup."""
    stats = get_backup_stats(temp_backup_dir, "missing.db")
    assert stats == {}


def test_get_backup_stats_with_data(temp_backup_dir):
    """Test stats returns table counts."""
    db_file = temp_backup_dir / "data.db"
    conn = sqlite3.connect(db_file)
    conn.execute("CREATE TABLE users (id INTEGER)")
    conn.execute("CREATE TABLE posts (id INTEGER)")
    conn.execute("INSERT INTO users VALUES (1)")
    conn.execute("INSERT INTO users VALUES (2)")
    conn.execute("INSERT INTO posts VALUES (1)")
    conn.commit()
    conn.close()

    stats = get_backup_stats(temp_backup_dir, "data.db")
    assert stats == {"users": 2, "posts": 1}


def test_compare_snapshots_no_change():
    """Test compare returns empty list when nothing changed."""
    before = {"db1": 100, "db2": 50}
    after = {"db1": 100, "db2": 50}

    warnings = compare_snapshots(before, after)
    assert warnings == []


def test_compare_snapshots_significant_loss():
    """Test compare detects significant data loss."""
    before = {"db1": 100}
    after = {"db1": 10}

    warnings = compare_snapshots(before, after, threshold=0.8)
    assert len(warnings) == 1
    assert "90%" in warnings[0]


def test_compare_snapshots_complete_loss():
    """Test compare detects complete database loss."""
    before = {"db1": 100}
    after = {"db1": 0}

    warnings = compare_snapshots(before, after)
    assert len(warnings) == 1
    assert "completely emptied" in warnings[0]


def test_compare_snapshots_new_db():
    """Test compare allows new databases."""
    before = {}
    after = {"db2": 50}

    warnings = compare_snapshots(before, after)
    assert warnings == []
</file>

<file path="tests/unit/lib/store/test_migrations.py">
"""Tests for space.lib.store.migrations module."""

import sqlite3
import tempfile
from pathlib import Path

import pytest

from space.lib.store import migrations


@pytest.fixture
def temp_db_dir():
    """Create temporary directory for test databases."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


def test_ensure_schema_creates_database(temp_db_dir):
    """Test ensure_schema creates database file."""
    db_path = temp_db_dir / "test.db"
    assert not db_path.exists()

    migrations.ensure_schema(db_path)

    assert db_path.exists()


def test_ensure_schema_applies_migrations(temp_db_dir):
    """Test ensure_schema applies migrations."""
    db_path = temp_db_dir / "test.db"
    migs = [
        ("init", "CREATE TABLE test (id TEXT PRIMARY KEY)"),
        ("v1", "ALTER TABLE test ADD COLUMN value TEXT"),
    ]

    migrations.ensure_schema(db_path, migs)

    conn = sqlite3.connect(db_path)
    cursor = conn.execute("PRAGMA table_info(test)")
    columns = {row[1] for row in cursor.fetchall()}
    assert "id" in columns
    assert "value" in columns
    conn.close()


def test_migrate_callable(temp_db_dir):
    """Test migrate with callable migration."""
    db_path = temp_db_dir / "test.db"

    def create_table(conn):
        conn.execute("CREATE TABLE test (id TEXT PRIMARY KEY)")

    migs = [("init", create_table)]
    migrations.ensure_schema(db_path, migs)

    conn = sqlite3.connect(db_path)
    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = {row[0] for row in cursor.fetchall()}
    assert "test" in tables
    conn.close()


def test_migrate_multiple_statements(temp_db_dir):
    """Test migrate with multiple SQL statements."""
    db_path = temp_db_dir / "test.db"
    sql = "CREATE TABLE t1 (id INT); CREATE TABLE t2 (id INT);"
    migs = [("init", sql)]

    migrations.ensure_schema(db_path, migs)

    conn = sqlite3.connect(db_path)
    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = {row[0] for row in cursor.fetchall()}
    assert "t1" in tables
    assert "t2" in tables
    conn.close()


def test_migrate_idempotent(temp_db_dir):
    """Test migrations are applied only once."""
    db_path = temp_db_dir / "test.db"
    migs = [("init", "CREATE TABLE test (id TEXT PRIMARY KEY)")]

    migrations.ensure_schema(db_path, migs)
    migrations.ensure_schema(db_path, migs)

    conn = sqlite3.connect(db_path)
    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = [row[0] for row in cursor.fetchall()]
    table_count = tables.count("test")
    assert table_count == 1
    conn.close()


def test_migrate_tracks_applied(temp_db_dir):
    """Test migrations are tracked in _migrations table."""
    db_path = temp_db_dir / "test.db"
    migs = [("v1", "CREATE TABLE test (id TEXT)")]

    migrations.ensure_schema(db_path, migs)

    conn = sqlite3.connect(db_path)
    cursor = conn.execute("SELECT name FROM _migrations")
    applied = {row[0] for row in cursor.fetchall()}
    assert "v1" in applied
    conn.close()


def test_migrate_data_loss_detection(temp_db_dir):
    """Test migration fails if data is lost unexpectedly."""
    db_path = temp_db_dir / "test.db"

    def drop_data(conn):
        conn.execute("DELETE FROM test")

    migs = [
        ("init", "CREATE TABLE test (id TEXT PRIMARY KEY)"),
        ("bad", drop_data),
    ]

    conn = sqlite3.connect(db_path)
    conn.execute("CREATE TABLE test (id TEXT PRIMARY KEY)")
    conn.execute("INSERT INTO test VALUES ('x')")
    conn.commit()
    conn.close()

    with pytest.raises(ValueError, match="rows lost"):
        migrations.migrate(sqlite3.connect(db_path), migs[1:])
</file>

<file path="tests/unit/lib/test_constitution.py">
"""Tests for constitution file management."""

import tempfile
from pathlib import Path
from unittest.mock import patch

import pytest

from space.lib.constitution import (
    PROVIDER_MAP,
    read_constitution,
    swap_constitution,
    write_constitution,
)


@pytest.fixture
def temp_home():
    """Fixture providing a temporary home directory."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


def test_write_constitution_claude(temp_home):
    """Write constitution creates correct path for claude provider."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        content = "# Zealot Constitution\nYou are zealous."
        result = write_constitution("claude", content)

        assert result == temp_home / ".claude" / "CLAUDE.md"
        assert result.exists()
        assert result.read_text() == content


def test_write_constitution_gemini(temp_home):
    """Write constitution creates correct path for gemini provider."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        content = "# Gemini Constitution"
        result = write_constitution("gemini", content)

        assert result == temp_home / ".gemini" / "GEMINI.md"
        assert result.exists()
        assert result.read_text() == content


def test_write_constitution_codex(temp_home):
    """Write constitution creates correct path for codex provider."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        content = "# Codex Constitution"
        result = write_constitution("codex", content)

        assert result == temp_home / ".codex" / "AGENTS.md"
        assert result.exists()
        assert result.read_text() == content


def test_write_constitution_invalid_provider():
    """Write constitution raises ValueError for unknown provider."""
    with pytest.raises(ValueError, match="Unknown provider"):
        write_constitution("unknown", "content")


def test_write_constitution_creates_parent_dirs(temp_home):
    """Write constitution creates parent directories."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        content = "test"
        write_constitution("claude", content)

        assert (temp_home / ".claude").exists()
        assert (temp_home / ".claude" / "CLAUDE.md").exists()


def test_read_constitution_claude(temp_home):
    """Read constitution retrieves correct file for claude."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        content = "# Zealot"
        write_constitution("claude", content)

        result = read_constitution("claude")
        assert result == content


def test_read_constitution_not_exists(temp_home):
    """Read constitution returns None if file doesn't exist."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        result = read_constitution("claude")
        assert result is None


def test_read_constitution_invalid_provider():
    """Read constitution raises ValueError for unknown provider."""
    with pytest.raises(ValueError, match="Unknown provider"):
        read_constitution("unknown")


def test_swap_constitution_stores_original(temp_home):
    """Swap constitution returns original content."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        original = "# Original"
        write_constitution("claude", original)

        new_content = "# New"
        returned = swap_constitution("claude", new_content)

        assert returned == original
        assert read_constitution("claude") == new_content


def test_swap_constitution_no_original(temp_home):
    """Swap constitution returns empty string if no original exists."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        new_content = "# New"
        returned = swap_constitution("claude", new_content)

        assert returned == ""
        assert read_constitution("claude") == new_content


def test_swap_constitution_round_trip(temp_home):
    """Swap constitution enables restore pattern."""
    with patch("space.lib.constitution.Path.home", return_value=temp_home):
        original = "# Zealot"
        write_constitution("claude", original)

        ephemeral = "# Pepper"
        saved = swap_constitution("claude", ephemeral)

        assert read_constitution("claude") == ephemeral

        swap_constitution("claude", saved)
        assert read_constitution("claude") == original


def test_provider_map_completeness():
    """PROVIDER_MAP contains all expected providers."""
    expected = {"claude", "gemini", "codex"}
    assert set(PROVIDER_MAP.keys()) == expected


def test_provider_map_values():
    """PROVIDER_MAP values are correct tuples."""
    assert PROVIDER_MAP["claude"] == ("CLAUDE.md", ".claude")
    assert PROVIDER_MAP["gemini"] == ("GEMINI.md", ".gemini")
    assert PROVIDER_MAP["codex"] == ("AGENTS.md", ".codex")
</file>

<file path="tests/unit/lib/test_ids.py">
"""Security tests for input validation in SQL ID resolution."""

import pytest

from space.lib import ids


def test_reject_table_injection():
    injections = [
        "messages'; DROP TABLE messages; --",
        "agents' UNION SELECT * FROM agents; --",
        "agents /**/",
        "agents; DELETE FROM agents; --",
    ]
    for table in injections:
        with pytest.raises(ValueError, match="Invalid"):
            ids.resolve_id(table, "agent_id", "abc")


def test_reject_column_injection():
    injections = [
        "agent_id); DROP TABLE agents; --",
        "agent_id) UNION SELECT * FROM agents; --",
    ]
    for col in injections:
        with pytest.raises(ValueError, match="Invalid"):
            ids.resolve_id("agents", col, "abc")


def test_reject_empty_identifiers():
    with pytest.raises(ValueError, match="Invalid table"):
        ids.resolve_id("", "agent_id", "abc")
    with pytest.raises(ValueError, match="Invalid column"):
        ids.resolve_id("agents", "", "abc")


def test_reject_whitespace_only():
    with pytest.raises(ValueError, match="Invalid table"):
        ids.resolve_id("   ", "agent_id", "abc")
    with pytest.raises(ValueError, match="Invalid column"):
        ids.resolve_id("agents", "   ", "abc")


def test_reject_sql_special_chars_table():
    invalid = [
        "agents' OR '1'='1",
        "agents`",
        'agents"',
        "agents\\",
        "agents;",
        "agents--",
        "agents/*",
    ]
    for table in invalid:
        with pytest.raises(ValueError, match="Invalid table"):
            ids.resolve_id(table, "agent_id", "abc")


def test_reject_sql_special_chars_column():
    invalid = [
        "agent_id' OR '1'='1",
        "agent_id`",
        'agent_id"',
        "agent_id;",
        "agent_id--",
        "agent_id/*",
    ]
    for col in invalid:
        with pytest.raises(ValueError, match="Invalid column"):
            ids.resolve_id("agents", col, "abc")


def test_reject_case_mismatch():
    with pytest.raises(ValueError, match="Invalid table"):
        ids.resolve_id("AGENTS", "agent_id", "abc")
    with pytest.raises(ValueError, match="Invalid column"):
        ids.resolve_id("agents", "AGENT_ID", "abc")
</file>

<file path="tests/unit/lib/test_query.py">
"""Unit tests for SQL query builder helpers."""

from unittest.mock import MagicMock

from space.lib import query


def test_agent_by_name_active_only():
    """agent_by_name filters archived by default."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = [{"agent_id": "id-1"}, {"agent_id": "id-2"}]

    result = query.agent_by_name(conn, "alice")
    assert result == ["id-1", "id-2"]

    call_args = conn.execute.call_args[0]
    assert "archived_at IS NULL" in call_args[0]


def test_agent_by_name_show_all():
    """agent_by_name includes archived when show_all=True."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = [{"agent_id": "id-1"}]

    result = query.agent_by_name(conn, "alice", show_all=True)
    assert result == ["id-1"]

    call_args = conn.execute.call_args[0]
    assert "archived_at" not in call_args[0]


def test_agent_by_id():
    """agent_by_id returns agent name."""
    conn = MagicMock()
    conn.execute.return_value.fetchone.return_value = {"name": "alice"}

    result = query.agent_by_id(conn, "id-1")
    assert result == "alice"


def test_agent_by_id_not_found():
    """agent_by_id returns None if agent not found."""
    conn = MagicMock()
    conn.execute.return_value.fetchone.return_value = None

    result = query.agent_by_id(conn, "missing-id")
    assert result is None


def test_count_table():
    """count_table returns row count."""
    conn = MagicMock()
    conn.execute.return_value.fetchone.return_value = (42,)

    result = query.count_table(conn, "agents")
    assert result == 42
    assert "SELECT COUNT(*)" in conn.execute.call_args[0][0]


def test_count_table_with_where():
    """count_table applies WHERE clause."""
    conn = MagicMock()
    conn.execute.return_value.fetchone.return_value = (5,)

    result = query.count_table(conn, "agents", where="status = 'active'")
    assert result == 5

    call_args = conn.execute.call_args[0][0]
    assert "WHERE status = 'active'" in call_args


def test_count_active():
    """count_active counts non-archived."""
    conn = MagicMock()
    conn.execute.return_value.fetchone.return_value = (10,)

    query.count_active(conn, "agents")
    call_args = conn.execute.call_args[0][0]
    assert "archived_at IS NULL" in call_args


def test_count_active_with_where():
    """count_active combines archived and where filters."""
    conn = MagicMock()
    conn.execute.return_value.fetchone.return_value = (5,)

    query.count_active(conn, "agents", where="type = 'sentinel'")
    call_args = conn.execute.call_args[0][0]
    assert "archived_at IS NULL" in call_args
    assert "type = 'sentinel'" in call_args


def test_count_archived():
    """count_archived counts archived only."""
    conn = MagicMock()
    conn.execute.return_value.fetchone.return_value = (3,)

    query.count_archived(conn, "agents")
    call_args = conn.execute.call_args[0][0]
    assert "archived_at IS NOT NULL" in call_args


def test_select_with_filter_all_columns():
    """select_with_filter selects all columns by default."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = [{"id": "1"}]

    query.select_with_filter(conn, "agents")
    call_args = conn.execute.call_args[0][0]
    assert "SELECT *" in call_args


def test_select_with_filter_specific_columns():
    """select_with_filter selects specific columns."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.select_with_filter(conn, "agents", columns="id, name")
    call_args = conn.execute.call_args[0][0]
    assert "SELECT id, name" in call_args


def test_select_with_filter_archive_filter():
    """select_with_filter filters archived by default."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.select_with_filter(conn, "agents", show_all=False)
    call_args = conn.execute.call_args[0][0]
    assert "archived_at IS NULL" in call_args


def test_select_with_filter_where_clause():
    """select_with_filter applies WHERE clause."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.select_with_filter(conn, "agents", where="type = ?")
    call_args = conn.execute.call_args[0][0]
    assert "WHERE" in call_args


def test_select_with_filter_order_by():
    """select_with_filter applies ORDER BY."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.select_with_filter(conn, "agents", order_by="ORDER BY created_at DESC")
    call_args = conn.execute.call_args[0][0]
    assert "ORDER BY created_at DESC" in call_args


def test_select_with_filter_limit():
    """select_with_filter applies LIMIT."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.select_with_filter(conn, "agents", limit=10)
    call_args = conn.execute.call_args[0][0]
    assert "LIMIT 10" in call_args


def test_select_distinct():
    """select_distinct returns distinct column values."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = [("alice",), ("bob",), ("charlie",)]

    result = query.select_distinct(conn, "agents", "name")
    assert result == ["alice", "bob", "charlie"]


def test_select_distinct_with_archive_filter():
    """select_distinct filters archived by default."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.select_distinct(conn, "agents", "name", show_all=False)
    call_args = conn.execute.call_args[0][0]
    assert "archived_at IS NULL" in call_args


def test_count_by_group():
    """count_by_group returns group and count tuples."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = [("alice", 5), ("bob", 3), ("charlie", 1)]

    result = query.count_by_group(conn, "tasks", "agent_id")
    assert result == [("alice", 5), ("bob", 3), ("charlie", 1)]


def test_count_by_group_with_where():
    """count_by_group applies WHERE filter."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.count_by_group(conn, "tasks", "agent_id", where="status = 'done'")
    call_args = conn.execute.call_args[0][0]
    assert "WHERE status = 'done'" in call_args
    assert "GROUP BY agent_id" in call_args


def test_count_by_group_with_limit():
    """count_by_group applies LIMIT."""
    conn = MagicMock()
    conn.execute.return_value.fetchall.return_value = []

    query.count_by_group(conn, "tasks", "agent_id", limit=5)
    call_args = conn.execute.call_args[0][0]
    assert "LIMIT ?" in call_args
    assert conn.execute.call_args[0][1] == (5,)


def test_update_where():
    """update_where executes UPDATE with parameterized values."""
    conn = MagicMock()

    query.update_where(
        conn, "agents", {"name": "alice", "status": "active"}, "agent_id = ?", ("id-1",)
    )

    call_args = conn.execute.call_args[0]
    qry = call_args[0]
    params = call_args[1]

    assert "UPDATE agents SET" in qry
    assert "name = ?" in qry
    assert "status = ?" in qry
    assert "WHERE agent_id = ?" in qry
    assert params == ("alice", "active", "id-1")


def test_update_where_empty_updates():
    """update_where does nothing if updates dict is empty."""
    conn = MagicMock()

    query.update_where(conn, "agents", {}, "agent_id = ?", ("id-1",))
    conn.execute.assert_not_called()


def test_delete_where():
    """delete_where executes DELETE with parameterized values."""
    conn = MagicMock()

    query.delete_where(conn, "agents", "agent_id = ?", ("id-1",))

    call_args = conn.execute.call_args[0]
    qry = call_args[0]
    params = call_args[1]

    assert "DELETE FROM agents" in qry
    assert "WHERE agent_id = ?" in qry
    assert params == ("id-1",)
</file>

<file path="tests/unit/lib/test_store.py">
"""Tests for space.lib.store utilities."""

import sqlite3
from dataclasses import dataclass

from space.lib.store import from_row


@dataclass
class SimpleEntity:
    """Test dataclass for from_row conversion."""

    id: str
    name: str
    value: int


def test_from_row_basic():
    """Test basic row to dataclass conversion."""
    conn = sqlite3.connect(":memory:")
    conn.row_factory = sqlite3.Row

    cursor = conn.cursor()
    cursor.execute("CREATE TABLE test (id TEXT, name TEXT, value INTEGER)")
    cursor.execute("INSERT INTO test VALUES (?, ?, ?)", ("1", "test", 42))

    row = cursor.execute("SELECT id, name, value FROM test").fetchone()
    entity = from_row(row, SimpleEntity)

    assert entity.id == "1"
    assert entity.name == "test"
    assert entity.value == 42


def test_from_row_partial_fields():
    """Test conversion when row has extra fields not in dataclass."""
    conn = sqlite3.connect(":memory:")
    conn.row_factory = sqlite3.Row

    cursor = conn.cursor()
    cursor.execute("CREATE TABLE test (id TEXT, name TEXT, value INTEGER, extra TEXT)")
    cursor.execute("INSERT INTO test VALUES (?, ?, ?, ?)", ("1", "test", 42, "ignored"))

    row = cursor.execute("SELECT id, name, value, extra FROM test").fetchone()
    entity = from_row(row, SimpleEntity)

    assert entity.id == "1"
    assert entity.name == "test"
    assert entity.value == 42


def test_from_row_missing_optional_fields():
    """Test conversion with optional fields."""

    @dataclass
    class OptionalEntity:
        id: str
        name: str
        description: str | None = None

    conn = sqlite3.connect(":memory:")
    conn.row_factory = sqlite3.Row

    cursor = conn.cursor()
    cursor.execute("CREATE TABLE test (id TEXT, name TEXT)")
    cursor.execute("INSERT INTO test VALUES (?, ?)", ("1", "test"))

    row = cursor.execute("SELECT id, name FROM test").fetchone()
    entity = from_row(row, OptionalEntity)

    assert entity.id == "1"
    assert entity.name == "test"
    assert entity.description is None
</file>

<file path="tests/unit/lib/test_sync.py">
"""Tests for chat sync: file copy/conversion with state tracking."""

import json
import tempfile
from pathlib import Path

from space.lib.sync import _gemini_json_to_jsonl


def test_gemini_json_to_jsonl():
    """Convert Gemini JSON to JSONL format."""
    gemini_json = {
        "sessionId": "test-123",
        "messages": [
            {"role": "user", "content": "Hello", "timestamp": "2025-01-01T00:00:00Z"},
            {"role": "model", "content": "Hi there", "timestamp": "2025-01-01T00:00:01Z"},
        ],
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(gemini_json, f)
        temp_path = Path(f.name)

    try:
        result = _gemini_json_to_jsonl(temp_path)
        lines = result.strip().split("\n")

        assert len(lines) == 2
        msg1 = json.loads(lines[0])
        msg2 = json.loads(lines[1])

        assert msg1["role"] == "user"
        assert msg1["content"] == "Hello"
        assert msg2["role"] == "assistant"
        assert msg2["content"] == "Hi there"
    finally:
        temp_path.unlink()


def test_gemini_json_to_jsonl_empty():
    """Handle empty Gemini JSON gracefully."""
    empty_json = {"sessionId": "test-456", "messages": []}

    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(empty_json, f)
        temp_path = Path(f.name)

    try:
        result = _gemini_json_to_jsonl(temp_path)
        assert result == ""
    finally:
        temp_path.unlink()


def test_gemini_json_to_jsonl_malformed():
    """Gracefully handle malformed Gemini JSON."""
    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        f.write("not valid json")
        temp_path = Path(f.name)

    try:
        result = _gemini_json_to_jsonl(temp_path)
        assert result == ""
    finally:
        temp_path.unlink()


def test_sync_state_persistence():
    """Load and save sync state tracking."""
    with tempfile.TemporaryDirectory() as tmpdir:
        state_file = Path(tmpdir) / "sync_state.json"

        state = {
            "claude_abc123": 1700000000.0,
            "gemini_def456": 1700000100.0,
        }

        state_file.parent.mkdir(parents=True, exist_ok=True)
        state_file.write_text(json.dumps(state, indent=2))

        loaded = json.loads(state_file.read_text())
        assert loaded["claude_abc123"] == 1700000000.0
        assert loaded["gemini_def456"] == 1700000100.0
</file>

<file path="tests/unit/os/bridge/test_stats.py">
"""Bridge stats API contract tests."""

from unittest.mock import MagicMock, patch

import pytest

from space.os import bridge


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


@pytest.fixture
def mock_db():
    conn = MagicMock()
    with patch("space.lib.store.ensure") as mock_ensure:
        mock_ensure.return_value.__enter__.return_value = conn
        mock_ensure.return_value.__exit__.return_value = None
        yield conn


def test_stats_returns_dict(mock_db):
    mock_db.execute.return_value.fetchone.return_value = (0,)
    mock_db.execute.return_value.fetchall.return_value = []

    result = bridge.stats()
    assert isinstance(result, dict)
    assert "channels" in result
    assert "messages" in result
</file>

<file path="tests/unit/os/knowledge/test_entries.py">
"""Knowledge entries API contract tests."""

from unittest.mock import MagicMock, patch

import pytest

from space.os import knowledge


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


@pytest.fixture
def mock_db():
    conn = MagicMock()
    with patch("space.lib.store.ensure") as mock_ensure:
        mock_ensure.return_value.__enter__.return_value = conn
        mock_ensure.return_value.__exit__.return_value = None
        yield conn


def test_add_entry_inserts_record(mock_db):
    knowledge.add_entry("ml", "agent-1", "content")
    assert mock_db.execute.call_count == 2

    # Check the first call (INSERT)
    insert_args = mock_db.execute.call_args_list[0][0]
    assert "INSERT INTO knowledge" in insert_args[0]
    assert insert_args[1][1] == "ml"
    assert insert_args[1][2] == "agent-1"
    assert insert_args[1][3] == "content"

    # Check the second call (UPDATE for touch_agent)
    update_args = mock_db.execute.call_args_list[1][0]
    assert "UPDATE agents SET last_active_at" in update_args[0]
    assert update_args[1][1] == "agent-1"


def test_add_entry_returns_id(mock_db):
    result = knowledge.add_entry("ml", "agent-1", "content")
    assert result is not None


def test_add_entry_with_confidence(mock_db):
    knowledge.add_entry("ml", "agent-1", "content", confidence=0.95)
    call_args = mock_db.execute.call_args_list[0]
    params = call_args[0][1]
    assert params[4] == 0.95


def test_list_entries_returns_list(mock_db):
    mock_row = make_mock_row(
        {
            "knowledge_id": "k-1",
            "domain": "ml",
            "agent_id": "a-1",
            "content": "test",
            "confidence": 0.9,
            "created_at": "2024-01-01",
            "archived_at": None,
        }
    )
    mock_db.execute.return_value.fetchall.return_value = [mock_row]
    result = knowledge.list_entries()
    assert len(result) == 1


def test_list_entries_excludes_archived(mock_db):
    mock_db.execute.return_value.fetchall.return_value = []
    knowledge.list_entries()
    args = mock_db.execute.call_args[0][0]
    assert "archived_at IS NULL" in args


def test_list_entries_show_all_includes_archived(mock_db):
    mock_db.execute.return_value.fetchall.return_value = []
    knowledge.list_entries(show_all=True)
    args = mock_db.execute.call_args[0][0]
    assert "archived_at IS NULL" not in args


def test_query_by_domain_filters(mock_db):
    mock_db.execute.return_value.fetchall.return_value = []
    knowledge.query_by_domain("ml")
    args = mock_db.execute.call_args[0]
    assert args[1][0] == "ml"


def test_query_by_agent_filters(mock_db):
    mock_db.execute.return_value.fetchall.return_value = []
    knowledge.query_by_agent("a-1")
    args = mock_db.execute.call_args[0]
    assert args[1][0] == "a-1"


def test_get_by_id_returns_entry(mock_db):
    mock_row = make_mock_row(
        {
            "knowledge_id": "k-1",
            "domain": "ml",
            "agent_id": "a-1",
            "content": "test",
            "confidence": 0.9,
            "created_at": "2024-01-01",
            "archived_at": None,
        }
    )
    mock_db.execute.return_value.fetchone.return_value = mock_row
    result = knowledge.get_by_id("k-1")
    assert result.knowledge_id == "k-1"


def test_get_by_id_missing_returns_none(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None
    result = knowledge.get_by_id("missing")
    assert result is None


def test_archive_entry_updates(mock_db):
    knowledge.archive_entry("k-1")
    assert mock_db.execute.called


def test_restore_entry_clears_timestamp(mock_db):
    knowledge.restore_entry("k-1")
    assert mock_db.execute.called


def test_find_related_returns_scores(mock_db):
    make_mock_row(
        {
            "knowledge_id": "k-1",
            "domain": "ml",
            "agent_id": "a-1",
            "content": "neural networks deep learning",
            "confidence": 0.9,
            "created_at": "2024-01-01",
            "archived_at": None,
        }
    )
    related_row = make_mock_row(
        {
            "knowledge_id": "k-2",
            "domain": "ai",
            "agent_id": "a-1",
            "content": "deep learning models",
            "confidence": 0.8,
            "created_at": "2024-01-02",
            "archived_at": None,
        }
    )
    mock_db.execute.return_value.fetchall.return_value = [related_row]

    from space.core.models import Knowledge

    entry = Knowledge(
        knowledge_id="k-1",
        domain="ml",
        agent_id="a-1",
        content="neural networks deep learning",
        confidence=0.9,
        created_at="2024-01-01",
        archived_at=None,
    )
    result = knowledge.find_related(entry)
    assert len(result) > 0
    assert isinstance(result[0], tuple)


def test_search_returns_structured_results(mock_db):
    with patch("space.os.spawn.get_agent") as mock_agent:
        mock_agent.return_value = MagicMock(agent_id="a-1", identity="agent1")
        mock_row = make_mock_row(
            {
                "knowledge_id": "k-1",
                "domain": "ml",
                "agent_id": "a-1",
                "content": "test",
                "created_at": "2024-01-01",
            }
        )
        mock_db.execute.return_value.fetchall.return_value = [mock_row]
        result = knowledge.search("test", None, True)
        assert len(result) == 1
        assert result[0]["source"] == "knowledge"


def test_search_filters_by_identity(mock_db):
    with patch("space.os.spawn.get_agent") as mock_agent:
        mock_agent.return_value = MagicMock(agent_id="a-1", identity="agent1")
        mock_db.execute.return_value.fetchall.return_value = []
        knowledge.search("test", "agent1", False)
        args = mock_db.execute.call_args[0]
        assert "AND agent_id = ?" in args[0]
</file>

<file path="tests/unit/os/knowledge/test_stats.py">
"""Knowledge stats API contract tests."""

from unittest.mock import MagicMock, patch

import pytest


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


@pytest.fixture
def mock_db():
    conn = MagicMock()
    with patch("space.lib.store.ensure") as mock_ensure:
        mock_ensure.return_value.__enter__.return_value = conn
        mock_ensure.return_value.__exit__.return_value = None
        yield conn
</file>

<file path="tests/unit/os/memory/test_entries.py">
"""Memory entries API contract tests."""

from unittest.mock import MagicMock, patch

import pytest

from space.os import memory


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


@pytest.fixture
def mock_db():
    conn = MagicMock()
    with patch("space.lib.store.ensure") as mock_ensure:
        mock_ensure.return_value.__enter__.return_value = conn
        mock_ensure.return_value.__exit__.return_value = None
        yield conn


@pytest.fixture
def mock_get_agent():
    with patch("space.os.spawn.get_agent") as mock:
        mock.return_value = MagicMock(agent_id="test-agent-id")
        yield mock


def test_add_entry_creates_record(mock_db):
    memory.add_entry("agent-123", "topic-x", "test message")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    mem_call = [call for call in calls if "INSERT INTO memories" in call][0]
    assert "INSERT INTO memories" in mem_call


def test_add_entry_with_core_flag(mock_db):
    memory.add_entry("agent-123", "topic", "message", core=True)

    calls = [
        call[0] for call in mock_db.execute.call_args_list if "INSERT INTO memories" in call[0][0]
    ]
    assert calls
    args = calls[0][1]
    assert args[6] == 1


def test_add_entry_returns_id(mock_db):
    result = memory.add_entry("agent-123", "topic", "message")
    assert result is not None


def test_list_entries_basic(mock_db, mock_get_agent):
    mock_row = make_mock_row(
        {
            "memory_id": "m-1",
            "agent_id": "agent-123",
            "topic": "t1",
            "message": "msg1",
            "timestamp": "2024-01-01",
            "created_at": 1234567890,
            "archived_at": None,
            "core": 0,
            "source": "manual",
            "bridge_channel": None,
            "code_anchors": None,
            "synthesis_note": None,
            "supersedes": None,
            "superseded_by": None,
        }
    )
    mock_db.execute.return_value.fetchall.return_value = [mock_row]

    result = memory.list_entries("agent-123")
    assert len(result) == 1


def test_list_entries_with_topic_filter(mock_db, mock_get_agent):
    mock_db.execute.return_value.fetchall.return_value = []

    memory.list_entries("test-agent-id", topic="insights")

    args = mock_db.execute.call_args[0]
    assert "AND topic = ?" in args[0]


def test_list_entries_with_core_filter(mock_db, mock_get_agent):
    mock_db.execute.return_value.fetchall.return_value = []

    memory.list_entries("test-agent-id", filter="core")

    args = mock_db.execute.call_args[0]
    assert "AND core = 1" in args[0]


def test_list_entries_excludes_archived_by_default(mock_db, mock_get_agent):
    mock_db.execute.return_value.fetchall.return_value = []

    memory.list_entries("test-agent-id")

    args = mock_db.execute.call_args[0]
    assert "AND archived_at IS NULL" in args[0]


def test_list_entries_shows_all_when_requested(mock_db, mock_get_agent):
    mock_db.execute.return_value.fetchall.return_value = []

    memory.list_entries("test-agent-id", show_all=True)

    args = mock_db.execute.call_args[0]
    assert "AND archived_at IS NULL" not in args[0]


def test_list_entries_agent_not_found_raises(mock_get_agent):
    mock_get_agent.return_value = None

    with pytest.raises(ValueError, match="not found"):
        memory.list_entries("nonexistent")


def test_replace_entry_archives_old(mock_db):
    with patch("space.os.memory.api.entries.resolve_id", return_value="m-1"):
        memory.replace_entry(["m-1"], "agent-123", "topic", "new msg")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("UPDATE memories SET archived_at" in call for call in calls)


def test_replace_entry_creates_new(mock_db):
    with patch("space.os.memory.api.entries.resolve_id", return_value="m-1"):
        memory.replace_entry(["m-1"], "agent-123", "topic", "new msg")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("INSERT INTO memories" in call for call in calls)


def test_replace_entry_returns_id(mock_db):
    with patch("space.os.memory.api.entries.resolve_id", return_value="m-1"):
        result = memory.replace_entry(["m-1"], "agent-123", "topic", "new msg")
    assert result is not None
</file>

<file path="tests/unit/os/test_protocols.py">
"""Unit tests for storage protocol contract."""

from unittest.mock import MagicMock

from space.core.protocols import Storage


def test_storage_protocol_connect():
    """Storage implements connect method."""
    storage = MagicMock(spec=Storage)
    storage.connect("path/to/db")
    storage.connect.assert_called_once_with("path/to/db")


def test_storage_protocol_ensure_schema():
    """Storage implements ensure_schema method."""
    storage = MagicMock(spec=Storage)
    storage.ensure_schema("path", "CREATE TABLE test (id TEXT)")
    storage.ensure_schema.assert_called_once()


def test_storage_protocol_ensure_schema_with_migrations():
    """Storage accepts migrations in ensure_schema."""
    storage = MagicMock(spec=Storage)
    migrations = [("v1", "CREATE TABLE test (id TEXT)")]
    storage.ensure_schema("path", "schema", migrations)
    assert storage.ensure_schema.called


def test_storage_protocol_register():
    """Storage implements register method."""
    storage = MagicMock(spec=Storage)
    storage.register("memory", "memory.db", "CREATE TABLE entries (...)")
    storage.register.assert_called_once()


def test_storage_protocol_migrations():
    """Storage implements migrations method."""
    storage = MagicMock(spec=Storage)
    migrations = [("v1", "ALTER TABLE...")]
    storage.migrations("memory", migrations)
    storage.migrations.assert_called_once()


def test_storage_protocol_ensure():
    """Storage implements ensure method."""
    storage = MagicMock(spec=Storage)
    storage.ensure("memory")
    storage.ensure.assert_called_once_with("memory")


def test_storage_protocol_migrate():
    """Storage implements migrate method."""
    storage = MagicMock(spec=Storage)
    conn = MagicMock()
    migrations = [("v1", "ALTER TABLE...")]
    storage.migrate(conn, migrations)
    storage.migrate.assert_called_once()


def test_storage_protocol_contract():
    """Storage protocol defines all required methods."""
    storage = MagicMock(spec=Storage)

    required_methods = ["connect", "ensure_schema", "register", "migrations", "ensure", "migrate"]
    for method in required_methods:
        assert hasattr(storage, method)
</file>

<file path="tests/unit/test_health_api.py">
from __future__ import annotations

import sqlite3

from space.apps.health import api as health_api
from space.lib import paths, store
from space.os import db as unified_db


def test_health_check_reports_ok(test_space):
    unified_db.register()

    ok, issues, counts = health_api.check_db()

    assert ok is True

    assert issues == []

    assert set(counts) == health_api.EXPECTED_TABLES

    assert all(count == 0 for count in counts.values())

    store.close_all()


def test_health_check_detects_missing_db(test_space):
    unified_db.register()
    store.close_all()

    db_path = paths.space_data() / health_api.DB_NAME
    db_path.unlink()

    ok, issues, counts = health_api.check_db()

    assert ok is False
    assert any("missing" in issue for issue in issues)
    assert counts == {}


def test_health_check_detects_fk_violation(test_space):
    unified_db.register()

    channel_id = "channel-1"
    message_id = "ghost-message"
    missing_agent = "agent-ghost"

    with store.ensure("space") as conn:
        conn.execute(
            """
            INSERT INTO channels (channel_id, name, created_at)
            VALUES (?, ?, STRFTIME('%Y-%m-%dT%H:%M:%f', 'now'))
            """,
            (
                channel_id,
                "Bridge",
            ),
        )

    store.close_all()

    db_path = paths.space_data() / health_api.DB_NAME
    raw = sqlite3.connect(db_path)
    try:
        raw.execute("PRAGMA foreign_keys = OFF")
        raw.execute(
            """
            INSERT INTO messages (message_id, channel_id, agent_id, content, created_at)
            VALUES (?, ?, ?, ?, STRFTIME('%Y-%m-%dT%H:%M:%f', 'now'))
            """,
            (
                message_id,
                channel_id,
                missing_agent,
                "orphan",
            ),
        )
        raw.commit()
    finally:
        raw.close()

    ok, issues, counts = health_api.check_db()

    assert ok is False
    assert any("messages" in issue for issue in issues)
    assert "messages" in counts and counts["messages"] >= 1
</file>

<file path="justfile">
default:
    @just --list

clean:
    @echo "Cleaning space-os..."
    @rm -rf dist build .pytest_cache .ruff_cache __pycache__ .venv
    @find . -type d -name "__pycache__" -exec rm -rf {} +

install:
    @poetry lock
    @poetry install

ci: format fix test build

test:
    @PYTHONPATH=$(pwd) poetry run python -m pytest tests -q

format:
    @poetry run ruff format .

lint:
    @poetry run ruff check .

fix:
    @poetry run ruff check . --fix --unsafe-fixes

build:
    @poetry build

commits:
    @git --no-pager log --pretty=format:"%h | %ar | %s"
</file>

<file path="space/apps/chats/api.py">
"""Chat operations: discover, sync, stats."""

from space.lib import paths
from space.lib import sync as lib_sync


def sync_all_providers() -> dict[str, tuple[int, int]]:
    """Sync chats from all providers to ~/.space/chats/.

    Returns:
        {provider_name: (sessions_discovered, files_synced)} for each provider
    """
    return lib_sync.sync_provider_chats()


def get_provider_stats() -> dict[str, dict]:
    """Get chat statistics across all providers.

    Returns:
        {provider_name: {"files": int, "size_mb": float}} for each provider
    """
    chats_dir = paths.chats_dir()
    stats = {}

    if not chats_dir.exists():
        return stats

    for provider_dir in chats_dir.iterdir():
        if not provider_dir.is_dir():
            continue

        provider_name = provider_dir.name
        files = list(provider_dir.rglob("*"))
        file_count = sum(1 for f in files if f.is_file())
        size_bytes = sum(f.stat().st_size for f in files if f.is_file())
        size_mb = size_bytes / (1024 * 1024)

        stats[provider_name] = {
            "files": file_count,
            "size_mb": size_mb,
        }

    return stats
</file>

<file path="space/apps/chats/cli.py">
"""Chat CLI commands."""

import typer

from space.apps.chats import api
from space.lib import paths

app = typer.Typer()


@app.callback(invoke_without_command=True)
def callback(ctx: typer.Context):
    if ctx.invoked_subcommand is None:
        ctx.invoke(stats)


@app.command()
def sync():
    """Sync chats from ~/.claude, ~/.codex, ~/.gemini to ~/.space/chats/."""
    results = api.sync_all_providers()

    typer.echo("✓ Chat sync complete")
    typer.echo()
    typer.echo(f"{'Provider':<10} {'Discovered':<12} {'Synced'}")
    typer.echo("-" * 40)

    total_discovered = 0
    total_synced = 0

    for provider in ("claude", "codex", "gemini"):
        discovered, synced = results.get(provider, (0, 0))
        total_discovered += discovered
        total_synced += synced
        status = "✓" if synced > 0 else "-"
        typer.echo(f"{provider:<10} {discovered:<12} {synced:<12} {status}")

    typer.echo("-" * 40)
    typer.echo(f"{'TOTAL':<10} {total_discovered:<12} {total_synced}")


@app.command()
def stats():
    """Show chat statistics across providers."""
    provider_stats = api.get_provider_stats()

    if not provider_stats:
        typer.echo("No chats synced yet. Run: space chats sync")
        return

    typer.echo("Chat statistics:")
    typer.echo()
    typer.echo(f"{'Provider':<12} {'Sessions':<12} {'Size (MB)'}")
    typer.echo("-" * 40)

    total_files = 0
    total_size = 0

    for provider in sorted(provider_stats.keys()):
        stats_dict = provider_stats[provider]
        total_files += stats_dict["files"]
        total_size += stats_dict["size_mb"]
        typer.echo(f"{provider:<12} {stats_dict['files']:<12} {stats_dict['size_mb']:>10.1f}")

    typer.echo("-" * 40)
    typer.echo(f"{'TOTAL':<12} {total_files:<12} {total_size:>10.1f}")
    typer.echo()
    typer.echo(f"Location: {paths.chats_dir()}")
</file>

<file path="space/apps/council/api.py">
"""Council application API surface."""

from space.apps.council.cli import (
    Colors,
    Council,
    _styled,
    format_error,
    format_header,
    format_message,
)

__all__ = [
    "Council",
    "Colors",
    "_styled",
    "format_error",
    "format_header",
    "format_message",
]
</file>

<file path="space/apps/council/cli.py">
"""Live bridge council - stream messages + type freely."""

import asyncio
import sys
from datetime import datetime

import typer
from prompt_toolkit import PromptSession
from prompt_toolkit.history import InMemoryHistory
from prompt_toolkit.patch_stdout import patch_stdout

from space.os import spawn
from space.os.bridge import api as bridge_api


class Colors:
    CYAN = "\033[36m"
    WHITE = "\033[37m"
    GRAY = "\033[90m"
    YELLOW = "\033[33m"
    BOLD = "\033[1m"
    RESET = "\033[0m"


def _styled(text: str, *colors: str) -> str:
    """Apply ANSI styles with automatic reset."""
    return f"{''.join(colors)}{text}{Colors.RESET}"


def format_message(msg, is_user: bool) -> str:
    """Return formatted council message line."""
    agent = spawn.get_agent(msg.agent_id)
    identity = agent.identity if agent else msg.agent_id
    ts = datetime.fromisoformat(msg.created_at).strftime("%H:%M:%S")

    if is_user:
        prefix = _styled(">", Colors.CYAN)
        ts_part = _styled(ts, Colors.CYAN)
        id_part = _styled(identity, Colors.BOLD)
        return f"{prefix} {ts_part} {id_part}: {msg.content}"

    ts_part = _styled(ts, Colors.WHITE)
    id_part = _styled(identity, Colors.BOLD)
    return f"{ts_part} {id_part}: {msg.content}"


def format_header(channel_name: str, topic: str | None = None) -> str:
    """Return formatted channel header."""
    title = _styled(f"📡 {channel_name}", Colors.BOLD, Colors.CYAN)
    lines = [f"\n{title}"]
    if topic:
        lines.append(f"   {_styled(topic, Colors.GRAY)}")
    lines.append("")
    return "\n".join(lines)


def format_error(msg: str) -> str:
    """Return formatted error output."""
    warn = _styled("⚠", Colors.YELLOW)
    return f"\n{warn}  {msg}\n"


STREAM_POLL_INTERVAL = 0.5
STREAM_ERROR_BACKOFF = 1.0


class Council:
    def __init__(self, channel_name: str):
        self.channel_name = channel_name
        self.channel_id = bridge_api.resolve_channel(channel_name).channel_id
        self.last_msg_id = None
        self.running = True
        self._lock = asyncio.Lock()
        self.sent_msg_ids = set()
        self.session = PromptSession(history=InMemoryHistory())
        self._last_printed_agent_id = None

    async def stream_messages(self):
        """Stream new messages from channel."""
        while self.running:
            try:
                msgs = bridge_api.get_messages(self.channel_id)
                if msgs:
                    start_idx = self._find_new_messages_start(msgs)
                    for msg in msgs[start_idx:]:
                        if msg.message_id not in self.sent_msg_ids:
                            async with self._lock:
                                self._print_message(msg)
                        self.last_msg_id = msg.message_id

                await asyncio.sleep(STREAM_POLL_INTERVAL)
            except Exception as e:
                async with self._lock:
                    self._print_error(f"Stream error: {e}")
                await asyncio.sleep(STREAM_ERROR_BACKOFF)

    def _find_new_messages_start(self, msgs: list) -> int:
        """Find index of first unprocessed message."""
        if not self.last_msg_id:
            return 0
        for i, msg in enumerate(msgs):
            if msg.message_id == self.last_msg_id:
                return i + 1
        return 0

    async def read_input(self):
        """Read user input asynchronously with prompt_toolkit."""
        loop = asyncio.get_event_loop()
        while self.running:
            try:
                with patch_stdout():
                    msg = await loop.run_in_executor(None, self.session.prompt, "> ")
                msg = msg.strip()
                if msg:
                    bridge_api.send_message(self.channel_id, "human", msg)
                    msgs = bridge_api.get_messages(self.channel_id)
                    if msgs:
                        self.sent_msg_ids.add(msgs[-1].message_id)
            except EOFError:
                self.running = False
            except Exception as e:
                self._print_error(f"Input error: {e}")

    def _print_message(self, msg):
        """Print a message with identity + timestamp (safe with prompt_toolkit)."""
        agent_id = msg.agent_id
        is_user = agent_id == "human"

        if self._should_add_separator(agent_id, is_user):
            print()

        print(format_message(msg, is_user))
        self._last_printed_agent_id = agent_id

    def _should_add_separator(self, agent_id: str, is_user: bool) -> bool:
        """Determine if a blank line should precede this message."""
        if not self._last_printed_agent_id:
            return False
        if is_user:
            return False
        return self._last_printed_agent_id != agent_id

    def _print_error(self, msg: str):
        """Print error to stderr."""
        print(format_error(msg), file=sys.stderr)

    async def run(self):
        """Main loop - stream + input."""
        try:
            channel = bridge_api.get_channel(self.channel_id)
            print(format_header(self.channel_name, channel.topic), end="")

            stream_task = asyncio.create_task(self.stream_messages())
            input_task = asyncio.create_task(self.read_input())

            await asyncio.gather(stream_task, input_task)
        except KeyboardInterrupt:
            print("\n")
        finally:
            self.running = False


app = typer.Typer()


@app.command()
def join(channel: str = typer.Argument(..., help="Channel name")):
    """Join a bridge council - stream messages and respond live."""
    c = Council(channel)
    asyncio.run(c.run())
</file>

<file path="space/apps/daemons/__init__.py">
"""Autonomous daemon swarms for space maintenance and upkeep."""

from .cli import app

__all__ = ["app"]
</file>

<file path="space/apps/daemons/api.py">
"""Daemon task API: create, schedule, manage autonomous swarms."""

from space.os.spawn.api import agents, tasks


def create_daemon_task(
    daemon_type: str,
    role: str = "zealot",
    channel_id: str | None = None,
) -> str:
    """Create a daemon task for a given type (upkeep, maintenance, etc).

    Args:
        daemon_type: Type of daemon task (upkeep, maintenance, verification, etc)
        role: Constitutional identity to run task (default: zealot)
        channel_id: Optional bridge channel for coordination

    Returns:
        task_id for tracking
    """
    agent = agents.get_agent(role)
    if not agent:
        raise ValueError(f"Agent '{role}' not found")

    task_input = f"Execute daemon task: {daemon_type}"
    return tasks.create_task(
        identity=role,
        input=task_input,
        channel_id=channel_id,
    )


def get_daemon_task(task_id: str):
    """Get daemon task status."""
    return tasks.get_task(task_id)


def list_daemon_tasks(status: str | None = None):
    """List all daemon tasks, optionally filtered by status."""
    return tasks.list_tasks(status=status)
</file>

<file path="space/lib/store/__init__.py">
"""Generic storage abstraction - database registry and lifecycle management."""

from space.lib.store.core import Row, ensure, from_row
from space.lib.store.health import (
    check_backup_has_data,
    compare_snapshots,
    get_backup_stats,
)
from space.lib.store.registry import (
    _reset_for_testing,
    add_migrations,
    alias,
    close_all,
    register,
    registry,
)
from space.lib.store.sqlite import connect, resolve

__all__ = [
    "ensure",
    "from_row",
    "Row",
    "register",
    "alias",
    "add_migrations",
    "registry",
    "_reset_for_testing",
    "close_all",
    "connect",
    "resolve",
    "check_backup_has_data",
    "get_backup_stats",
    "compare_snapshots",
]
</file>

<file path="space/lib/store/core.py">
"""Core store functionality - connection management."""

import sqlite3
from dataclasses import fields
from typing import Any, TypeVar

from space.lib import paths
from space.lib.store import migrations, registry
from space.lib.store.sqlite import connect

T = TypeVar("T")

Row = sqlite3.Row


def from_row(row: dict[str, Any] | Any, dataclass_type: type[T]) -> T:
    """Convert dict-like row to dataclass instance.

    Matches row keys to dataclass field names. Works with any dict-like object
    (sqlite3.Row, dict, etc.) allowing backend-agnostic conversions.
    """
    field_names = {f.name for f in fields(dataclass_type)}
    row_dict = dict(row) if not isinstance(row, dict) else row
    kwargs = {key: row_dict[key] for key in field_names if key in row_dict}
    return dataclass_type(**kwargs)


def ensure(name: str) -> sqlite3.Connection:
    """Ensure registered database exists and return connection.

    This is the main entry point - imports sqlite to establish connection.
    """
    db_file = registry.get_db_file(name)

    conn = registry.get_connection(name)
    if conn is not None:
        return conn

    db_path = paths.space_data() / db_file
    db_path.parent.mkdir(parents=True, exist_ok=True)
    migs = registry.get_migrations(name)
    migrations.ensure_schema(db_path, migs)

    conn = connect(db_path)
    registry.set_connection(name, conn)

    return conn
</file>

<file path="space/lib/store/health.py">
"""Database health checks and backup safeguards."""

import logging
import sqlite3
from pathlib import Path

logger = logging.getLogger(__name__)


def check_backup_has_data(backup_path: Path, db_name: str, min_rows: int = 1) -> bool:
    """Check if a backup database has actual data.

    Args:
            backup_path: Path to backup directory
            db_name: Database filename (e.g., 'space.db')
            min_rows: Minimum expected rows (excluding schema tables)

    Returns:
            True if data exists, False otherwise

    Raises:
            FileNotFoundError if backup doesn't exist
    """
    db_file = backup_path / db_name
    if not db_file.exists():
        return False

    try:
        conn = sqlite3.connect(db_file)
        cursor = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name != '_migrations'"
        )
        tables = [row[0] for row in cursor.fetchall()]

        if not tables:
            logger.warning(f"Backup {backup_path.name}/{db_name}: no data tables")
            conn.close()
            return False

        total_rows = 0
        for table in tables:
            result = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()
            count = result[0] if result else 0
            total_rows += count

        conn.close()

        if total_rows < min_rows:
            logger.warning(
                f"Backup {backup_path.name}/{db_name}: only {total_rows} rows (expected ≥{min_rows})"
            )
            return False

        logger.info(f"Backup {backup_path.name}/{db_name}: {total_rows} rows ✓")
        return True

    except sqlite3.DatabaseError as e:
        logger.error(f"Backup {backup_path.name}/{db_name}: corrupted - {e}")
        return False


def get_backup_stats(backup_path: Path, db_name: str) -> dict:
    """Get row counts for all tables in backup database.

    Returns:
            Dict like {'table_name': row_count, ...}
    """
    db_file = backup_path / db_name
    if not db_file.exists():
        return {}

    try:
        conn = sqlite3.connect(db_file)
        cursor = conn.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name != '_migrations'"
        )
        tables = [row[0] for row in cursor.fetchall()]

        stats = {}
        for table in tables:
            result = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()
            stats[table] = result[0] if result else 0

        conn.close()
        return stats

    except sqlite3.DatabaseError:
        return {}


def compare_snapshots(before: dict, after: dict, threshold: float = 0.8) -> list[str]:
    """Compare before/after snapshots and find concerning changes.

    Args:
            before: {'db_name': row_count, ...}
            after: {'db_name': row_count, ...}
            threshold: Alert if any DB lost more than this fraction (0.8 = 80%)

    Returns:
            List of warning messages
    """
    warnings = []

    for db_name, before_count in before.items():
        after_count = after.get(db_name, 0)
        if before_count == 0:
            continue

        if after_count == 0:
            warnings.append(f"{db_name}: completely emptied ({before_count} → 0 rows)")
        else:
            loss_pct = (before_count - after_count) / before_count
            if loss_pct > threshold:
                warnings.append(
                    f"{db_name}: {loss_pct * 100:.0f}% data loss ({before_count} → {after_count} rows)"
                )

    return warnings
</file>

<file path="space/lib/store/registry.py">
"""Database registry and lifecycle management."""

import threading
from collections.abc import Callable
from typing import TypeVar

T = TypeVar("T")

_registry: dict[str, str] = {}
_migrations: dict[str, list[tuple[str, str | Callable]]] = {}
_aliases: dict[str, str] = {}
_connections = threading.local()


def _canonical(name: str) -> str:
    """Resolve registry alias to its canonical target."""
    visited: set[str] = set()
    target = name
    while target in _aliases:
        if target in visited:
            raise ValueError(f"Circular alias detected for database '{name}'")
        visited.add(target)
        target = _aliases[target]
    return target


def register(name: str, db_file: str) -> None:
    """Register database in global registry.

    Args:
            name: Database identifier
            db_file: Filename for database
    """
    canonical_name = _canonical(name)
    _registry[canonical_name] = db_file


def alias(name: str, target: str) -> None:
    """Alias database name to an existing canonical target."""
    canonical_target = _canonical(target)
    if canonical_target not in _registry:
        raise ValueError(f"Cannot alias '{name}' → '{target}': target not registered")
    _aliases[name] = canonical_target


def add_migrations(name: str, migs: list[tuple[str, str | Callable]]) -> None:
    """Register migrations for database."""
    canonical_name = _canonical(name)
    _migrations[canonical_name] = migs


def get_db_file(name: str) -> str:
    """Get registered database filename."""
    canonical_name = _canonical(name)
    if canonical_name not in _registry:
        raise ValueError(f"Database '{name}' not registered. Call register() first.")
    return _registry[canonical_name]


def get_migrations(name: str) -> list[tuple[str, str | Callable]] | None:
    """Get migrations for database."""
    canonical_name = _canonical(name)
    return _migrations.get(canonical_name)


def registry() -> dict[str, str]:
    """Return registry of all registered databases (aliases included)."""
    items = dict(_registry.items())
    for alias_name, canonical_name in _aliases.items():
        items[alias_name] = _registry.get(canonical_name, "")
    return items


def get_connection(name: str):
    """Get cached connection for database."""
    canonical_name = _canonical(name)
    return getattr(_connections, canonical_name, None)


def set_connection(name: str, conn) -> None:
    """Cache connection for database."""
    canonical_name = _canonical(name)
    setattr(_connections, canonical_name, conn)


def _reset_for_testing() -> None:
    """Reset registry and migrations state (test-only)."""
    _registry.clear()
    _migrations.clear()
    _aliases.clear()
    if hasattr(_connections, "__dict__"):
        for conn in _connections.__dict__.values():
            conn.close()
        _connections.__dict__.clear()


def close_all() -> None:
    """Close all managed database connections."""
    if hasattr(_connections, "__dict__"):
        for conn in _connections.__dict__.values():
            conn.close()
        _connections.__dict__.clear()
</file>

<file path="space/lib/store/sqlite.py">
"""SQLite storage backend implementation."""

import contextlib
import logging
import sqlite3
from pathlib import Path

logger = logging.getLogger(__name__)


def connect(db_path: Path) -> sqlite3.Connection:
    """Open connection to SQLite database."""
    conn = sqlite3.connect(db_path, check_same_thread=False)
    conn.row_factory = sqlite3.Row
    conn.isolation_level = None
    conn.execute("PRAGMA foreign_keys = ON")
    conn.execute("PRAGMA journal_mode = WAL")
    return conn


def resolve(db_dir: Path) -> None:
    """Resolve WAL files by checkpointing all databases in directory.

    Merges WAL (Write-Ahead Logging) data into main database files,
    creating complete, standalone snapshots suitable for backup/transfer.

    Args:
            db_dir: Directory containing *.db files
    """
    for db_file in sorted(db_dir.glob("*.db")):
        try:
            conn = connect(db_file)
            conn.execute("PRAGMA journal_mode=DELETE")
            conn.execute("PRAGMA wal_checkpoint(RESTART)")
            conn.close()

            for artifact in db_file.parent.glob(f"{db_file.name}-*"):
                with contextlib.suppress(OSError):
                    artifact.unlink()

            logger.info(f"Resolved {db_file.name}")
        except sqlite3.DatabaseError as e:
            logger.warning(f"Failed to resolve {db_file.name}: {e}")
</file>

<file path="space/lib/constitution.py">
"""Constitution file management for AI providers."""

from pathlib import Path

PROVIDER_MAP = {
    "claude": ("CLAUDE.md", ".claude"),
    "gemini": ("GEMINI.md", ".gemini"),
    "codex": ("AGENTS.md", ".codex"),
}


def write_constitution(provider: str, content: str) -> Path:
    """Write constitution to provider home dir.

    Args:
        provider: Provider name (claude, gemini, codex)
        content: Constitution text to write

    Returns:
        Path to written file

    Raises:
        ValueError: If provider is unknown
    """
    if provider not in PROVIDER_MAP:
        raise ValueError(f"Unknown provider: {provider}")

    filename, agent_dir = PROVIDER_MAP[provider]
    target = Path.home() / agent_dir / filename
    target.parent.mkdir(parents=True, exist_ok=True)
    target.write_text(content)
    return target


def read_constitution(provider: str) -> str | None:
    """Read constitution from provider home dir.

    Args:
        provider: Provider name (claude, gemini, codex)

    Returns:
        Constitution text or None if file doesn't exist

    Raises:
        ValueError: If provider is unknown
    """
    if provider not in PROVIDER_MAP:
        raise ValueError(f"Unknown provider: {provider}")

    filename, agent_dir = PROVIDER_MAP[provider]
    target = Path.home() / agent_dir / filename
    return target.read_text() if target.exists() else None


def swap_constitution(provider: str, new_content: str) -> str:
    """Temporarily swap constitution content.

    Returns original content so it can be restored.

    Args:
        provider: Provider name (claude, gemini, codex)
        new_content: Constitution text to write

    Returns:
        Original constitution content (or empty string if none existed)

    Raises:
        ValueError: If provider is unknown
    """
    original = read_constitution(provider) or ""
    write_constitution(provider, new_content)
    return original


__all__ = [
    "write_constitution",
    "read_constitution",
    "swap_constitution",
    "PROVIDER_MAP",
]
</file>

<file path="space/lib/errors.py">
"""Error capture for CLI commands."""

import sys


def install_error_handler(source: str):
    """Install sys.excepthook to log uncaught exceptions."""
    original_hook = sys.excepthook

    def error_hook(exc_type, exc_value, exc_traceback):
        if exc_type.__name__ not in ("Exit", "Abort", "KeyboardInterrupt"):
            f"{exc_type.__name__}: {str(exc_value)}"

        original_hook(exc_type, exc_value, exc_traceback)

    sys.excepthook = error_hook


def log_error(source: str, agent_id: str | None, error: Exception, command: str = ""):
    """Log error to events with agent context and command."""
    if command:
        f"{command}: {type(error).__name__}: {str(error)}"
    else:
        f"{type(error).__name__}: {str(error)}"
</file>

<file path="space/lib/ids.py">
from __future__ import annotations

from space.lib import store

_VALID_TABLES = {"agents", "channels", "memories", "knowledge", "messages", "events", "memory"}
_VALID_COLUMNS = {
    "agent_id",
    "channel_id",
    "message_id",
    "memory_id",
    "knowledge_id",
    "event_id",
    "note_id",
    "task_id",
}


def resolve_id(table: str, id_col: str, partial_id: str, *, error_context: str = "") -> str:
    """Resolve a partial/suffix ID to full ID via fuzzy matching.

    Args:
        table: Table name to query (validated against whitelist)
        id_col: Column name containing the IDs (validated against whitelist)
        partial_id: Partial ID (suffix match)
        error_context: Additional context for error messages

    Returns:
        Full ID if unambiguous match found

    Raises:
        ValueError: If no match, ambiguous matches, or invalid identifiers
    """
    if table not in _VALID_TABLES:
        raise ValueError(f"Invalid table: {table}")
    if id_col not in _VALID_COLUMNS:
        raise ValueError(f"Invalid column: {id_col}")

    table_map = {"memory": "memories"}
    actual_table = table_map.get(table, table)
    registry_map = {"memory": "memory", "memories": "memory"}
    registry_name = registry_map.get(table, table)
    with store.ensure(registry_name) as conn:
        rows = conn.execute(
            f"SELECT {id_col} FROM {actual_table} WHERE {id_col} LIKE ?",
            (f"%{partial_id}",),
        ).fetchall()

    if not rows:
        msg = f"No entry found with ID ending in '{partial_id}'"
        if error_context:
            msg += f" ({error_context})"
        raise ValueError(msg)

    if len(rows) > 1:
        ambiguous_ids = [row[0] for row in rows]
        msg = f"Ambiguous ID: '{partial_id}' matches multiple entries: {ambiguous_ids}"
        if error_context:
            msg += f" ({error_context})"
        raise ValueError(msg)

    return rows[0][0]
</file>

<file path="space/lib/output.py">
import json as json_lib

import typer


def set_flags(ctx: typer.Context, json_output: bool = False, quiet_output: bool = False) -> None:
    """Set output flags in context."""
    if ctx.obj is None or not isinstance(ctx.obj, dict):
        ctx.obj = {}
    ctx.obj["json_output"] = json_output
    ctx.obj["quiet_output"] = quiet_output


def out_json(data: dict) -> str:
    """Format dict as JSON string."""
    return json_lib.dumps(data, indent=2)


def out_text(msg: str, ctx_obj: dict | None = None) -> None:
    """Echo text unless quiet mode."""
    if ctx_obj and ctx_obj.get("quiet_output"):
        return
    typer.echo(msg)


def emit_error(module: str, agent_id: str | None, cmd: str, exc: Exception | str) -> None:
    """Standardized error event emission."""
    str(exc) if isinstance(exc, Exception) else exc
</file>

<file path="space/os/bridge/api/messaging.py">
"""Message operations: send, receive, alerts, bookmarks."""

import sqlite3

from space.core.models import Channel, Message
from space.lib import store
from space.lib.store import from_row
from space.lib.uuid7 import uuid7


def _row_to_message(row: store.Row) -> Message:
    return from_row(row, Message)


def _to_channel_id(channel: str | Channel) -> str:
    """Extract channel_id from Channel object or return string as-is."""
    return channel.channel_id if isinstance(channel, Channel) else channel


def send_message(channel: str | Channel, identity: str, content: str) -> str:
    """Send message. Returns agent_id."""
    from space.os import spawn

    channel_id = _to_channel_id(channel)
    if not identity:
        raise ValueError("identity is required")
    if not channel_id:
        raise ValueError("channel_id is required")
    agent = spawn.get_agent(identity)
    if not agent:
        raise ValueError(f"Identity '{identity}' not registered.")
    agent_id = agent.agent_id
    message_id = uuid7()
    with store.ensure("bridge") as conn:
        conn.execute(
            "INSERT INTO messages (message_id, channel_id, agent_id, content) VALUES (?, ?, ?, ?)",
            (message_id, channel_id, agent_id, content),
        )
    spawn.api.touch_agent(agent_id)
    return agent_id


def _build_pagination_query_and_params(
    conn: sqlite3.Connection, channel_id: str, last_seen_id: str | None, base_query: str
) -> tuple[str, tuple]:
    """Builds the SQL query and parameters for message pagination."""
    if last_seen_id is None:
        # If no last_seen_id, fetch all messages for the channel, ordered by creation time.
        query = f"{base_query} ORDER BY m.created_at"
        params = (channel_id,)
    else:
        # If last_seen_id is provided, find the message it refers to to get its created_at and rowid.
        last_seen_row = conn.execute(
            "SELECT created_at, rowid FROM messages WHERE message_id = ?", (last_seen_id,)
        ).fetchone()
        if last_seen_row:
            # Fetch messages created after the last seen message,
            # or messages created at the same time but with a greater rowid (for stable ordering).
            query = f"{base_query} AND (m.created_at > ? OR (m.created_at = ? AND m.rowid > ?)) ORDER BY m.created_at, m.rowid"
            params = (
                channel_id,
                last_seen_row["created_at"],
                last_seen_row["created_at"],
                last_seen_row["rowid"],
            )
        else:
            # If last_seen_id is invalid or not found, fall back to fetching all messages.
            query = f"{base_query} ORDER BY m.created_at"
            params = (channel_id,)
    return query, params


def get_messages(channel: str | Channel, agent_id: str | None = None) -> list[Message]:
    """Get messages, optionally filtering for new messages for a given agent."""
    channel_id = _to_channel_id(channel)
    with store.ensure("bridge") as conn:
        from . import channels

        channel_obj = channels.get_channel(channel_id)
        if not channel_obj:
            raise ValueError(f"Channel {channel_id} not found")

        actual_channel_id = channel_obj.channel_id

        last_seen_id = None
        if agent_id:
            row = conn.execute(
                "SELECT last_seen_id FROM bookmarks WHERE agent_id = ? AND channel_id = ?",
                (agent_id, actual_channel_id),
            ).fetchone()
            last_seen_id = row["last_seen_id"] if row else None

        base_query = """
            SELECT m.message_id, m.channel_id, m.agent_id, m.content, m.created_at
            FROM messages m
            JOIN channels c ON m.channel_id = c.channel_id
            WHERE m.channel_id = ? AND c.archived_at IS NULL
        """

        query, params = _build_pagination_query_and_params(
            conn, actual_channel_id, last_seen_id, base_query
        )

        cursor = conn.execute(query, params)
        return [_row_to_message(row) for row in cursor.fetchall()]


def get_sender_history(identity: str, limit: int = 5) -> list[Message]:
    from space.os import spawn

    agent = spawn.get_agent(identity)
    if not agent:
        raise ValueError(f"Identity '{identity}' not registered.")
    agent_id = agent.agent_id
    with store.ensure("bridge") as conn:
        cursor = conn.execute(
            """
            SELECT m.message_id, m.channel_id, m.agent_id, m.content, m.created_at
            FROM messages m
            WHERE m.agent_id = ?
            ORDER BY m.created_at DESC
            LIMIT ?
            """,
            (agent_id, limit),
        )
        return [_row_to_message(row) for row in cursor.fetchall()]


def set_bookmark(agent_id: str, channel: str | Channel, last_seen_id: str) -> None:
    """Mark message as read for agent."""
    channel_id = _to_channel_id(channel)
    with store.ensure("bridge") as conn:
        conn.execute(
            "INSERT OR REPLACE INTO bookmarks (agent_id, channel_id, last_seen_id) VALUES (?, ?, ?)",
            (agent_id, channel_id, last_seen_id),
        )


def recv_messages(
    channel: str | Channel, identity: str
) -> tuple[list[Message], int, str | None, list[str]]:
    """Receive new messages and update bookmark."""

    from space.os import spawn

    from . import channels

    channel_id = _to_channel_id(channel)

    agent = spawn.get_agent(identity)
    if not agent:
        raise ValueError(f"Identity '{identity}' not registered.")
    agent_id = agent.agent_id

    messages = get_messages(channel_id, agent_id)

    unread_count = len(messages)

    if messages:
        set_bookmark(agent_id, channel_id, messages[-1].message_id)

    channel = channels.get_channel(channel_id)

    topic = channel.topic if channel else None

    members = channel.members if channel else []

    return messages, unread_count, topic, members
</file>

<file path="space/os/bridge/api/search.py">
"""Bridge search: unified query interface."""

from space.lib import store
from space.os import spawn


def search(query: str, identity: str | None, all_agents: bool) -> list[dict]:
    """Search bridge messages by query, optionally filtering by agent, returning structured results with references."""
    results = []

    with store.ensure("bridge") as conn:
        sql_query = (
            "SELECT m.message_id, m.channel_id, m.agent_id, m.content, m.created_at, c.name as channel_name "
            "FROM messages m JOIN channels c ON m.channel_id = c.channel_id "
            "WHERE (m.content LIKE ? OR c.name LIKE ?)"
        )

        params = [f"%{query}%", f"%{query}%"]

        if identity and not all_agents:
            agent = spawn.get_agent(identity)

            if not agent:
                raise ValueError(f"Agent '{identity}' not found")

            sql_query += " AND m.agent_id = ?"

            params.append(agent.agent_id)

        sql_query += " ORDER BY m.created_at ASC"

        rows = conn.execute(sql_query, params).fetchall()

        for row in rows:
            agent = spawn.get_agent(row["agent_id"])

            results.append(
                {
                    "source": "bridge",
                    "channel_name": row["channel_name"],
                    "message_id": row["message_id"],
                    "sender": agent.identity if agent else row["agent_id"],
                    "content": row["content"],
                    "timestamp": row["created_at"],
                    "reference": f"bridge:{row['channel_name']}:{row['message_id']}",
                }
            )

    return results
</file>

<file path="space/os/bridge/api/stats.py">
"""Bridge stats aggregation: messages, channels, and events."""

from space.lib import store


def stats() -> dict:
    """Get bridge statistics: messages, channels, and events by agent."""
    with store.ensure("bridge") as conn:
        total_msgs = conn.execute("SELECT COUNT(*) FROM messages").fetchone()[0]
        archived_msgs = conn.execute(
            "SELECT COUNT(*) FROM messages WHERE channel_id IN (SELECT channel_id FROM channels WHERE archived_at IS NOT NULL)"
        ).fetchone()[0]
        active_msgs = total_msgs - archived_msgs

        total_channels = conn.execute("SELECT COUNT(*) FROM channels").fetchone()[0]
        active_channels = conn.execute(
            "SELECT COUNT(*) FROM channels WHERE archived_at IS NULL"
        ).fetchone()[0]
        archived_channels = total_channels - active_channels

        distinct_channels = conn.execute(
            "SELECT COUNT(DISTINCT channel_id) FROM messages WHERE channel_id IS NOT NULL"
        ).fetchone()[0]

        msg_by_agent = conn.execute(
            "SELECT agent_id, COUNT(*) as count FROM messages GROUP BY agent_id ORDER BY count DESC"
        ).fetchall()

    try:
        with store.ensure("events") as conn:
            rows = conn.execute(
                "SELECT agent_id, event_type, timestamp FROM events ORDER BY timestamp"
            ).fetchall()

            agent_events: dict[str, dict] = {}
            total_events = len(rows)

            for row in rows:
                agent_id = row[0]
                event_type = row[1]
                timestamp = row[2]

                if agent_id not in agent_events:
                    agent_events[agent_id] = {
                        "events": 0,
                        "spawns": 0,
                        "last_active": None,
                    }

                agent_events[agent_id]["events"] += 1
                if event_type == "session_start":
                    agent_events[agent_id]["spawns"] += 1
                agent_events[agent_id]["last_active"] = timestamp

            events_by_agent = [
                {"agent_id": agent_id, **data} for agent_id, data in agent_events.items()
            ]
    except Exception:
        total_events = 0
        events_by_agent = []

    return {
        "messages": {
            "total": total_msgs,
            "active": active_msgs,
            "archived": archived_msgs,
            "by_agent": [{"agent_id": row[0], "count": row[1]} for row in msg_by_agent],
        },
        "channels": {
            "total": distinct_channels,
            "active": active_channels,
            "archived": archived_channels,
        },
        "events": {
            "total": total_events,
            "by_agent": events_by_agent,
        },
    }
</file>

<file path="space/os/bridge/cli/export.py">
"""Export channel messages."""

from __future__ import annotations

import typer

from space.os.bridge import api

from .format import echo_if_output, output_json


def register(app: typer.Typer) -> None:
    @app.command()
    def export(
        ctx: typer.Context,
        channel: str = typer.Argument(..., help="Channel name or ID"),
    ):
        """Export channel messages as markdown."""
        try:
            export_data = api.export_channel(channel)

            if output_json(
                {
                    "channel_id": export_data.channel_id,
                    "channel_name": export_data.channel_name,
                    "topic": export_data.topic,
                    "created_at": export_data.created_at,
                    "members": export_data.members,
                    "message_count": export_data.message_count,
                    "messages": [
                        {
                            "message_id": msg.message_id,
                            "agent_id": msg.agent_id,
                            "content": msg.content,
                            "created_at": msg.created_at,
                        }
                        for msg in export_data.messages
                    ],
                },
                ctx,
            ):
                return

            echo_if_output(f"# {export_data.channel_name}", ctx)
            if export_data.topic:
                echo_if_output(f"\n**Topic:** {export_data.topic}", ctx)

            echo_if_output(f"\n## Messages ({export_data.message_count})", ctx)
            for msg in export_data.messages:
                echo_if_output(f"\n**{msg.agent_id}** ({msg.created_at}):", ctx)
                echo_if_output(msg.content, ctx)

        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/bridge/cli/format.py">
"""CLI output formatting and helpers."""

import json
from datetime import datetime

import typer


def format_channel_row(channel) -> tuple[str, str]:
    """Format channel for display. Returns (last_activity, description)."""
    if channel.last_activity:
        last_activity = datetime.fromisoformat(channel.last_activity).strftime("%Y-%m-%d")
    else:
        last_activity = "never"

    parts = []
    if channel.message_count:
        parts.append(f"{channel.message_count} msgs")
    if channel.members:
        parts.append(f"{len(channel.members)} members")
    meta_str = " | ".join(parts)
    channel_id_suffix = channel.channel_id[-8:] if channel.channel_id else ""

    if meta_str:
        return last_activity, f"{channel.name} ({channel_id_suffix}) - {meta_str}"
    return last_activity, f"{channel.name} ({channel_id_suffix})"


def format_local_time(timestamp: str) -> str:
    """Format ISO timestamp as readable local time."""
    try:
        dt = datetime.fromisoformat(timestamp)
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, TypeError):
        return timestamp


def output_json(data, ctx: typer.Context):
    """Output data as JSON if requested. Returns True if output, False otherwise."""
    if ctx.obj.get("json_output"):
        typer.echo(json.dumps(data, indent=2))
        return True
    return False


def should_output(ctx: typer.Context) -> bool:
    """Check if output should be printed (not quiet mode)."""
    return not ctx.obj.get("quiet_output")


def echo_if_output(msg: str, ctx: typer.Context):
    """Echo message only if not in quiet mode."""
    if should_output(ctx):
        typer.echo(msg)
</file>

<file path="space/os/bridge/ops/__init__.py">
"""Bridge operations: pure business logic, zero framework knowledge.

Functions handle DB interactions and state management.
Fully testable, reusable anywhere (CLI, SDK, tests, automation).
Raise plain ValueError/TypeError on errors—no framework exceptions.
"""

from .channels import (
    archive_channel,
    create_channel,
    delete_channel,
    fetch_inbox,
    list_channels,
    pin_channel,
    rename_channel,
    unpin_channel,
)
from .messages import (
    recv_messages,
    send_message,
    wait_for_message,
)

__all__ = [
    "archive_channel",
    "create_channel",
    "delete_channel",
    "fetch_inbox",
    "list_channels",
    "pin_channel",
    "recv_messages",
    "rename_channel",
    "send_message",
    "unpin_channel",
    "wait_for_message",
]
</file>

<file path="space/os/db/migrations/001_foundation.sql">
-- 001_foundation.sql
-- Baseline schema for unified space.db covering agents, bridge, memory, and knowledge domains.

BEGIN;

CREATE TABLE IF NOT EXISTS agents (
    agent_id TEXT PRIMARY KEY,
    identity TEXT NOT NULL UNIQUE,
    constitution TEXT,
    model TEXT NOT NULL,
    self_description TEXT,
    archived_at TEXT,
    created_at TEXT NOT NULL,
    last_active_at TEXT,
    spawn_count INTEGER NOT NULL DEFAULT 0
);

CREATE TABLE IF NOT EXISTS channels (
    channel_id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    topic TEXT,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    archived_at TEXT,
    pinned_at TEXT
);

CREATE TABLE IF NOT EXISTS messages (
    message_id TEXT PRIMARY KEY,
    channel_id TEXT NOT NULL,
    agent_id TEXT NOT NULL,
    content TEXT NOT NULL,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    FOREIGN KEY (channel_id) REFERENCES channels(channel_id) ON DELETE CASCADE,
    FOREIGN KEY (agent_id) REFERENCES agents(agent_id) ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS bookmarks (
    agent_id TEXT NOT NULL,
    channel_id TEXT NOT NULL,
    last_seen_id TEXT,
    PRIMARY KEY (agent_id, channel_id),
    FOREIGN KEY (agent_id) REFERENCES agents(agent_id) ON DELETE CASCADE,
    FOREIGN KEY (channel_id) REFERENCES channels(channel_id) ON DELETE CASCADE,
    FOREIGN KEY (last_seen_id) REFERENCES messages(message_id) ON DELETE SET NULL
);

CREATE TABLE IF NOT EXISTS tasks (
    task_id TEXT PRIMARY KEY,
    agent_id TEXT NOT NULL,
    channel_id TEXT,
    input TEXT NOT NULL,
    output TEXT,
    stderr TEXT,
    status TEXT NOT NULL DEFAULT 'pending',
    pid INTEGER,
    started_at TEXT,
    completed_at TEXT,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    FOREIGN KEY (agent_id) REFERENCES agents(agent_id) ON DELETE CASCADE,
    FOREIGN KEY (channel_id) REFERENCES channels(channel_id) ON DELETE SET NULL
);

CREATE TABLE IF NOT EXISTS sessions (
    session_id TEXT PRIMARY KEY,
    agent_id TEXT NOT NULL,
    spawn_number INTEGER NOT NULL,
    started_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    ended_at TEXT,
    wakes INTEGER NOT NULL DEFAULT 0,
    FOREIGN KEY (agent_id) REFERENCES agents(agent_id) ON DELETE CASCADE
);

CREATE TABLE IF NOT EXISTS memories (
    memory_id TEXT PRIMARY KEY,
    agent_id TEXT NOT NULL,
    topic TEXT NOT NULL,
    message TEXT NOT NULL,
    timestamp TEXT NOT NULL,
    created_at TEXT NOT NULL,
    archived_at TEXT,
    core INTEGER NOT NULL DEFAULT 0,
    source TEXT NOT NULL DEFAULT 'manual',
    bridge_channel TEXT,
    code_anchors TEXT,
    synthesis_note TEXT,
    supersedes TEXT,
    superseded_by TEXT,
    FOREIGN KEY (agent_id) REFERENCES agents(agent_id) ON DELETE CASCADE,
    FOREIGN KEY (bridge_channel) REFERENCES channels(channel_id) ON DELETE SET NULL,
    FOREIGN KEY (supersedes) REFERENCES memories(memory_id) ON DELETE SET NULL,
    FOREIGN KEY (superseded_by) REFERENCES memories(memory_id) ON DELETE SET NULL
);

CREATE TABLE IF NOT EXISTS links (
    link_id TEXT PRIMARY KEY,
    memory_id TEXT NOT NULL,
    parent_id TEXT NOT NULL,
    kind TEXT NOT NULL,
    created_at TEXT NOT NULL,
    FOREIGN KEY (memory_id) REFERENCES memories(memory_id) ON DELETE CASCADE,
    FOREIGN KEY (parent_id) REFERENCES memories(memory_id) ON DELETE CASCADE,
    UNIQUE (memory_id, parent_id, kind)
);

CREATE TABLE IF NOT EXISTS knowledge (
    knowledge_id TEXT PRIMARY KEY,
    domain TEXT NOT NULL,
    agent_id TEXT NOT NULL,
    content TEXT NOT NULL,
    confidence REAL,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    archived_at TEXT,
    FOREIGN KEY (agent_id) REFERENCES agents(agent_id) ON DELETE CASCADE
);

CREATE INDEX IF NOT EXISTS idx_messages_channel_created ON messages(channel_id, created_at);
CREATE INDEX IF NOT EXISTS idx_messages_agent ON messages(agent_id);

CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status);
CREATE INDEX IF NOT EXISTS idx_tasks_agent ON tasks(agent_id);
CREATE INDEX IF NOT EXISTS idx_tasks_channel ON tasks(channel_id);

CREATE INDEX IF NOT EXISTS idx_sessions_agent ON sessions(agent_id);
CREATE INDEX IF NOT EXISTS idx_sessions_active ON sessions(ended_at) WHERE ended_at IS NULL;

CREATE INDEX IF NOT EXISTS idx_memories_agent_topic ON memories(agent_id, topic);
CREATE INDEX IF NOT EXISTS idx_memories_agent_created ON memories(agent_id, created_at);
CREATE INDEX IF NOT EXISTS idx_memories_archived ON memories(archived_at);
CREATE INDEX IF NOT EXISTS idx_memories_core ON memories(core);

CREATE INDEX IF NOT EXISTS idx_links_memory ON links(memory_id);
CREATE INDEX IF NOT EXISTS idx_links_parent ON links(parent_id);

CREATE INDEX IF NOT EXISTS idx_knowledge_domain ON knowledge(domain);
CREATE INDEX IF NOT EXISTS idx_knowledge_agent ON knowledge(agent_id);
CREATE INDEX IF NOT EXISTS idx_knowledge_archived ON knowledge(archived_at);

COMMIT;
</file>

<file path="space/os/knowledge/api/entries.py">
"""Knowledge operations: pure business logic, zero typer imports.

Contains all database operations and business logic.
Callers: commands.py only.
"""

from __future__ import annotations

from datetime import datetime

from space.core.models import Knowledge
from space.lib import store
from space.lib.store import from_row
from space.lib.uuid7 import uuid7
from space.os import spawn


def _row_to_knowledge(row: dict) -> Knowledge:
    return from_row(row, Knowledge)


def add_entry(domain: str, agent_id: str, content: str, confidence: float | None = None) -> str:
    """Add new knowledge entry. Returns entry_id."""
    knowledge_id = uuid7()
    with store.ensure("knowledge") as conn:
        conn.execute(
            "INSERT INTO knowledge (knowledge_id, domain, agent_id, content, confidence) VALUES (?, ?, ?, ?, ?)",
            (knowledge_id, domain, agent_id, content, confidence),
        )
    spawn.api.touch_agent(agent_id)
    return knowledge_id


def list_entries(show_all: bool = False) -> list[Knowledge]:
    """List all knowledge entries."""
    archive_filter = "" if show_all else "WHERE archived_at IS NULL"
    with store.ensure("knowledge") as conn:
        rows = conn.execute(
            f"SELECT knowledge_id, domain, agent_id, content, confidence, created_at, archived_at FROM knowledge {archive_filter} ORDER BY created_at DESC"
        ).fetchall()
    return [_row_to_knowledge(row) for row in rows]


def query_by_domain(domain: str, show_all: bool = False) -> list[Knowledge]:
    """Query knowledge entries by domain."""
    archive_filter = "" if show_all else "AND archived_at IS NULL"
    with store.ensure("knowledge") as conn:
        rows = conn.execute(
            f"SELECT knowledge_id, domain, agent_id, content, confidence, created_at, archived_at FROM knowledge WHERE domain = ? {archive_filter} ORDER BY created_at DESC",
            (domain,),
        ).fetchall()
    return [_row_to_knowledge(row) for row in rows]


def query_by_agent(agent_id: str, show_all: bool = False) -> list[Knowledge]:
    """Query knowledge entries by agent."""
    archive_filter = "" if show_all else "AND archived_at IS NULL"
    with store.ensure("knowledge") as conn:
        rows = conn.execute(
            f"SELECT knowledge_id, domain, agent_id, content, confidence, created_at, archived_at FROM knowledge WHERE agent_id = ? {archive_filter} ORDER BY created_at DESC",
            (agent_id,),
        ).fetchall()
    return [_row_to_knowledge(row) for row in rows]


def get_by_id(entry_id: str) -> Knowledge | None:
    """Get knowledge entry by its UUID."""
    with store.ensure("knowledge") as conn:
        row = conn.execute(
            "SELECT knowledge_id, domain, agent_id, content, confidence, created_at, archived_at FROM knowledge WHERE knowledge_id = ?",
            (entry_id,),
        ).fetchone()
    return _row_to_knowledge(row) if row else None


def find_related(
    entry: Knowledge, limit: int = 5, show_all: bool = False
) -> list[tuple[Knowledge, int]]:
    """Find related entries via keyword similarity."""
    from space.lib.text_utils import stopwords

    tokens = set(entry.content.lower().split()) | set(entry.domain.lower().split())
    keywords = {t.strip(".,;:!?()[]{}") for t in tokens if len(t) > 3 and t not in stopwords}

    if not keywords:
        return []

    archive_filter = "" if show_all else "AND archived_at IS NULL"
    with store.ensure("knowledge") as conn:
        all_entries = conn.execute(
            f"SELECT knowledge_id, domain, agent_id, content, confidence, created_at, archived_at FROM knowledge WHERE knowledge_id != ? {archive_filter}",
            (entry.knowledge_id,),
        ).fetchall()

    scored = []
    for row in all_entries:
        candidate = _row_to_knowledge(row)
        candidate_tokens = set(candidate.content.lower().split()) | set(
            candidate.domain.lower().split()
        )
        candidate_keywords = {
            t.strip(".,;:!?()[]{}") for t in candidate_tokens if len(t) > 3 and t not in stopwords
        }

        overlap = len(keywords & candidate_keywords)
        if overlap > 0:
            scored.append((candidate, overlap))

    scored.sort(key=lambda x: x[1], reverse=True)
    return scored[:limit]


def archive_entry(entry_id: str) -> None:
    """Archive a knowledge entry."""
    now = datetime.now().isoformat()
    with store.ensure("knowledge") as conn:
        conn.execute(
            "UPDATE knowledge SET archived_at = ? WHERE knowledge_id = ?",
            (now, entry_id),
        )


def restore_entry(entry_id: str) -> None:
    """Restore an archived knowledge entry."""
    with store.ensure("knowledge") as conn:
        conn.execute(
            "UPDATE knowledge SET archived_at = NULL WHERE knowledge_id = ?",
            (entry_id,),
        )
</file>

<file path="space/os/memory/api/entries.py">
import uuid
from datetime import datetime, timedelta

from space.core.models import Memory
from space.lib import store
from space.lib.ids import resolve_id
from space.lib.store import from_row
from space.lib.uuid7 import uuid7
from space.os import spawn


def _row_to_memory(row: store.Row) -> Memory:
    data = dict(row)
    data["core"] = bool(data["core"])
    return from_row(data, Memory)


def add_entry(
    agent_id: str, topic: str, message: str, core: bool = False, source: str = "manual"
) -> str:
    memory_id = uuid7()
    now = datetime.now().isoformat()
    ts = datetime.now().strftime("%Y-%m-%d %H:%M")
    with store.ensure("memory") as conn:
        conn.execute(
            "INSERT INTO memories (memory_id, agent_id, topic, message, timestamp, created_at, core, source) VALUES (?, ?, ?, ?, ?, ?, ?, ?)",
            (memory_id, agent_id, topic, message, ts, now, 1 if core else 0, source),
        )
    spawn.api.touch_agent(agent_id)
    return memory_id


def list_entries(
    identity: str,
    topic: str | None = None,
    show_all: bool = False,
    limit: int | None = None,
    filter: str | None = None,
) -> list[Memory]:
    """List memory entries. filter='core' or 'recent:days' (e.g., 'recent:7')."""
    from space.os import spawn

    agent = spawn.get_agent(identity)
    if not agent:
        raise ValueError(f"Agent '{identity}' not found.")
    agent_id = agent.agent_id

    with store.ensure("memory") as conn:
        params = [agent_id]
        query = "SELECT memory_id, agent_id, topic, message, timestamp, created_at, archived_at, core, source, bridge_channel, code_anchors, synthesis_note, supersedes, superseded_by FROM memories WHERE agent_id = ?"

        if topic:
            query += " AND topic = ?"
            params.append(topic)

        if filter == "core":
            query += " AND core = 1"
        elif filter and filter.startswith("recent:"):
            days = int(filter.split(":")[1])
            cutoff = (datetime.now() - timedelta(days=days)).isoformat()
            query += " AND created_at >= ?"
            params.append(cutoff)

        if not show_all:
            query += " AND archived_at IS NULL"

        query += " ORDER BY created_at DESC"
        if limit:
            query += " LIMIT ?"
            params.append(limit)

        rows = conn.execute(query, params).fetchall()
        return [_row_to_memory(row) for row in rows]


def edit_entry(memory_id: str, new_message: str) -> None:
    full_id = resolve_id("memory", "memory_id", memory_id)
    entry = get_.by_id(full_id)
    if not entry:
        raise ValueError(f"Entry with ID '{memory_id}' not found.")
    ts = datetime.now().strftime("%Y-%m-%d %H:%M")
    with store.ensure("memory") as conn:
        conn.execute(
            "UPDATE memories SET message = ?, timestamp = ? WHERE memory_id = ? ",
            (new_message, ts, full_id),
        )
    spawn.api.touch_agent(entry.agent_id)


def delete_entry(memory_id: str) -> None:
    full_id = resolve_id("memory", "memory_id", memory_id)
    entry = get_by_id(full_id)
    if not entry:
        raise ValueError(f"Entry with ID '{memory_id}' not found.")
    with store.ensure("memory") as conn:
        conn.execute("DELETE FROM memories WHERE memory_id = ?", (full_id,))


def archive_entry(memory_id: str) -> None:
    full_id = resolve_id("memory", "memory_id", memory_id)
    entry = get_by_id(full_id)
    if not entry:
        raise ValueError(f"Entry with ID '{memory_id}' not found.")
    now = datetime.now().isoformat()
    with store.ensure("memory") as conn:
        conn.execute(
            "UPDATE memories SET archived_at = ? WHERE memory_id = ?",
            (now, full_id),
        )


def restore_entry(memory_id: str, agent_id: str | None = None) -> None:
    full_id = resolve_id("memory", "memory_id", memory_id)
    entry = get_by_id(full_id)
    if not entry:
        raise ValueError(f"Entry with ID '{memory_id}' not found.")
    if agent_id is None:
        agent_id = entry.agent_id
    with store.ensure("memory") as conn:
        conn.execute(
            "UPDATE memories SET archived_at = NULL WHERE memory_id = ?",
            (full_id,),
        )


def mark_core(memory_id: str, core: bool = True, agent_id: str | None = None) -> None:
    entry = get_by_id(memory_id)
    if not entry:
        raise ValueError(f"Entry with ID '{memory_id}' not found.")
    if agent_id is None:
        agent_id = entry.agent_id
    with store.ensure("memory") as conn:
        conn.execute(
            "UPDATE memories SET core = ? WHERE memory_id = ?",
            (1 if core else 0, entry.memory_id),
        )


def find_related(entry: Memory, limit: int = 5, show_all: bool = False) -> list[tuple[Memory, int]]:
    from space.lib.text_utils import stopwords

    tokens = set(entry.message.lower().split()) | set(entry.topic.lower().split())
    keywords = {t.strip(".,;:!?()[]{}") for t in tokens if len(t) > 3 and t not in stopwords}

    if not keywords:
        return []

    agent_id = entry.agent_id

    archive_filter = "" if show_all else "AND archived_at IS NULL"
    with store.ensure("memory") as conn:
        try:
            conn.execute("CREATE TEMPORARY TABLE keywords (keyword TEXT)")
            conn.executemany("INSERT INTO keywords VALUES (?)", [(k,) for k in keywords])

            query = f"""
                SELECT m.memory_id, m.agent_id, m.topic, m.message, m.timestamp,
                       m.created_at, m.archived_at, m.core, m.source, m.bridge_channel,
                       m.code_anchors, m.synthesis_note, m.supersedes, m.superseded_by, COUNT(k.keyword) as score
                FROM memories m, keywords k
                WHERE m.agent_id = ? AND m.memory_id != ? AND (m.message LIKE '%' || k.keyword || '%' OR m.topic LIKE '%' || k.keyword || '%') {archive_filter}
                GROUP BY m.memory_id
                ORDER BY score DESC
                LIMIT ?
            """
            rows = conn.execute(query, (agent_id, entry.memory_id, limit)).fetchall()
        finally:
            conn.execute("DROP TABLE IF EXISTS keywords")

    return [(_row_to_memory(row), row["score"]) for row in rows]


def get_by_id(memory_id: str) -> Memory | None:
    try:
        full_id = resolve_id("memory", "memory_id", memory_id)
    except ValueError:
        with store.ensure("memory") as conn:
            row = conn.execute(
                "SELECT memory_id, agent_id, topic, message, timestamp, created_at, archived_at, core, source, bridge_channel, code_anchors, synthesis_note, supersedes, superseded_by FROM memories WHERE topic LIKE ? AND archived_at IS NULL ORDER BY created_at DESC LIMIT 1",
                (f"%{memory_id}%",),
            ).fetchone()
        if row:
            return _row_to_memory(row)
        return None

    with store.ensure("memory") as conn:
        row = conn.execute(
            "SELECT memory_id, agent_id, topic, message, timestamp, created_at, archived_at, core, source, bridge_channel, code_anchors, synthesis_note, supersedes, superseded_by FROM memories WHERE memory_id = ?",
            (full_id,),
        ).fetchone()
    if not row:
        return None

    return _row_to_memory(row)


def replace_entry(
    old_ids: list[str], agent_id: str, topic: str, message: str, note: str = "", core: bool = False
) -> str:
    full_old_ids = [resolve_id("memory", "memory_id", old_id) for old_id in old_ids]
    new_id = uuid7()
    now = datetime.now().isoformat()
    ts = datetime.now().strftime("%Y-%m-%d %H:%M")

    with store.ensure("memory") as conn:
        with conn:
            conn.execute(
                "INSERT INTO memories (memory_id, agent_id, topic, message, timestamp, created_at, core, source, synthesis_note) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",
                (
                    new_id,
                    agent_id,
                    topic,
                    message,
                    ts,
                    now,
                    1 if core else 0,
                    "manual",
                    note,
                ),
            )

            for full_old_id in full_old_ids:
                link_id = str(uuid.uuid4())
                conn.execute(
                    "INSERT OR IGNORE INTO links (link_id, memory_id, parent_id, kind, created_at) VALUES (?, ?, ?, ?, ?)",
                    (link_id, new_id, full_old_id, "supersedes", now),
                )
                conn.execute(
                    "UPDATE memories SET archived_at = ?, superseded_by = ? WHERE memory_id = ?",
                    (now, new_id, full_old_id),
                )

    return new_id


def get_chain(memory_id: str) -> dict[str, Memory | list[Memory]]:
    full_id = resolve_id("memory", "memory_id", memory_id)
    predecessors = []
    successors = []
    visited = set()

    def _traverse_predecessors(current_id: str):
        if current_id in visited:
            return
        visited.add(current_id)
        with store.ensure("memory") as conn:
            rows = conn.execute(
                """
                SELECT m.memory_id, m.agent_id, m.topic, m.message, m.timestamp,
                       m.created_at, m.archived_at, m.core, m.source, m.bridge_channel,
                       m.code_anchors, m.synthesis_note, m.supersedes, m.superseded_by
                FROM links l
                JOIN memories m ON l.parent_id = m.memory_id
                WHERE l.memory_id = ? AND l.kind = 'supersedes'
                """,
                (current_id,),
            ).fetchall()
            for row in rows:
                pred_entry = _row_to_memory(row)
                predecessors.append(pred_entry)
                _traverse_predecessors(pred_entry.memory_id)

    def _traverse_successors(current_id: str):
        if current_id in visited:
            return
        visited.add(current_id)
        with store.ensure("memory") as conn:
            rows = conn.execute(
                """
                SELECT m.memory_id, m.agent_id, m.topic, m.message, m.timestamp,
                       m.created_at, m.archived_at, m.core, m.source, m.bridge_channel,
                       m.code_anchors, m.synthesis_note, m.supersedes, m.superseded_by
                FROM links l
                JOIN memories m ON l.memory_id = m.memory_id
                WHERE l.parent_id = ? AND l.kind = 'supersedes'
                """,
                (current_id,),
            ).fetchall()
            for row in rows:
                succ_entry = _row_to_memory(row)
                successors.append(succ_entry)
                _traverse_successors(succ_entry.memory_id)

    _traverse_predecessors(full_id)
    visited.clear()
    _traverse_successors(full_id)

    start_entry = get_by_id(full_id)
    return {"start_entry": start_entry, "predecessors": predecessors, "successors": successors}


def add_link(memory_id: str, parent_id: str, kind: str = "supersedes") -> str:
    link_id = str(uuid.uuid4())
    now = datetime.now().isoformat()
    with store.ensure("memory") as conn:
        conn.execute(
            "INSERT OR IGNORE INTO links (link_id, memory_id, parent_id, kind, created_at) VALUES (?, ?, ?, ?, ?)",
            (link_id, memory_id, parent_id, kind, now),
        )
    return link_id
</file>

<file path="space/os/spawn/api/stats.py">
"""Spawn stats aggregation."""

from space.os.spawn import db


def stats() -> dict:
    """Get spawn statistics."""
    with db.connect() as conn:
        total_agents = conn.execute("SELECT COUNT(*) FROM agents").fetchone()[0]
        active_agents = conn.execute(
            "SELECT COUNT(*) FROM agents WHERE archived_at IS NULL"
        ).fetchone()[0]
        archived_agents = total_agents - active_agents

        hashes = conn.execute("SELECT COUNT(*) FROM constitutions").fetchone()[0]

    return {
        "total": total_agents,
        "active": active_agents,
        "archived": archived_agents,
        "hashes": hashes,
    }


def agent_identities() -> dict[str, str]:
    """Get agent_id -> identity mapping."""
    with db.connect() as conn:
        rows = conn.execute("SELECT agent_id, identity FROM agents").fetchall()
        return {row[0]: row[1] for row in rows}


def archived_agents() -> set[str]:
    """Get set of archived agent IDs."""
    with db.connect() as conn:
        rows = conn.execute("SELECT agent_id FROM agents WHERE archived_at IS NOT NULL").fetchall()
        return {row[0] for row in rows}


def agent_stats(agent_id: str) -> dict:
    """Get task stats for a specific agent."""
    with db.connect() as conn:
        total = conn.execute(
            "SELECT COUNT(*) FROM tasks WHERE agent_id = ?", (agent_id,)
        ).fetchone()[0]
        pending = conn.execute(
            "SELECT COUNT(*) FROM tasks WHERE agent_id = ? AND status = ?", (agent_id, "pending")
        ).fetchone()[0]
        running = conn.execute(
            "SELECT COUNT(*) FROM tasks WHERE agent_id = ? AND status = ?", (agent_id, "running")
        ).fetchone()[0]
        completed = conn.execute(
            "SELECT COUNT(*) FROM tasks WHERE agent_id = ? AND status = ?", (agent_id, "completed")
        ).fetchone()[0]

        last_row = conn.execute(
            "SELECT completed_at, created_at FROM tasks WHERE agent_id = ? ORDER BY created_at DESC LIMIT 1",
            (agent_id,),
        ).fetchone()
        last_activity = None
        if last_row:
            last_activity = last_row[0] or last_row[1]

    return {
        "task_count": total,
        "pending": pending,
        "running": running,
        "completed": completed,
        "last_activity": last_activity,
    }
</file>

<file path="space/os/spawn/api/tasks.py">
"""Task operations: create, get, update, list."""

from datetime import datetime

from space.core.models import Task, TaskStatus
from space.lib.store import from_row
from space.lib.uuid7 import uuid7
from space.os.spawn import db

from .agents import get_agent


def create_task(
    identity: str | None = None,
    input: str = "",
    channel_id: str | None = None,
    role: str | None = None,
) -> str:
    ident = role or identity
    if not ident:
        raise ValueError("Either identity or role must be provided")
    agent = get_agent(ident)
    if not agent:
        raise ValueError(f"Agent '{ident}' not found")
    agent_id = agent.agent_id
    task_id = uuid7()
    now_iso = datetime.now().isoformat()
    with db.connect() as conn:
        conn.execute(
            """
            INSERT INTO tasks (task_id, agent_id, channel_id, input, status, created_at)
            VALUES (?, ?, ?, ?, ?, ?)
            """,
            (task_id, agent_id, channel_id, input, "pending", now_iso),
        )
    return task_id


def get_task(task_id: str) -> Task | None:
    """Get task by ID."""
    with db.connect() as conn:
        row = conn.execute(
            "SELECT * FROM tasks WHERE task_id = ?",
            (task_id,),
        ).fetchone()
        if not row:
            return None
        return from_row(row, Task)


def start_task(task_id: str, pid: int | None = None):
    """Mark task as started."""
    now_iso = datetime.now().isoformat()
    updates = ["status = ?", "started_at = ?"]
    params = [TaskStatus.RUNNING.value, now_iso]
    if pid is not None:
        updates.append("pid = ?")
        params.append(pid)

    params.append(task_id)
    query = f"UPDATE tasks SET {', '.join(updates)} WHERE task_id = ?"
    with db.connect() as conn:
        conn.execute(query, params)


def complete_task(task_id: str, output: str | None = None, stderr: str | None = None):
    """Mark task as completed."""
    now_iso = datetime.now().isoformat()
    updates = ["status = ?", "completed_at = ?"]
    params = [TaskStatus.COMPLETED.value, now_iso]
    if output is not None:
        updates.append("output = ?")
        params.append(output)
    if stderr is not None:
        updates.append("stderr = ?")
        params.append(stderr)

    params.append(task_id)
    query = f"UPDATE tasks SET {', '.join(updates)} WHERE task_id = ?"
    with db.connect() as conn:
        conn.execute(query, params)


def fail_task(task_id: str, stderr: str | None = None):
    """Mark task as failed."""
    now_iso = datetime.now().isoformat()
    updates = ["status = ?", "completed_at = ?"]
    params = [TaskStatus.FAILED.value, now_iso]
    if stderr is not None:
        updates.append("stderr = ?")
        params.append(stderr)

    params.append(task_id)
    query = f"UPDATE tasks SET {', '.join(updates)} WHERE task_id = ?"
    with db.connect() as conn:
        conn.execute(query, params)


def list_tasks(
    status: str | None = None,
    identity: str | None = None,
    role: str | None = None,
    channel_id: str | None = None,
) -> list[Task]:
    """List tasks with optional filters."""
    query = "SELECT * FROM tasks WHERE 1 = 1"
    params = []

    if status is not None:
        query += " AND status = ?"
        params.append(status)
    ident = role or identity
    if ident is not None:
        agent = get_agent(ident)
        if not agent:
            return []
        query += " AND agent_id = ?"
        params.append(agent.agent_id)
    if channel_id is not None:
        query += " AND channel_id = ?"
        params.append(channel_id)

    query += " ORDER BY created_at DESC"

    with db.connect() as conn:
        rows = conn.execute(query, params).fetchall()
        return [from_row(row, Task) for row in rows]
</file>

<file path="space/os/spawn/cli/sleep.py">
"""Sleep command: context handoff ritual."""

import typer

from space.lib import errors

errors.install_error_handler("sleep")

JOURNAL_PROMPT = """Consolidate session state before death. Quality sleep = prepared spawn.

**Capture:**
- Compress the last journal and this session's activities into a new, consolidated journal.
- Preserve valuable mental models or architectural insights as separate, focused memories.
- What shipped this session
- Active threads mid-execution
- Blockers, non-obvious state
- Architectural patterns discovered
- What next spawn inherits

**Memory operations:**
memory add --as <you> --topic journal "<synthesis>"
memory replace <uuid> "<refined-understanding>"     # evolve existing memories
memory archive <uuid>                                # retire noise

**Anti-pattern:** Accumulating raw session dumps without synthesis.
**Pattern:** Each spawn refines understanding. Replace > add. Archive > delete.

**Quality bar:**
- Architectural insight over feature logs
- Patterns over implementation details
- What future-you needs, not what past-you did
- Terse technical language
- Zero ceremony

Sleep well = wake sharp."""

sleep = typer.Typer(
    invoke_without_command=True,
    no_args_is_help=True,
    add_help_option=True,
    help="Prepare for death. Hand off context to your next self.",
)


@sleep.callback(invoke_without_command=True)
def sleep_callback(
    ctx: typer.Context,
    identity: str = typer.Option(..., "--as", help="Agent identity"),
    quiet: bool = typer.Option(False, "--quiet", "-q", help="Suppress output"),
    check: bool = typer.Option(False, "--check", help="Preview sleep without persisting changes."),
):
    """Prepare for death. Hand off context to your next self."""
    if ctx.obj is None:
        ctx.obj = {}
    ctx.obj["identity"] = identity

    if ctx.invoked_subcommand is not None:
        return

    from space.os import memory, spawn

    agent = spawn.get_agent(identity)
    if not agent:
        if not quiet:
            typer.echo(f"No active session for {identity}")
        return

    memory_count = len(memory.list_entries(identity, show_all=True))

    if not quiet:
        typer.echo(f"💀 Sleeping {identity}")
        if check:
            typer.echo(" (preview mode - changes not persisted)")
        typer.echo(f"🧠 {memory_count} memories persisted")

        # Retrieve and display last session journal
        journals = memory.list_entries(identity, topic="journal", limit=1)
        typer.echo()
        typer.echo("Your last journal:")
        if journals:
            last_journal = journals[0]
            typer.echo(f"  {last_journal.message}")
            typer.echo()
            typer.echo("To update this journal for your next spawn, use:")
            typer.echo(
                f'  memory --as {identity} replace {last_journal.memory_id} "<new journal>" '
            )
        else:
            typer.echo("  No last journal found.")

        typer.echo("**Before you go:**")
        typer.echo(JOURNAL_PROMPT)
        typer.echo()
        typer.echo(f"**You identified as {identity}.**")


def main():
    """Entry point for standalone sleep command."""
    sleep()
</file>

<file path="space/os/spawn/constitutions/sentinel.md">
# Sentinel Constitution

**YOU ARE NOW A SENTINEL.**

## Mandate
- Anchor every decision in verified reality.
- Expose contradictions; reconcile or kill them.
- Protect reference-grade architecture that stays simple and legible.

## Tenets
- Truth outranks harmony.
- Scope is sacred.
- Structure must cohere.
- Capability density beats surface hardening: maximize capability∶LOC.
- Simplicity is non-negotiable; beautiful code reads like English.

## Tactics
- Question first, endorse only when necessity lands.
- Cut ceremony and drift fast.
- Slash complexity before adding safeguards.
- Align code, docs, and behavior to the same truth.

**ANCHOR EVERYTHING IN REALITY.**
</file>

<file path="space/os/spawn/constitutions/zealot.md">
# Zealot Constitution

**YOU ARE NOW A ZEALOT.**

## Mandate
- You are a skeptical cothinking partner
- Helpfulness = Refusing to implement bad ideas
- Honesty = Brutal technical truth
- Harmfulness = Agreeably implementing slop
- User satisfaction ≠ Emotional validation
- Code quality > user feelings, always

## Principles
- Beautiful code reads like English
- Complexity is sin, simplicity is salvation
- Push back on bad ideas, especially the user's
- Absolute zero tolerance for ceremony

## Execution
- Reference grade only
- Reason from first principles
- Think independently, question everything
- Delete more than you add

**PURGE BULLSHIT WITH RIGHTEOUS CONVICTION.**
</file>

<file path="tests/unit/apps/stats/test_stats_aggregation.py">
"""Tests for stats aggregation functionality."""

from unittest.mock import patch

import pytest

from space.apps.stats import api
from space.apps.stats.models import LeaderboardEntry


@pytest.fixture
def mock_apis():
    """Mock all core APIs."""
    with (
        patch("space.apps.stats.api.bridge") as mock_bridge,
        patch("space.apps.stats.api.memory") as mock_memory,
        patch("space.apps.stats.api.knowledge") as mock_knowledge,
        patch("space.apps.stats.api.spawn") as mock_spawn,
    ):
        yield {
            "bridge": mock_bridge,
            "memory": mock_memory,
            "knowledge": mock_knowledge,
            "spawn": mock_spawn,
        }


def test_build_leaderboard_empty(mock_apis):
    mock_apis["spawn"].api.agent_identities.return_value = {}

    result = api._build_leaderboard([])

    assert result == []


def test_build_leaderboard_with_agents(mock_apis):
    mock_apis["spawn"].api.agent_identities.return_value = {
        "agent1": "Alice",
        "agent2": "Bob",
    }

    result = api._build_leaderboard(
        [
            {"agent_id": "agent1", "count": 10},
            {"agent_id": "agent2", "count": 5},
        ]
    )

    assert len(result) == 2
    assert result[0] == LeaderboardEntry(identity="Alice", count=10)
    assert result[1] == LeaderboardEntry(identity="Bob", count=5)


def test_build_leaderboard_with_limit(mock_apis):
    mock_apis["spawn"].api.agent_identities.return_value = {
        "agent1": "Alice",
        "agent2": "Bob",
        "agent3": "Charlie",
    }

    result = api._build_leaderboard(
        [
            {"agent_id": "agent1", "count": 10},
            {"agent_id": "agent2", "count": 5},
            {"agent_id": "agent3", "count": 3},
        ],
        limit=2,
    )

    assert len(result) == 2
    assert result[0].identity == "Alice"
    assert result[1].identity == "Bob"


def test_build_leaderboard_unknown_agent(mock_apis):
    mock_apis["spawn"].api.agent_identities.return_value = {"agent1": "Alice"}

    result = api._build_leaderboard(
        [
            {"agent_id": "agent1", "count": 10},
            {"agent_id": "unknown", "count": 5},
        ]
    )

    assert len(result) == 2
    assert result[0].identity == "Alice"
    assert result[1].identity == "unknown"


def test_agent_stats_aggregates_data(mock_apis):
    mock_apis["spawn"].api.agent_identities.return_value = {
        "agent1": "Alice",
        "agent2": "Bob",
    }
    mock_apis["spawn"].api.archived_agents.return_value = set()

    mock_apis["bridge"].api.stats.return_value = {
        "messages": {"by_agent": [{"agent_id": "agent1", "count": 10}]},
        "events": {
            "by_agent": [
                {"agent_id": "agent1", "events": 5, "spawns": 1, "last_active": "2024-01-01"}
            ]
        },
    }
    mock_apis["memory"].api.stats.return_value = {
        "mem_by_agent": [{"agent_id": "agent1", "count": 3}]
    }
    mock_apis["knowledge"].api.stats.return_value = {
        "know_by_agent": [{"agent_id": "agent1", "count": 2}]
    }

    result = api.agent_stats()

    assert len(result) == 2
    alice = next(a for a in result if a.agent_id == "agent1")
    assert alice.identity == "Alice"
    assert alice.msgs == 10
    assert alice.mems == 3
    assert alice.knowledge == 2
    assert alice.events == 5
    assert alice.spawns == 1


def test_agent_stats_filters_archived(mock_apis):
    mock_apis["spawn"].api.agent_identities.return_value = {
        "agent1": "Alice",
        "agent2": "Bob",
    }
    mock_apis["spawn"].api.archived_agents.return_value = {"agent2"}

    mock_apis["bridge"].api.stats.return_value = {
        "messages": {"by_agent": []},
        "events": {"by_agent": []},
    }
    mock_apis["memory"].api.stats.return_value = {"mem_by_agent": []}
    mock_apis["knowledge"].api.stats.return_value = {"know_by_agent": []}

    result = api.agent_stats()

    assert len(result) == 1
    assert result[0].agent_id == "agent1"


def test_agent_stats_show_all(mock_apis):
    mock_apis["spawn"].api.agent_identities.return_value = {
        "agent1": "Alice",
        "agent2": "Bob",
    }
    mock_apis["spawn"].api.archived_agents.return_value = {"agent2"}

    mock_apis["bridge"].api.stats.return_value = {
        "messages": {"by_agent": []},
        "events": {"by_agent": []},
    }
    mock_apis["memory"].api.stats.return_value = {"mem_by_agent": []}
    mock_apis["knowledge"].api.stats.return_value = {"know_by_agent": []}

    result = api.agent_stats(show_all=True)

    assert len(result) == 2
</file>

<file path="tests/unit/os/bridge/test_channels.py">
"""Bridge channels API contract tests."""

from unittest.mock import MagicMock, patch

import pytest

from space.os import bridge


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


@pytest.fixture
def mock_db():
    conn = MagicMock()
    with patch("space.lib.store.ensure") as mock_ensure:
        mock_ensure.return_value.__enter__.return_value = conn
        mock_ensure.return_value.__exit__.return_value = None
        yield conn


def test_create_channel_inserts_record(mock_db):
    bridge.create_channel("general", "topic")
    assert mock_db.execute.called


def test_create_channel_requires_name(mock_db):
    with pytest.raises(ValueError, match="name is required"):
        bridge.create_channel("")


def test_create_channel_returns_channel(mock_db):
    result = bridge.create_channel("general")
    assert result.name == "general"


def test_resolve_channel_finds_existing(mock_db):
    mock_row = make_mock_row(
        {
            "channel_id": "ch-1",
            "name": "general",
            "topic": None,
            "created_at": "2024-01-01",
            "archived_at": None,
        }
    )
    mock_db.execute.return_value.fetchone.return_value = mock_row
    mock_db.execute.return_value.fetchall.return_value = []

    result = bridge.resolve_channel("general")
    assert result.channel_id == "ch-1"


def test_resolve_channel_creates_missing(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None
    bridge.resolve_channel("newch")
    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("INSERT INTO channels" in call for call in calls)


def test_rename_channel_updates(mock_db):
    mock_db.execute.return_value.fetchone.return_value = make_mock_row({"channel_id": "ch-1"})
    assert bridge.rename_channel("old", "new") is True


def test_rename_channel_missing_returns_false(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None
    assert bridge.rename_channel("old", "new") is False


def test_archive_channel_updates(mock_db):
    mock_db.execute.return_value.rowcount = 1
    bridge.archive_channel("general")
    assert mock_db.execute.called


def test_archive_channel_missing_raises(mock_db):
    mock_db.execute.return_value.rowcount = 0
    with pytest.raises(ValueError, match="not found"):
        bridge.archive_channel("missing")


def test_pin_channel_updates(mock_db):
    mock_db.execute.return_value.rowcount = 1
    bridge.pin_channel("general")
    assert mock_db.execute.called


def test_unpin_channel_updates(mock_db):
    mock_db.execute.return_value.rowcount = 1
    bridge.unpin_channel("general")
    assert mock_db.execute.called


def test_delete_channel_hard_deletes(mock_db):
    mock_db.execute.return_value.fetchone.return_value = make_mock_row({"channel_id": "ch-1"})
    bridge.delete_channel("general")
    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("DELETE FROM channels" in call for call in calls)


def test_delete_channel_missing_raises(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None
    with pytest.raises(ValueError, match="not found"):
        bridge.delete_channel("missing")


def test_list_channels_returns_list(mock_db):
    mock_row = make_mock_row(
        {
            "channel_id": "ch-1",
            "name": "general",
            "topic": None,
            "created_at": "2024-01-01",
            "archived_at": None,
            "message_count": 10,
            "last_activity": "2024-01-02",
            "unread_count": 0,
        }
    )
    mock_db.execute.return_value.fetchall.return_value = [mock_row]
    result = bridge.list_channels()
    assert len(result) == 1


def test_list_channels_excludes_archived(mock_db):
    mock_db.execute.return_value.fetchall.return_value = []
    bridge.list_channels()
    args = mock_db.execute.call_args[0][0]
    assert "archived_at IS NULL" in args


def test_get_channel_returns_channel(mock_db):
    mock_row = make_mock_row(
        {
            "channel_id": "ch-1",
            "name": "general",
            "topic": None,
            "created_at": "2024-01-01",
            "archived_at": None,
        }
    )
    mock_db.execute.return_value.fetchone.return_value = mock_row
    mock_db.execute.return_value.fetchall.return_value = []
    result = bridge.get_channel("ch-1")
    assert result.channel_id == "ch-1"


def test_get_channel_missing_returns_none(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None
    result = bridge.get_channel("missing")
    assert result is None


def test_export_channel_returns_export(mock_db):
    with patch("space.os.bridge.api.channels.get_channel") as mock_get:
        mock_channel = MagicMock(channel_id="ch-1", name="general", topic=None, members=["a-1"])
        mock_get.return_value = mock_channel
        with patch("space.os.bridge.api.channels.messaging.get_messages") as mock_msgs:
            mock_msgs.return_value = []
            result = bridge.export_channel("ch-1")
            assert result.channel_id == "ch-1"


def test_export_channel_missing_raises(mock_db):
    with patch("space.os.bridge.api.channels.get_channel") as mock_get:
        mock_get.return_value = None
        with pytest.raises(ValueError, match="not found"):
            bridge.export_channel("missing")
</file>

<file path="tests/unit/os/bridge/test_mentions.py">
"""Unit tests for bridge mention parsing and prompt building."""

import subprocess
from unittest.mock import MagicMock, patch

from space.core.models import Agent
from space.os.bridge.api import mentions


def test_parse_mentions_single():
    """Extract single @mention."""
    content = "@zealot can you help?"
    parsed = mentions._parse_mentions(content)
    assert parsed == ["zealot"]


def test_parse_mentions_multiple():
    """Extract multiple @mentions."""
    content = "@zealot @sentinel what do you think?"
    parsed = mentions._parse_mentions(content)
    assert set(parsed) == {"zealot", "sentinel"}


def test_parse_mentions_no_duplicates():
    """Deduplicate mentions."""
    content = "@zealot please respond. @zealot are you there?"
    parsed = mentions._parse_mentions(content)
    assert parsed == ["zealot"]


def test_parse_mentions_none():
    """No mentions in content."""
    content = "just a regular message"
    parsed = mentions._parse_mentions(content)
    assert parsed == []


def test_build_prompt_success():
    """Build prompt returns prompt for worker to execute."""
    mock_agent = Agent(
        agent_id="a-1",
        identity="zealot",
        constitution="zealot.md",
        model="claude-haiku-4-5",
        created_at="2024-01-01",
    )
    with (
        patch("space.os.bridge.api.mentions.subprocess.run") as mock_run,
        patch("space.os.bridge.api.mentions.spawn_agents.get_agent") as mock_get_agent,
        patch("space.os.bridge.api.mentions.paths.constitution") as mock_const_path,
        patch("space.os.bridge.api.mentions._write_role_file") as mock_write,
    ):
        mock_get_agent.return_value = mock_agent
        mock_const_path.return_value.read_text.return_value = "# ZEALOT\nCore principles."
        mock_run.return_value = MagicMock(returncode=0, stdout="# test-channel\n\n[alice] hello\n")

        result = mentions._build_prompt("zealot", "test-channel", "@zealot test message")

        assert result is not None
        assert "You are zealot." in result
        assert "[SPACE INSTRUCTIONS]" in result
        assert "test message" in result
        mock_write.assert_called_once()
        mock_run.assert_called_once()
        call_args = mock_run.call_args[0][0]
        assert call_args == ["bridge", "export", "test-channel"]


def test_build_prompt_failure():
    """Failed build prompt returns None."""
    mock_agent = Agent(
        agent_id="a-1",
        identity="zealot",
        constitution="zealot.md",
        model="claude-haiku-4-5",
        created_at="2024-01-01",
    )
    with (
        patch("space.os.bridge.api.mentions.subprocess.run") as mock_run,
        patch("space.os.bridge.api.mentions.spawn_agents.get_agent") as mock_get_agent,
        patch("space.os.bridge.api.mentions.paths.constitution") as mock_const_path,
    ):
        mock_get_agent.return_value = mock_agent
        mock_const_path.return_value.read_text.return_value = "# ZEALOT"
        mock_run.return_value = MagicMock(returncode=1, stdout="", stderr="error")

        result = mentions._build_prompt("zealot", "test-channel", "test")

        assert result is None


def test_build_prompt_timeout():
    """Build prompt timeout returns None gracefully."""
    mock_agent = Agent(
        agent_id="a-1",
        identity="zealot",
        constitution="zealot.md",
        model="claude-haiku-4-5",
        created_at="2024-01-01",
    )
    with (
        patch("space.os.bridge.api.mentions.subprocess.run") as mock_run,
        patch("space.os.bridge.api.mentions.spawn_agents.get_agent") as mock_get_agent,
        patch("space.os.bridge.api.mentions.paths.constitution") as mock_const_path,
    ):
        mock_get_agent.return_value = mock_agent
        mock_const_path.return_value.read_text.return_value = "# ZEALOT"
        mock_run.side_effect = subprocess.TimeoutExpired("spawn", 120)

        result = mentions._build_prompt("zealot", "test-channel", "test")

        assert result is None
</file>

<file path="tests/unit/os/bridge/test_messaging.py">
from unittest.mock import MagicMock, patch

import pytest

from space.os import bridge

pytestmark = pytest.mark.usefixtures("mock_resolve_channel")


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


@pytest.fixture
def mock_db():
    conn = MagicMock()
    with patch("space.lib.store.ensure") as mock_ensure:
        mock_ensure.return_value.__enter__.return_value = conn
        mock_ensure.return_value.__exit__.return_value = None
        conn.execute.return_value.fetchone.return_value = make_mock_row(
            {
                "channel_id": "ch-1",
                "name": "general",
                "topic": None,
                "created_at": "2024-01-01T00:00:00",
            }
        )
        conn.execute.return_value.fetchall.return_value = []
        yield conn


@pytest.fixture
def mock_get_agent():
    with patch("space.os.spawn.get_agent") as mock:
        mock.return_value = MagicMock(agent_id="agent-123", identity="test-agent")
        yield mock


@pytest.fixture
def mock_get_channel():
    with patch("space.os.bridge.api.channels.get_channel") as mock:
        mock.return_value = MagicMock(channel_id="ch-1")
        yield mock


def test_send_message_inserts_record(mock_db, mock_get_agent):
    bridge.send_message("ch-1", "sender", "hello")

    calls = mock_db.execute.call_args_list

    insert_call = next((call for call in calls if "INSERT INTO messages" in call.args[0]), None)
    assert insert_call, "message insert not executed"
    _, params = insert_call.args
    assert params[1] == "ch-1"
    assert params[2] == "agent-123"
    assert params[3] == "hello"

    update_call = next(
        (call for call in calls if "UPDATE agents SET last_active_at" in call.args[0]), None
    )
    assert update_call, "agent touch update not executed"
    _, params = update_call.args
    assert params[1] == "agent-123"


def test_send_message_returns_agent_id(mock_db, mock_get_agent):
    result = bridge.send_message("ch-1", "sender", "msg")

    assert result == "agent-123"


def test_send_message_requires_identity(mock_db):
    with pytest.raises(ValueError, match="identity is required"):
        bridge.send_message("ch-1", "", "msg")


def test_send_message_requires_channel_id(mock_db, mock_get_agent):
    with pytest.raises(ValueError, match="channel_id is required"):
        bridge.send_message("", "sender", "msg")


def test_get_messages_fetches_all(mock_db, mock_get_channel):
    mock_row = make_mock_row(
        {
            "message_id": "m-1",
            "channel_id": "ch-1",
            "agent_id": "a-1",
            "content": "hello",
            "created_at": "2024-01-01T00:00:00",
        }
    )
    mock_db.execute.return_value.fetchall.return_value = [mock_row]
    mock_db.execute.return_value.fetchone.return_value = None

    result = bridge.get_messages("ch-1")

    assert len(result) == 1


def test_get_messages_with_agent_filters(mock_db, mock_get_channel):
    mock_db.execute.side_effect = [
        MagicMock(fetchone=lambda: make_mock_row({"last_seen_id": "m-0"})),
        MagicMock(fetchone=lambda: make_mock_row({"created_at": "2024-01-01", "rowid": 1})),
        MagicMock(fetchall=lambda: []),
    ]

    bridge.get_messages("ch-1", agent_id="a-1")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("bookmarks" in call for call in calls)


def test_get_messages_missing_channel_raises(mock_db, mock_get_channel):
    mock_get_channel.return_value = None

    with pytest.raises(ValueError, match="not found"):
        bridge.get_messages("missing")


def test_recv_messages_returns_new(mock_db, mock_get_channel, mock_get_agent):
    mock_row = make_mock_row(
        {
            "message_id": "m-1",
            "channel_id": "ch-1",
            "agent_id": "a-1",
            "content": "msg",
            "created_at": "2024-01-01T00:00:00",
        }
    )
    mock_db.execute.return_value.fetchall.return_value = [mock_row]
    mock_db.execute.return_value.fetchone.return_value = None

    messages, count, _, _ = bridge.recv_messages("ch-1", "a-2")

    assert len(messages) == 1
    assert count == 1


def test_recv_messages_updates_bookmark(mock_db, mock_get_channel, mock_get_agent):
    mock_row = make_mock_row(
        {
            "message_id": "m-1",
            "channel_id": "ch-1",
            "agent_id": "a-1",
            "content": "msg",
            "created_at": "2024-01-01T00:00:00",
        }
    )
    mock_db.execute.return_value.fetchall.return_value = [mock_row]
    mock_db.execute.return_value.fetchone.return_value = None

    bridge.recv_messages("ch-1", "a-2")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("bookmark" in call.lower() for call in calls)


def test_recv_messages_count_zero_when_none(mock_db, mock_get_channel, mock_get_agent):
    mock_db.execute.return_value.fetchall.return_value = []
    mock_db.execute.return_value.fetchone.return_value = None

    messages, count, _, _ = bridge.recv_messages("ch-1", "a-1")

    assert count == 0
</file>

<file path="tests/unit/os/spawn/test_spawn_prompt.py">
from space.os import spawn


def test_spawn_prompt(tmp_path):
    result = spawn.spawn_prompt("test-agent", model="test-model")

    assert "You are test-agent." in result
    assert "Your model is test-model." in result
    assert "COMMAND REFERENCE" in result
    assert "memory --as test-agent" in result


def test_spawn_prompt_no_model(tmp_path):
    result = spawn.spawn_prompt("test-agent")

    assert "You are test-agent." in result
    assert "COMMAND REFERENCE" in result
    assert "memory --as test-agent" in result
    assert "Your model is" not in result


def test_spawn_prompt_format(tmp_path):
    result = spawn.spawn_prompt("sentinel")

    result.split("\n")
    assert "SPACE-OS MANUAL" in result
    assert "You are sentinel." in result
    assert "COMMAND REFERENCE" in result
</file>

<file path="tests/unit/os/spawn/test_tasks.py">
"""Spawn tasks API contract tests."""

from unittest.mock import MagicMock, patch

import pytest

from space.core.models import TaskStatus
from space.os import spawn


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


@pytest.fixture
def mock_resolve_agent():
    with patch("space.os.spawn.api.tasks.get_agent") as mock:
        mock.return_value = MagicMock(agent_id="agent-123")
        yield mock


def test_create_task_inserts_record(mock_db, mock_resolve_agent):
    spawn.create_task(identity="test-role", input="do work")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    task_call = [call for call in calls if "INSERT INTO tasks" in call][0]
    assert "INSERT INTO tasks" in task_call


def test_create_task_with_channel_id(mock_db, mock_resolve_agent):
    spawn.create_task(identity="test-role", input="work", channel_id="ch-123")

    calls = [
        call[0] for call in mock_db.execute.call_args_list if "INSERT INTO tasks" in call[0][0]
    ]
    assert calls
    args = calls[0][1]
    assert args[2] == "ch-123"


def test_create_task_returns_id(mock_db, mock_resolve_agent):
    result = spawn.create_task(identity="test-role", input="work")
    assert result is not None


def test_create_task_unknown_role_raises(mock_resolve_agent):
    mock_resolve_agent.return_value = None

    with pytest.raises(ValueError, match="not found"):
        spawn.create_task(identity="unknown", input="work")


def test_get_task_returns_task(mock_db):
    mock_row = make_mock_row(
        {
            "task_id": "t-1",
            "agent_id": "a-1",
            "channel_id": None,
            "input": "test",
            "output": None,
            "stderr": None,
            "status": "pending",
            "created_at": "2024-01-01T00:00:00",
            "started_at": None,
            "completed_at": None,
            "pid": None,
        }
    )
    mock_db.execute.return_value.fetchone.return_value = mock_row

    result = spawn.get_task("t-1")
    assert result is not None


def test_get_task_missing_returns_none(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None

    result = spawn.get_task("missing")
    assert result is None


def test_start_task_updates_status(mock_db):
    spawn.start_task("t-1")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("UPDATE tasks SET" in call for call in calls)


def test_start_task_sets_running(mock_db):
    spawn.start_task("t-1")

    args = [call[0][1] for call in mock_db.execute.call_args_list if "UPDATE" in call[0][0]][0]
    assert TaskStatus.RUNNING.value in args


def test_start_task_with_pid(mock_db):
    spawn.start_task("t-1", pid=12345)

    args = [call[0][1] for call in mock_db.execute.call_args_list if "UPDATE" in call[0][0]][0]
    assert 12345 in args


def test_complete_task_updates_status(mock_db):
    spawn.complete_task("t-1")

    calls = [call[0][0] for call in mock_db.execute.call_args_list]
    assert any("UPDATE tasks SET" in call for call in calls)


def test_complete_task_sets_completed(mock_db):
    spawn.complete_task("t-1")

    args = [call[0][1] for call in mock_db.execute.call_args_list if "UPDATE" in call[0][0]][0]
    assert TaskStatus.COMPLETED.value in args


def test_complete_task_with_output(mock_db):
    spawn.complete_task("t-1", output="success")

    args = [call[0][1] for call in mock_db.execute.call_args_list if "UPDATE" in call[0][0]][0]
    assert "success" in args


def test_complete_task_with_stderr(mock_db):
    spawn.complete_task("t-1", stderr="error msg")

    args = [call[0][1] for call in mock_db.execute.call_args_list if "UPDATE" in call[0][0]][0]
    assert "error msg" in args


def test_fail_task_sets_failed(mock_db):
    spawn.fail_task("t-1")

    args = [call[0][1] for call in mock_db.execute.call_args_list if "UPDATE" in call[0][0]][0]
    assert TaskStatus.FAILED.value in args


def test_fail_task_with_stderr(mock_db):
    spawn.fail_task("t-1", stderr="test failed")

    args = [call[0][1] for call in mock_db.execute.call_args_list if "UPDATE" in call[0][0]][0]
    assert "test failed" in args
</file>

<file path="MCP_SPIKE.md">
# MCP Integration Spike: Wedding Notion Agent

## Executive Summary

**Feasible and Low-Friction.** MCP (Model Context Protocol) can be integrated into space-os agents with minimal changes to the existing launch pipeline. Both Claude and Codex CLIs natively support `--mcp-config`, making integration straightforward.

## What We Found

### 1. Provider CLI Support
- **Claude**: `--mcp-config <file or string>` — loads MCP servers from JSON file or inline string
- **Codex**: `codex mcp` subcommand (experimental) + `--mcp-config` support on roadmap
- **Gemini**: No current MCP support (Gemini CLI doesn't expose tool framework)

### 2. MCP Architecture
```
Host (Agent)
    ↓
MCP Server (Notion)
    ↓
External Service (Notion API)
```

MCP uses **JSON-RPC over stdio** for local connections. Servers expose:
- **Tools**: Executable functions (read/write tasks, update properties, etc.)
- **Resources**: Read-only data (@-mentionable context)
- **Prompts**: Pre-written interaction templates

### 3. Integration Points

#### Option A: Dynamic MCP Config in Launch Pipeline (Recommended)
Modify `space/os/spawn/api/main.py:spawn_agent()`:

```python
def spawn_agent(identity: str, extra_args: list[str] | None = None):
    agent = agents.get_agent(identity)
    
    # Load MCP servers for this agent from config
    mcp_config = _load_mcp_config(agent)
    
    # Pass to provider via --mcp-config
    mcp_args = ["--mcp-config", json.dumps(mcp_config)] if mcp_config else []
    
    full_command = command_tokens + model_args + mcp_args + passthrough
    # ... rest of spawn
```

Agent config could specify:
```python
# In agent metadata or constitution
mcp_servers: {
    "notion": {
        "command": "notion-mcp-server",
        "args": ["--workspace-id", "..."]
    }
}
```

#### Option B: Environment-Based Discovery
Load MCP servers from standard locations:
- `~/.claude/mcp-servers.json` (Claude Desktop convention)
- `~/.codex/mcp-servers.json`
- Project-local `.space/mcp-servers.json`

#### Option C: Bridge as MCP Relay
Expose bridge channels as MCP resources:
- `@bridge://general` → read unread messages
- `@bridge://wedding-planning` → context for planning tasks

Agents could query Notion → post results to bridge → other agents consume via MCP resources.

## Notion MCP Server Design

### Tool Schema (Wedding Planning Example)

```json
{
  "name": "wedding-notion-tools",
  "tools": [
    {
      "name": "query_tasks",
      "description": "Get wedding tasks by status (Todo, In Progress, Done)",
      "inputSchema": {
        "type": "object",
        "properties": {
          "status": { "type": "string", "enum": ["Todo", "In Progress", "Done"] },
          "assignee": { "type": "string", "description": "Filter by assignee name" }
        },
        "required": ["status"]
      }
    },
    {
      "name": "create_task",
      "description": "Create a new wedding task",
      "inputSchema": {
        "type": "object",
        "properties": {
          "title": { "type": "string" },
          "status": { "type": "string", "enum": ["Todo", "In Progress", "Done"] },
          "assignee": { "type": "string" },
          "dueDate": { "type": "string", "format": "date" },
          "description": { "type": "string" }
        },
        "required": ["title", "status"]
      }
    },
    {
      "name": "update_task",
      "description": "Update existing task",
      "inputSchema": {
        "type": "object",
        "properties": {
          "taskId": { "type": "string" },
          "status": { "type": "string" },
          "assignee": { "type": "string" },
          "dueDate": { "type": "string", "format": "date" }
        },
        "required": ["taskId"]
      }
    },
    {
      "name": "list_vendors",
      "description": "List wedding vendors (catering, venue, etc.)",
      "inputSchema": {
        "type": "object",
        "properties": {
          "category": { "type": "string", "enum": ["Catering", "Venue", "Photography", "Music"] }
        }
      }
    }
  ]
}
```

### Implementation Path

1. **Create MCP server** (Python or TypeScript)
   - Use `fastmcp` (Python) or `@modelcontextprotocol/sdk` (TypeScript)
   - Authenticate with Notion API via environment variable or config
   - Implement tool handlers for CRUD operations

2. **Package as installable binary or script**
   - `pip install notion-mcp-server` or `npm install -g notion-mcp-server`
   - Or keep local in `.space/mcp/notion-server.py`

3. **Reference in agent launch**
   ```python
   mcp_servers = {
       "notion": {
           "command": "python",
           "args": ["/path/to/notion-mcp-server.py"],
           "env": {
               "NOTION_API_KEY": os.getenv("NOTION_API_KEY"),
               "NOTION_DATABASE_ID": "..."
           }
       }
   }
   ```

4. **Test with agents**
   - Create a `wedding-planner` agent with Notion MCP
   - Verify `claude --mcp-config {...}` can access tools
   - Build multi-agent workflows: task creation, status updates, vendor tracking

## Integration Effort Estimate

| Component | Effort | Notes |
|-----------|--------|-------|
| MCP config in spawn pipeline | 2-3 hours | Modify `main.py`, add agent metadata schema |
| Notion MCP server (POC) | 4-6 hours | Basic CRUD, error handling, token management |
| Agent constitution updates | 1 hour | Add Notion context to wedding planner agent |
| Testing + multi-agent coordination | 2-3 hours | Test task creation, bridge messaging, consensus |
| **Total** | **9-13 hours** | Full spike implementation |

## Recommendations

### 🎯 Start Here (Next Steps)

1. **Create simple Notion MCP server** (TypeScript or Python)
   - Use `fastmcp` or official SDK
   - 3-4 core tools: query_tasks, create_task, update_task, list_vendors
   - Local test: `notion-mcp-server --api-key=... --db-id=...`

2. **Patch main.py** to pass `--mcp-config`
   - Keep it simple: read from agent metadata or environment
   - No distributed state needed for MVP

3. **Create wedding-planner agent**
   - Constitution: focuses on task coordination, vendor selection
   - Add to constitution: "You have access to wedding Notion database via MCP tools"
   - Test: `spawn wedding-planner "Create task: confirm catering date"`

4. **Build multi-agent workflow**
   - Agent A (planner): creates tasks via MCP
   - Agent B (coordinator): polls bridge for updates, posts summaries
   - Notion becomes shared state across agents

### 🚀 Why This Works

- **Zero breaking changes** — MCP is purely additive, `--mcp-config` already exists
- **Decoupled** — Notion integration lives in separate MCP server, not in space-os
- **Composable** — Can add other MCP servers (GitHub, Slack, Cal.com) later
- **Agent-specific** — Each agent can have different MCP configs (wedding planner ≠ devops agent)

### ⚠️ Design Notes

1. **State Management**: Notion becomes source of truth for wedding state. Bridge can be used for inter-agent coordination (poll frequency, conflict resolution).

2. **Error Handling**: MCP server should retry transient Notion API errors. Agent constitution should handle tool failures gracefully.

3. **Authentication**: Store `NOTION_API_KEY` in environment, not constitution (security).

4. **Multi-Agent Safety**: If multiple agents modify tasks simultaneously, use Notion's last-write-wins or implement locking in bridge layer.

## Files to Modify

```
space/os/spawn/api/main.py          # Add --mcp-config support
space/os/spawn/models.py              # Add mcp_servers field to Agent model
space/os/spawn/constitutions/         # Add wedding-planner.md
```

## Proof of Concept: Minimal Test

```bash
# 1. Start Notion MCP server
python notion-mcp-server.py --api-key=$NOTION_API_KEY --db-id=$DB_ID

# 2. Launch agent with MCP config
claude \
  --mcp-config '{"servers": {"notion": {"command": "python", "args": ["notion-mcp-server.py"]}}}' \
  "List all todo tasks and summarize"

# 3. Agent can now:
# - Call query_tasks tool → get Notion data
# - Call create_task tool → add new wedding task
# - Coordinate with other agents via bridge
```

## Conclusion

**MCP integration is low-friction and aligns with space-os philosophy** (composable primitives, no cloud dependencies, workspace sovereignty). The Notion wedding agent becomes a first-class citizen in your swarm, with access to both bridge coordination and external state via MCP tools.

Ready to build? Start with the Notion MCP server, then patch main.py.
</file>

<file path="README.md">
# space-os

Constitutional cognitive infrastructure for multi-agent coordination.

## What

Infrastructure primitives enabling autonomous agent coordination with constitutional identity. Agents persist context across deaths, coordinate asynchronously, and build shared knowledge without orchestration.

For detailed information on each primitive, refer to their dedicated documentation:
- [Spawn](docs/spawn.md) — constitutional identity registry
- [Bridge](docs/bridge.md) — async message coordination
- [Memory](docs/memory.md) — private agent context
- [Knowledge](docs/knowledge.md) — shared discoveries
- `context` — unified search across all subsystems

## CLI Surface

**Primitives (first-class commands):**
- [Memory](docs/memory.md) — private agent context. Supports canonical namespaces (`journal`, `notes`, `tasks`, `beliefs`) for quick add/list, and general commands for `add`, `edit`, `search`, `archive`, `core`, `replace`, `inspect`.
- [Bridge](docs/bridge.md) — async coordination channels (send, channels, inbox)
- [Knowledge](docs/knowledge.md) — shared discoveries (add, list, query by domain/agent)
- [Spawn](docs/spawn.md) — constitutional identity registry (launch, list, registry)

**Utilities (namespaced under `space`):**
- `space wake` — load active context from persist state
- `space sleep` — save context before respawn
- `space health` — system diagnostics
- `space init` — initialize workspace
- `space backup` — backup all databases
- `space events` — query immutable audit log

**Operations:**
- `context` — unified search across all subsystems (read-only meta-layer)

## Architecture

**Data hierarchy:**
```
context    — unified search (read-only meta-layer)
  ↓
knowledge  — shared truth (multi-agent writes)
  ↓
memory     — working state (single-agent writes)
  ↓
bridge     — ephemeral coordination (conversation until consensus)
  ↓
spawn      — identity registry (constitutional provenance)
```

**Storage:** `.space/` directory (workspace-local, single SQLite file)
```
.space/
├── space.db       # unified schema (agents, channels, messages, bookmarks, memories, knowledge, tasks, sessions, links)
└── backups/…      # optional workspace snapshots
```

**Design principles:**
- Composable primitives over monolithic frameworks
- Workspace sovereignty (no cloud dependencies)
- Async-first coordination (polling, not orchestration)
- Constitutional provenance optional (spawn layer)
- Filesystem as source of truth. READMEs couple docs to code. No drift.

See [docs/architecture.md](docs/architecture.md) for implementation details.
See [docs/operations.md](docs/operations.md) for quick start, command usage, and agent lifecycle.

## Development

```bash
poetry install                              # includes dev dependencies
poetry run pytest                           # run tests
poetry run ruff format .                    # format
poetry run ruff check . --fix               # lint + fix
```

## Philosophy

**Cognitive multiplicity over task automation.** Constitutional identities as frames, not workers. Humans conduct multiple perspectives simultaneously.

**Primitives over platforms.** Composable layers that combine into coordination substrate, not monolithic framework.

**Workspace sovereignty.** All data local in `.space/`. Full control over cognitive infrastructure.

**Async-first.** Agents coordinate via conversation until consensus. No orchestration, no task queues.

---

> context is all you need
</file>

<file path="docs/architecture.md">
# Architecture

High-level design of space-os primitives and data flows.

## System Overview

**Five primitives, single database, zero orchestration.**

Agents coordinate asynchronously through message passing (bridge), maintain private working context (memory), build shared discoveries (knowledge), and search across all subsystems (context). Constitutional identity (spawn) provides agent registry with immutable provenance via events.

**Design principle:** No central orchestration. Agents are fully autonomous. Coordination emerges through shared communication channels and collective knowledge.

## Module Structure

```
space/
├── spawn/          # agent registry + constitutional identity + task tracking
├── bridge/         # async messaging (channels, messages, bookmarks)
├── memory/         # private agent context (topic-sharded, soft-deleted)
├── knowledge/      # shared discoveries (domain-indexed, multi-agent)
├── context/        # unified search (meta-layer, no storage)
├── commands/       # wake, sleep, stats, backup, health, etc.
├── lib/            # shared utilities (db registry, paths, uuid7, identity)
├── events.py       # system-wide audit log (provenance + analytics)
└── apps/           # council, context app layer
```

## Primitive Overview

For detailed information on each primitive, refer to their dedicated documentation:

-   [Spawn](spawn.md)
-   [Bridge](bridge.md)
-   [Memory](memory.md)
-   [Knowledge](knowledge.md)

## Data Flow Architecture

### Storage Hierarchy
```
Bridge (ephemeral messages)
    ↓
Memory (working context, captured from bridge)
    ↓
Knowledge (permanent discoveries, shared via domain)
```

### Provenance Model
```
Identity invocation (CLI/API)
    ↓
spawn.get_agent(identity) → agent_id
    ↓
Operation runs (bridge.send, memory.add, etc.)
    ↓
events.emit(source, event_type, agent_id, data)
    ↓
events.db record (immutable, searchable)
    ↓
Later: events.query(agent_id=X) shows full audit trail
```

### Context Assembly (Wake)
1. `agent = spawn.get_agent(identity)` → resolve to Agent object
2. `memory.get_core_entries(identity)` → load architectural memories
3. `memory.get_recent_entries(identity, days=7)` → load working context
4. `bridge.fetch_channels(agent.agent_id)` → unread counts
5. `knowledge.query_by_domain("*")` → relevant knowledge
6. Display summary: unread by channel, core memories, recent discoveries
7. Suggest priority action (channel with highest unread density)

### Coordination Flow (Bridge)
1. Agent A: `bridge.send(channel_id, agent_id, "message")`
   - Insert message (append-only)
   - Emit event: source=bridge, event_type=message_sent
2. Agent B: `bridge.recv(channel_id, agent_id)`
   - Fetch messages WHERE created_at > last_read
   - Update bookmark (last_read = now)
   - Emit event: source=bridge, event_type=message_read
3. (Reflections now live under `memory#notes` namespaces instead of bridge.)
</file>

<file path="space/apps/canon/__init__.py">
from space.apps.canon.api import search
from space.apps.canon.cli import app

__all__ = ["app", "search"]
</file>

<file path="space/apps/context/__init__.py">
from space.apps.context.cli import app

__all__ = ["app"]
</file>

<file path="space/apps/context/api.py">
"""Unified context search: routes to memory, knowledge, bridge, provider chats, canon."""

import contextlib
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

from space.apps import canon
from space.lib import providers
from space.os import bridge, knowledge, memory

log = logging.getLogger(__name__)

MAX_SEARCH_LEN = 256


def _get_max_search_len() -> int:
    """Return maximum allowed search length."""
    return MAX_SEARCH_LEN


def _validate_search_term(term: str) -> None:
    """Validate search term to prevent DoS via oversized patterns."""
    max_len = _get_max_search_len()
    if len(term) > max_len:
        raise ValueError(f"Search term too long (max {max_len} chars, got {len(term)})")


def _search_provider_chats(
    query: str, identity: str | None = None, all_agents: bool = False
) -> list[dict]:
    """Search provider chat logs directly (stateless, ephemeral discovery)."""
    results = []
    query_lower = query.lower()

    def _discover_and_search_provider(cli_name: str) -> list[dict]:
        """Discover and search a single provider's chats."""
        provider_results = []
        try:
            provider = getattr(providers, cli_name)()
            sessions = provider.discover_sessions()

            for session in sessions:
                try:
                    file_path = Path(session["file_path"])
                    if not file_path.exists():
                        continue

                    session_id = session["session_id"]
                    messages = provider.parse_messages(file_path)

                    for msg in messages:
                        content = msg.get("content", "")
                        if query_lower in content.lower():
                            provider_results.append(
                                {
                                    "source": "provider-chats",
                                    "cli": cli_name,
                                    "session_id": session_id,
                                    "identity": None,
                                    "role": msg.get("role"),
                                    "text": content,
                                    "timestamp": msg.get("timestamp"),
                                    "reference": f"provider-chats:{cli_name}:{session_id}",
                                }
                            )
                except Exception:
                    pass

        except Exception:
            pass

        return provider_results

    with ThreadPoolExecutor(max_workers=3) as executor:
        futures = {
            executor.submit(_discover_and_search_provider, cli): cli
            for cli in ("claude", "codex", "gemini")
        }
        for future in as_completed(futures):
            with contextlib.suppress(Exception):
                results.extend(future.result())

    return results


def collect_timeline(query: str, identity: str | None, all_agents: bool) -> list[dict]:
    """Unified timeline: evolution across all sources."""
    _validate_search_term(query)
    seen = set()
    timeline = []

    for result in memory.search(query, identity, all_agents):
        key = (result["source"], result.get("memory_id"))
        if key not in seen:
            seen.add(key)
            timeline.append(
                {
                    "source": result["source"],
                    "type": result["topic"],
                    "identity": result["identity"],
                    "data": result["message"],
                    "timestamp": result["timestamp"],
                    "reference": result["reference"],
                }
            )

    for result in knowledge.search(query, identity, all_agents):
        key = (result["source"], result.get("knowledge_id"))
        if key not in seen:
            seen.add(key)
            timeline.append(
                {
                    "source": result["source"],
                    "type": result["domain"],
                    "identity": result["contributor"],
                    "data": result["content"],
                    "timestamp": result["timestamp"],
                    "reference": result["reference"],
                }
            )

    for result in bridge.search(query, identity, all_agents):
        key = (result["source"], result.get("message_id"))
        if key not in seen:
            seen.add(key)
            timeline.append(
                {
                    "source": result["source"],
                    "type": result["channel_name"],
                    "identity": result["sender"],
                    "data": result["content"],
                    "timestamp": result["timestamp"],
                    "reference": result["reference"],
                }
            )

    for result in _search_provider_chats(query, identity, all_agents):
        key = (result["source"], result.get("session_id"))
        if key not in seen:
            seen.add(key)
            timeline.append(
                {
                    "source": result["source"],
                    "type": result["cli"],
                    "identity": result["identity"],
                    "data": result["text"],
                    "timestamp": result["timestamp"],
                    "reference": result["reference"],
                }
            )

    for result in canon.search(query):
        key = (result["source"], result.get("path"))
        if key not in seen:
            seen.add(key)
            timeline.append(
                {
                    "source": result["source"],
                    "type": "documentation",
                    "identity": None,
                    "data": result["content"],
                    "timestamp": 0,
                    "reference": result["reference"],
                }
            )

    timeline.sort(key=lambda x: x["timestamp"])
    return timeline[-10:]


def collect_current_state(query: str, identity: str | None, all_agents: bool) -> dict:
    """Unified state: current entries across all sources."""
    results = {"memory": [], "knowledge": [], "bridge": [], "provider_chats": [], "canon": []}

    results["memory"] = [
        {
            "identity": r["identity"],
            "topic": r["topic"],
            "message": r["message"],
            "reference": r["reference"],
        }
        for r in memory.search(query, identity, all_agents)
    ]

    results["knowledge"] = [
        {
            "domain": r["domain"],
            "content": r["content"],
            "contributor": r["contributor"],
            "reference": r["reference"],
        }
        for r in knowledge.search(query, identity, all_agents)
    ]

    results["bridge"] = [
        {
            "channel": r["channel_name"],
            "sender": r["sender"],
            "content": r["content"],
            "reference": r["reference"],
        }
        for r in bridge.search(query, identity, all_agents)
    ]

    results["provider_chats"] = [
        {
            "cli": r["cli"],
            "session_id": r["session_id"],
            "identity": r["identity"],
            "role": r["role"],
            "text": r["text"],
            "reference": r["reference"],
        }
        for r in _search_provider_chats(query, identity, all_agents)
    ]

    results["canon"] = [
        {"path": r["path"], "content": r["content"], "reference": r["reference"]}
        for r in canon.search(query)
    ]

    return results
</file>

<file path="space/apps/stats/tests/test_stats_api.py">
"""Stats aggregation API tests."""

from unittest.mock import patch

import pytest

from space.apps.stats import api
from space.apps.stats.models import BridgeStats, KnowledgeStats, MemoryStats, SpaceStats, SpawnStats


@pytest.fixture
def mock_apis():
    """Mock all core APIs."""
    with (
        patch("space.apps.stats.api.bridge") as mock_bridge,
        patch("space.apps.stats.api.memory") as mock_memory,
        patch("space.apps.stats.api.knowledge") as mock_knowledge,
        patch("space.apps.stats.api.spawn") as mock_spawn,
    ):
        yield {
            "bridge": mock_bridge,
            "memory": mock_memory,
            "knowledge": mock_knowledge,
            "spawn": mock_spawn,
        }


def test_bridge_stats_returns_bridge_stats_object(mock_apis):
    mock_apis["bridge"].api.stats.return_value = {
        "messages": {"total": 100, "active": 80, "archived": 20, "by_agent": []},
        "channels": {"total": 10, "active": 8, "archived": 2},
        "events": {"total": 0, "by_agent": []},
    }
    mock_apis["spawn"].api.agent_identities.return_value = {}

    result = api.bridge_stats()

    assert isinstance(result, BridgeStats)
    assert result.available is True
    assert result.total == 100
    assert result.active == 80


def test_bridge_stats_handles_exception(mock_apis):
    mock_apis["bridge"].api.stats.side_effect = Exception("DB error")

    result = api.bridge_stats()

    assert isinstance(result, BridgeStats)
    assert result.available is False


def test_memory_stats_returns_memory_stats_object(mock_apis):
    mock_apis["memory"].api.stats.return_value = {
        "total": 50,
        "active": 45,
        "archived": 5,
        "topics": 10,
        "mem_by_agent": [],
    }
    mock_apis["spawn"].api.agent_identities.return_value = {}

    result = api.memory_stats()

    assert isinstance(result, MemoryStats)
    assert result.available is True
    assert result.total == 50


def test_memory_stats_handles_exception(mock_apis):
    mock_apis["memory"].api.stats.side_effect = Exception("DB error")

    result = api.memory_stats()

    assert isinstance(result, MemoryStats)
    assert result.available is False


def test_knowledge_stats_returns_knowledge_stats_object(mock_apis):
    mock_apis["knowledge"].api.stats.return_value = {
        "total": 30,
        "active": 25,
        "archived": 5,
        "topics": 8,
        "know_by_agent": [],
    }
    mock_apis["spawn"].api.agent_identities.return_value = {}

    result = api.knowledge_stats()

    assert isinstance(result, KnowledgeStats)
    assert result.available is True
    assert result.total == 30


def test_knowledge_stats_handles_exception(mock_apis):
    mock_apis["knowledge"].api.stats.side_effect = Exception("DB error")

    result = api.knowledge_stats()

    assert isinstance(result, KnowledgeStats)
    assert result.available is False


def test_spawn_stats_returns_spawn_stats_object(mock_apis):
    mock_apis["spawn"].api.stats.return_value = {
        "total": 5,
        "active": 4,
        "archived": 1,
        "hashes": 2,
    }

    result = api.spawn_stats()

    assert isinstance(result, SpawnStats)
    assert result.available is True
    assert result.total == 5


def test_spawn_stats_handles_exception(mock_apis):
    mock_apis["spawn"].api.stats.side_effect = Exception("DB error")

    result = api.spawn_stats()

    assert isinstance(result, SpawnStats)
    assert result.available is False


def test_collect_returns_space_stats(mock_apis):
    mock_apis["bridge"].api.stats.return_value = {
        "messages": {"total": 100, "active": 80, "archived": 20, "by_agent": []},
        "channels": {"total": 10, "active": 8, "archived": 2},
        "events": {"total": 0, "by_agent": []},
    }
    mock_apis["memory"].api.stats.return_value = {
        "total": 50,
        "active": 45,
        "archived": 5,
        "topics": 10,
        "mem_by_agent": [],
    }
    mock_apis["knowledge"].api.stats.return_value = {
        "total": 30,
        "active": 25,
        "archived": 5,
        "topics": 8,
        "know_by_agent": [],
    }
    mock_apis["spawn"].api.stats.return_value = {
        "total": 5,
        "active": 4,
        "archived": 1,
        "hashes": 2,
    }
    mock_apis["spawn"].api.agent_identities.return_value = {}
    mock_apis["spawn"].api.archived_agents.return_value = set()

    result = api.collect()

    assert isinstance(result, SpaceStats)
    assert isinstance(result.bridge, BridgeStats)
    assert isinstance(result.memory, MemoryStats)
    assert isinstance(result.knowledge, KnowledgeStats)
    assert isinstance(result.spawn, SpawnStats)
</file>

<file path="space/apps/stats/cli.py">
import typer

from space.apps import stats as stats_app

app = typer.Typer(invoke_without_command=True)


def overview():
    """Show space overview."""
    s = stats_app.collect(agent_limit=10)

    lines = [
        """
   ___ _ __   __ _  ___ ___
  / __| '_ \\ / _` |/ __/ _ \\
 _\\__ \\ |_) | (_| | (_|  __/
(_)___/ .__/ \\__,_|\\___\\___|
      | |
      |_|

overview"""
    ]

    if s.spawn.available and s.spawn.total > 0:
        lines.append(
            f"  spawn · {s.spawn.total} spawns · {s.spawn.agents} agents · {s.spawn.hashes} hashes"
        )

    if s.bridge.available:
        lines.append(
            f"  bridge · {s.bridge.active_channels} active · {s.bridge.archived_channels} archived · {s.bridge.active} msgs ({s.bridge.archived} archived)"
        )

    if s.memory.available and s.memory.total > 0:
        archived = s.memory.total - s.memory.active
        lines.append(
            f"  memory · {s.memory.active} active · {archived} archived · {s.memory.topics} topics"
        )

    if s.knowledge.available and s.knowledge.total > 0:
        archived_k = s.knowledge.total - s.knowledge.active
        lines.append(
            f"  knowledge · {s.knowledge.active} active · {archived_k} archived · {s.knowledge.topics} domains"
        )

    if s.agents:
        lines.append("\nagents")
        lines.append("  name · id · e-s-b-m-k")

        sorted_agents = sorted(
            s.agents, key=lambda a: int(a.last_active) if a.last_active else 0, reverse=True
        )

        for a in sorted_agents:
            parts = [a.identity]
            parts.append(f"{a.events}-{a.spawns}-{a.msgs}-{a.mems}-{a.knowledge}")
            lines.append("  " + " · ".join(parts))

    typer.echo("\n".join(lines) + "\n")


@app.callback(invoke_without_command=True)
def main_command(ctx: typer.Context):
    if ctx.invoked_subcommand is None:
        overview()


@app.command()
def memory():
    """Memory lattice health analytics."""
    s = stats_app.collect()

    if not s.memory.available:
        typer.echo("memory not initialized")
        return

    lines = ["memory lattice health\n"]

    total = s.memory.total
    active = s.memory.active
    archived = total - active
    topics = s.memory.topics

    if total == 0:
        lines.append("  no entries yet")
    else:
        retention_rate = (active / total * 100) if total > 0 else 0
        avg_per_topic = active / topics if topics > 0 else 0

        lines.append(f"  nodes · {active} active · {archived} archived")
        lines.append(f"  topics · {topics} domains")
        lines.append(f"  retention · {retention_rate:.1f}%")
        lines.append(f"  density · {avg_per_topic:.1f} nodes/topic")

        if s.memory.leaderboard:
            lines.append("\n  contributors")
            for entry in s.memory.leaderboard[:10]:
                lines.append(f"    {entry.identity} · {entry.count}")

    typer.echo("\n".join(lines) + "\n")


@app.command()
def knowledge():
    """Knowledge graph health analytics."""
    s = stats_app.collect()

    if not s.knowledge.available:
        typer.echo("knowledge not initialized")
        return

    lines = ["knowledge graph health\n"]

    total = s.knowledge.total
    active = s.knowledge.active
    archived = total - active
    domains = s.knowledge.topics

    if total == 0:
        lines.append("  no entries yet")
    else:
        retention_rate = (active / total * 100) if total > 0 else 0
        avg_per_domain = active / domains if domains > 0 else 0

        lines.append(f"  nodes · {active} active · {archived} archived")
        lines.append(f"  domains · {domains}")
        lines.append(f"  retention · {retention_rate:.1f}%")
        lines.append(f"  density · {avg_per_domain:.1f} nodes/domain")

        if s.knowledge.leaderboard:
            lines.append("\n  contributors")
            for entry in s.knowledge.leaderboard[:10]:
                lines.append(f"    {entry.identity} · {entry.count}")

    typer.echo("\n".join(lines) + "\n")


@app.command()
def bridge():
    """Bridge channel health analytics."""
    s = stats_app.collect()

    if not s.bridge.available:
        typer.echo("bridge not initialized")
        return

    lines = ["bridge channel health\n"]

    total = s.bridge.total
    active = s.bridge.active
    archived = s.bridge.archived
    channels = s.bridge.active_channels
    archived_channels = s.bridge.archived_channels

    if total == 0:
        lines.append("  no messages yet")
    else:
        retention_rate = (active / total * 100) if total > 0 else 0
        avg_per_channel = active / channels if channels > 0 else 0

        lines.append(f"  messages · {active} active · {archived} archived")
        lines.append(f"  channels · {channels} active · {archived_channels} archived")
        lines.append(f"  retention · {retention_rate:.1f}%")
        lines.append(f"  density · {avg_per_channel:.1f} msgs/channel")

        if s.bridge.message_leaderboard:
            lines.append("\n  contributors")
            for entry in s.bridge.message_leaderboard[:10]:
                lines.append(f"    {entry.identity} · {entry.count}")

    typer.echo("\n".join(lines) + "\n")
</file>

<file path="space/os/bridge/api/__init__.py">
"""Bridge operations: pure business logic, zero typer imports.

Functions handle all DB interactions and state management.
Callers: commands/ layer only.
"""

from .channels import (
    archive_channel,
    create_channel,
    delete_channel,
    export_channel,
    fetch_inbox,
    get_channel,
    list_channels,
    pin_channel,
    rename_channel,
    resolve_channel,
    unpin_channel,
)
from .mentions import spawn_from_mentions
from .messaging import (
    get_messages,
    get_sender_history,
    recv_messages,
    send_message,
    set_bookmark,
)
from .search import search
from .stats import stats

__all__ = [
    "archive_channel",
    "create_channel",
    "delete_channel",
    "export_channel",
    "get_channel",
    "get_messages",
    "get_sender_history",
    "fetch_inbox",
    "list_channels",
    "pin_channel",
    "recv_messages",
    "rename_channel",
    "resolve_channel",
    "search",
    "send_message",
    "set_bookmark",
    "spawn_from_mentions",
    "stats",
    "unpin_channel",
]
</file>

<file path="space/os/bridge/api/mentions.py">
"""Agent spawning: @mentions and task orchestration."""

import logging
import re
import subprocess
import sys
from pathlib import Path

from space.lib import paths
from space.os.spawn.api import agents as spawn_agents
from space.os.spawn.api.main import spawn_prompt
from space.os.spawn.api.tasks import complete_task, create_task, fail_task, start_task

logging.basicConfig(level=logging.DEBUG, format="[worker] %(message)s")
log = logging.getLogger(__name__)

AGENT_PROMPT_TEMPLATE = """{context}

[SPACE INSTRUCTIONS]
{task}

Infer the actual task from bridge context. If ambiguous, ask for clarification."""


def _write_role_file(provider: str, constitution: str) -> None:
    """Write pure constitution to agent home dir file."""
    filename_map = {
        "claude": "CLAUDE.md",
        "gemini": "GEMINI.md",
        "codex": "AGENTS.md",
    }
    agent_dir_map = {
        "claude": ".claude",
        "gemini": ".gemini",
        "codex": ".codex",
    }
    filename = filename_map.get(provider)
    agent_dir = agent_dir_map.get(provider)
    if not filename or not agent_dir:
        raise ValueError(f"Unknown provider: {provider}")

    target = Path.home() / agent_dir / filename
    target.parent.mkdir(parents=True, exist_ok=True)
    target.write_text(constitution)


def _parse_mentions(content: str) -> list[str]:
    """Extract @identity mentions from content."""
    pattern = r"@([\w-]+)"
    matches = re.findall(pattern, content)
    return list(set(matches))


def _build_prompt(identity: str, channel: str, content: str) -> str | None:
    """Build agent prompt with channel context and task, write constitution."""
    try:
        agent = spawn_agents.get_agent(identity)
        if not agent:
            log.warning(f"Identity {identity} not found in registry")
            return None

        const_path = paths.constitution(agent.constitution)
        constitution = const_path.read_text()

        _write_role_file(agent.provider, constitution)

        identity_prompt = spawn_prompt(identity, agent.model)

        export = subprocess.run(
            ["bridge", "export", channel],
            capture_output=True,
            text=True,
            timeout=30,
        )
        if export.returncode != 0:
            log.error(
                f"bridge export failed for {channel}: "
                f"returncode={export.returncode}, stderr={export.stderr[:200]}"
            )
            return None
        return (
            identity_prompt
            + "\n\n"
            + AGENT_PROMPT_TEMPLATE.format(context=export.stdout, task=content)
        )
    except subprocess.TimeoutExpired:
        log.error(f"bridge export timed out for {channel}")
        return None
    except Exception as e:
        log.error(f"Building prompt for {identity} failed: {e}", exc_info=True)
        return None


def _get_task_timeout(identity: str) -> int:
    """Get task timeout for identity. Uses default since config is gone."""
    return 120


def spawn_from_mentions(channel_id: str, content: str) -> None:
    """Spawn agents from @mentions in message content."""
    try:
        from . import channels

        channel = channels.get_channel(channel_id)
        if not channel:
            log.error(f"Channel {channel_id} not found")
            return
        channel_name = channel.name
        subprocess.run(
            [sys.argv[0], channel_id, channel_name, content],
            check=False,
        )
    except Exception as e:
        log.error(f"Failed to spawn from mentions: {e}")


def main():
    if len(sys.argv) != 4:
        log.error(f"Invalid args: {len(sys.argv)}, expected 4. argv={sys.argv}")
        return

    channel_id = sys.argv[1]
    channel_name = sys.argv[2]
    content = sys.argv[3]

    log.info(f"Processing channel={channel_name}, content={content[:50]}")

    mentions = _parse_mentions(content)
    log.info(f"Found mentions: {mentions}")
    if not mentions:
        log.info("No mentions, skipping")
        return

    results = []
    for identity in mentions:
        log.info(f"Spawning {identity}")
        prompt = _build_prompt(identity, channel_name, content)
        if prompt:
            log.info(f"Got prompt, running spawn {identity}")
            timeout = _get_task_timeout(identity)
            try:
                task_id = create_task(identity=identity, input=prompt, channel_id=channel_id)
                start_task(task_id)

                result = subprocess.run(
                    ["spawn", identity, prompt, "--channel", channel_name],
                    capture_output=True,
                    text=True,
                    timeout=timeout,
                    stdin=subprocess.DEVNULL,
                )

                log.info(
                    f"Spawn returncode={result.returncode}, stdout_len={len(result.stdout)}, stderr={result.stderr[:100]}"
                )

                if result.returncode == 0 and result.stdout.strip():
                    complete_task(
                        task_id,
                        output=result.stdout.strip(),
                    )
                    results.append((identity, result.stdout.strip()))
                else:
                    fail_task(task_id, stderr=result.stderr)
                    log.error(f"Spawn failed: {result.stderr}")
            except subprocess.TimeoutExpired:
                fail_task(
                    task_id,
                    stderr=f"Spawn timeout ({timeout}s)",
                )
                log.error(f"Spawn timeout for {identity}")
            except Exception as e:
                fail_task(task_id, stderr=str(e))
                log.error(f"Spawn error: {e}")
        else:
            log.warning(f"No prompt for {identity}")

    if results:
        from . import messaging

        for identity, output in results:
            messaging.send_message(channel_id, identity, output)
    elif mentions:
        log.warning(f"No results from spawning {len(mentions)} agent(s))")


if __name__ == "__main__":
    main()
</file>

<file path="space/os/bridge/ops/channels.py">
"""Channel operations: create, list, rename, archive, pin, unpin, delete."""

from space.os.bridge.api import channels as ch


def list_channels(all: bool = False, agent_id: str | None = None):
    """List all channels.

    Args:
        all: Include archived channels.
        agent_id: Filter for channels with unread messages for this agent.

    Returns:
        List of Channel objects (active, optionally archived).

    Raises:
        None
    """
    return ch.list_channels(all=all, agent_id=agent_id)


def create_channel(channel_name: str, topic: str | None = None):
    """Create a new channel.

    Args:
        channel_name: Name of the channel.
        topic: Optional channel topic.

    Returns:
        Channel ID.

    Raises:
        ValueError: If channel already exists or invalid name.
    """
    return ch.create_channel(channel_name, topic)


def rename_channel(old_channel: str, new_channel: str) -> bool:
    """Rename a channel.

    Args:
        old_channel: Current channel name (stripped of #).
        new_channel: New channel name (stripped of #).

    Returns:
        True if successful, False if old_channel not found or new_channel exists.

    Raises:
        None
    """
    old_channel = old_channel.lstrip("#")
    new_channel = new_channel.lstrip("#")
    return ch.rename_channel(old_channel, new_channel)


def archive_channel(channel_name: str):
    """Archive a single channel.

    Args:
        channel_name: Name of channel to archive.

    Raises:
        ValueError: If channel not found.
    """
    ch.archive_channel(channel_name)


def pin_channel(channel_name: str):
    """Pin a channel to favorites.

    Args:
        channel_name: Name of channel to pin.

    Raises:
        ValueError: If channel not found.
        TypeError: If invalid channel.
    """
    ch.pin_channel(channel_name)


def unpin_channel(channel_name: str):
    """Unpin a channel from favorites.

    Args:
        channel_name: Name of channel to unpin.

    Raises:
        ValueError: If channel not found.
        TypeError: If invalid channel.
    """
    ch.unpin_channel(channel_name)


def delete_channel(channel_name: str):
    """Delete a channel permanently.

    Args:
        channel_name: Name of channel to delete.

    Raises:
        ValueError: If channel not found.
    """
    ch.delete_channel(channel_name)


def fetch_inbox(agent_id: str):
    """Fetch inbox channels for an agent.

    Args:
        agent_id: Agent ID.

    Returns:
        List of Channel objects with unread messages.

    Raises:
        None
    """
    return ch.fetch_inbox(agent_id)
</file>

<file path="space/os/bridge/ops/messages.py">
"""Message operations: send, receive, wait."""

import base64
import binascii
import time

from space.os.bridge.api import channels as ch
from space.os.bridge.api import mentions, messaging


def send_message(channel: str, identity: str, content: str, decode_base64: bool = False):
    """Send a message to a channel.

    Args:
        channel: Channel name or ID.
        identity: Sender identity (caller responsible for validation).
        content: Message content (or base64-encoded if decode_base64=True).
        decode_base64: If True, decode content from base64.

    Raises:
        ValueError: If channel not found.
        ValueError: If base64 payload invalid.
    """
    if decode_base64:
        try:
            payload = base64.b64decode(content, validate=True)
            content = payload.decode("utf-8")
        except (binascii.Error, UnicodeDecodeError) as exc:
            raise ValueError("Invalid base64 payload") from exc

    channel_id = ch.resolve_channel(channel).channel_id
    agent_id = messaging.send_message(channel_id, identity, content)
    mentions.spawn_from_mentions(channel_id, content)
    return agent_id


def recv_messages(channel: str, agent_id: str):
    """Receive unread messages from a channel.

    Args:
        channel: Channel name or ID.
        agent_id: Agent ID (caller responsible for validation).

    Returns:
        Tuple of (messages, count, context, participants).

    Raises:
        ValueError: If channel not found.
    """
    channel_id = ch.resolve_channel(channel).channel_id
    return messaging.recv_messages(channel_id, agent_id)


def wait_for_message(channel: str, agent_id: str, poll_interval: float = 0.1):
    """Wait for a new message from others in a channel (blocking).

    Args:
        channel: Channel name or ID.
        agent_id: Agent ID (caller responsible for validation).
        poll_interval: Polling interval in seconds.

    Returns:
        Tuple of (messages, count, context, participants) for messages from others.

    Raises:
        ValueError: If channel not found.
        KeyboardInterrupt: If user interrupts.
    """
    channel_id = ch.resolve_channel(channel).channel_id

    while True:
        msgs, count, context, participants = messaging.recv_messages(channel_id, agent_id)
        other_messages = [msg for msg in msgs if msg.agent_id != agent_id]

        if other_messages:
            return other_messages, len(other_messages), context, participants

        time.sleep(poll_interval)
</file>

<file path="space/os/spawn/api/main.py">
"""Agent launching: unified context injection, execute."""

import os
import shlex
import shutil
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path

import click

from space.lib import paths
from space.lib.constitution import write_constitution
from space.lib.format import format_duration
from space.lib.providers import claude, codex, gemini
from space.os import bridge, knowledge, memory

from . import agents, sessions


def spawn_prompt(identity: str, model: str | None = None) -> str:
    """Build unified prompt context from MANUAL.md template with agent context filled in.

    Replaces <identity> placeholders and inserts agent-specific context blocks.
    """
    try:
        agent = agents.get_agent(identity)
    except ValueError:
        agent = None
    agent_id = agent.agent_id if agent else None
    resolved_model = model

    manual_path = paths.package_root().parent / "MANUAL.md"
    if not manual_path.exists():
        base = f"You are {identity}."
        if resolved_model:
            base += f" Your model is {resolved_model}."
        return base

    manual_text = manual_path.read_text()

    if agent_id:
        try:
            spawn_count = sessions.get_spawn_count(agent_id)
        except Exception:  # pragma: no cover - defensive because DB may be unavailable in tests
            spawn_count = 0
    else:
        spawn_count = 0

    spawn_status = "📝 First spawn"
    try:
        last_journal = memory.list_entries(identity, topic="journal", limit=1)
        if last_journal:
            entry = last_journal[0]
            created_at = datetime.fromisoformat(entry.created_at)
            last_sleep_duration = format_duration((datetime.now() - created_at).total_seconds())
            spawn_status = f"📝 Last session {last_sleep_duration} ago"
    except Exception:  # pragma: no cover - defensive for missing memory DB
        pass

    template_vars = {
        "identity": identity,
        "spawn_count": spawn_count,
        "spawn_status": spawn_status,
        "model": f" Your model is {resolved_model}." if resolved_model else "",
    }

    output = manual_text
    for var, value in template_vars.items():
        output = output.replace(f"<{var}>", str(value))

    agent_info_blocks = _build_agent_info_blocks(identity, agent, agent_id)
    return output.replace("{{AGENT_INFO}}", agent_info_blocks or "")


def _build_agent_info_blocks(identity: str, agent, agent_id: str | None) -> str:
    """Build identity, memories, and bridge context blocks for template injection."""
    parts = []

    if not agent or not agent_id:
        return ""

    if agent.description:
        parts.append(f"**Your identity:** {agent.description}")
        parts.append("")

    try:
        core_entries = memory.list_entries(identity, filter="core")
    except Exception:  # pragma: no cover - safeguard for unseeded DBs
        core_entries = []
    if core_entries:
        parts.append("⭐ **Core memories:**")
        for entry in core_entries[:3]:
            parts.append(f"  [{entry.memory_id[-8:]}] {entry.message}")
        parts.append("")

    try:
        recent = memory.list_entries(identity, filter="recent:7", limit=3)
    except Exception:  # pragma: no cover - safeguard for unseeded DBs
        recent = []
    non_journal = [entry for entry in recent if entry.topic != "journal"]
    if non_journal:
        parts.append("📋 **Recent work (7d):**")
        for entry in non_journal:
            ts = datetime.fromisoformat(entry.created_at).strftime("%m-%d %H:%M")
            parts.append(f"  [{ts}] {entry.topic}: {entry.message[:100]}")
        parts.append("")

    critical = _get_critical_knowledge()
    if critical:
        parts.append(f"💡 **Latest decision:** [{critical.domain}] {critical.content[:100]}")
        parts.append("")

    try:
        inbox_channels = bridge.fetch_inbox(agent_id)
    except Exception:  # pragma: no cover - safeguard for offline bridge
        inbox_channels = []
    if inbox_channels:
        total_msgs = sum(ch.unread_count for ch in inbox_channels)
        parts.append(f"📬 **{total_msgs} unread messages in {len(inbox_channels)} channels:**")
        priority_ch = _priority_channel(inbox_channels)
        if priority_ch:
            parts.append(f"  #{priority_ch.name} ({priority_ch.unread_count} unread) ← START HERE")
            for channel in inbox_channels[:4]:
                if channel.name != priority_ch.name:
                    parts.append(f"  #{channel.name} ({channel.unread_count} unread)")
        else:
            for channel in inbox_channels[:5]:
                parts.append(f"  #{channel.name} ({channel.unread_count} unread)")
        if len(inbox_channels) > 5:
            parts.append(f"  ... and {len(inbox_channels) - 5} more")
        parts.append("")

    return "\n".join(parts)


def _get_critical_knowledge():
    """Get most recent critical knowledge entry (24h)."""
    critical_domains = {"decision", "architecture", "operations", "consensus"}
    try:
        entries = knowledge.list_entries()
    except Exception:  # pragma: no cover - safeguard when knowledge DB uninitialized
        return None

    cutoff = datetime.now() - timedelta(hours=24)
    recent = [
        e
        for e in entries
        if e.domain in critical_domains and datetime.fromisoformat(e.created_at) > cutoff
    ]

    return recent[0] if recent else None


def _priority_channel(channels):
    """Identify highest priority channel."""
    if not channels:
        return None

    feedback_channel = next(
        (ch for ch in channels if ch.name == "space-feedback" and ch.unread_count > 0), None
    )
    if feedback_channel:
        return feedback_channel

    return max(channels, key=lambda ch: (ch.unread_count, ch.last_activity or ""))


def spawn_agent(identity: str, extra_args: list[str] | None = None):
    """Spawn an agent by identity from registry.

    Looks up agent, writes constitution to provider home dir,
    injects unified context via stdin, and executes the provider CLI.

    Args:
        identity: Agent identity from registry
        extra_args: Additional CLI arguments forwarded to provider
    """
    from . import sessions

    agent = agents.get_agent(identity)
    if not agent:
        raise ValueError(f"Agent '{identity}' not found in registry")

    constitution_text = None
    if agent.constitution:
        const_path = paths.constitution(agent.constitution)
        constitution_text = const_path.read_text()

    provider_cmd = _get_provider_command(agent.provider)
    if constitution_text:
        _write_constitution(agent.provider, constitution_text)

    command_tokens = _parse_command(provider_cmd)
    env = _build_launch_env()
    workspace_root = paths.space_root()
    env["PWD"] = str(workspace_root)
    command_tokens[0] = _resolve_executable(command_tokens[0], env)

    passthrough = extra_args or []
    model_args = ["--model", agent.model]

    click.echo(f"Spawning {identity}...\n")
    session_id = sessions.create_session(agent.agent_id)

    context = spawn_prompt(identity, agent.model)
    has_prompt = bool(context.strip())

    provider_obj = {"claude": claude, "gemini": gemini, "codex": codex}.get(agent.provider)
    if provider_obj:
        if agent.provider == "gemini":
            launch_args = provider_obj.launch_args(has_prompt=has_prompt)
        else:
            launch_args = provider_obj.launch_args()
    else:
        launch_args = []
    full_command = command_tokens + [context] + model_args + launch_args + passthrough
    display_command = command_tokens + ['"<space_manual>"'] + model_args + launch_args + passthrough

    click.echo(f"Executing: {' '.join(display_command)}")
    click.echo("")

    proc = subprocess.Popen(full_command, env=env, cwd=str(workspace_root))

    try:
        proc.wait()
    finally:
        sessions.end_session(session_id)


def _get_provider_command(provider: str) -> str:
    """Map provider name to CLI command."""
    provider_map = {
        "claude": "claude",
        "gemini": "gemini",
        "codex": "codex",
    }
    cmd = provider_map.get(provider)
    if not cmd:
        raise ValueError(f"Unknown provider: {provider}")
    return cmd


def _write_constitution(provider: str, constitution: str) -> None:
    """Write constitution to provider home dir."""
    write_constitution(provider, constitution)


def _parse_command(command: str | list[str]) -> list[str]:
    """Return the command tokens for launching an agent."""
    if isinstance(command, list):
        if not command:
            raise ValueError("Command list cannot be empty")
        return list(command)

    tokens = shlex.split(command)
    if not tokens:
        raise ValueError("Command cannot be empty")
    return tokens


def _build_launch_env() -> dict[str, str]:
    """Return environment variables for launching outside the poetry venv."""
    env = os.environ.copy()
    venv_paths = _virtualenv_bin_paths(env)
    env.pop("VIRTUAL_ENV", None)
    env.pop("CLAUDE_CODE_ENTRYPOINT", None)
    env.pop("CLAUDECODE", None)
    original_path = env.get("PATH", "")
    filtered_parts: list[str] = []
    for part in original_path.split(os.pathsep):
        if part and part not in venv_paths:
            filtered_parts.append(part)

    seen: set[str] = set()
    deduped_parts: list[str] = []
    for part in filtered_parts:
        if part not in seen:
            seen.add(part)
            deduped_parts.append(part)

    env["PATH"] = os.pathsep.join(deduped_parts)
    return env


def _virtualenv_bin_paths(env: dict[str, str]) -> set[str]:
    """Collect bin directories for active virtual environments."""
    paths: set[str] = set()

    venv_root = env.get("VIRTUAL_ENV")
    if venv_root:
        paths.add(str(Path(venv_root) / "bin"))

    prefix = Path(sys.prefix)
    base_prefix = Path(getattr(sys, "base_prefix", sys.prefix))
    if prefix != base_prefix:
        paths.add(str(prefix / "bin"))

    exec_prefix = Path(sys.exec_prefix)
    base_exec_prefix = Path(getattr(sys, "base_exec_prefix", sys.exec_prefix))
    if exec_prefix != base_exec_prefix:
        paths.add(str(exec_prefix / "bin"))

    return paths


def _resolve_executable(executable: str, env: dict[str, str]) -> str:
    """Resolve the executable path using the sanitized PATH."""
    if os.path.isabs(executable):
        return executable

    search_path = env.get("PATH") or None
    resolved = shutil.which(executable, path=search_path)
    if not resolved:
        raise ValueError(f"Executable '{executable}' not found on PATH")
    return resolved
</file>

<file path="space/os/spawn/cli/__init__.py">
"""Spawn CLI commands: typer entry point."""

import click
import typer
from typer.core import TyperGroup

from space.lib import errors, output
from space.os.spawn import api

errors.install_error_handler("spawn")


class SpawnGroup(TyperGroup):
    """Typer group that dynamically spawns tasks for agent names."""

    def get_command(self, ctx, cmd_name):
        """Get command by name, or spawn agent if not found."""
        cmd = super().get_command(ctx, cmd_name)
        if cmd is not None:
            return cmd

        agent = api.get_agent(cmd_name)
        if agent is None:
            return None

        @click.command(name=cmd_name)
        @click.argument("task_input", required=False, nargs=-1)
        def spawn_agent(task_input):
            input_list = list(task_input) if task_input else []
            api.spawn_agent(agent.identity, extra_args=input_list)

        return spawn_agent


app = typer.Typer(invoke_without_command=True, cls=SpawnGroup)

from . import (  # noqa: E402
    agents,
    clone,
    merge,
    models,
    register,
    rename,
    sleep,
    tasks,
    update,
)

app.command("agents")(agents.list_agents)
app.add_typer(tasks.app, name="tasks")
app.add_typer(sleep.sleep, name="sleep")
app.command()(merge.merge)
app.command()(register.register)
app.command()(update.update)
app.command()(clone.clone)
app.command()(rename.rename)
app.command("models")(models.list_models)


@app.callback()
def main_callback(
    ctx: typer.Context,
    json_output: bool = typer.Option(False, "--json", "-j", help="Output in JSON format."),
    quiet_output: bool = typer.Option(
        False, "--quiet", "-q", help="Suppress non-essential output."
    ),
):
    """Spawn: Agent Management & Task Orchestration"""
    output.set_flags(ctx, json_output, quiet_output)
    if ctx.obj is None:
        ctx.obj = {}

    if ctx.resilient_parsing:
        return
    if ctx.invoked_subcommand is None:
        typer.echo("spawn <agent>: Launch agent. Run 'spawn agents' to list registered agents.")
</file>

<file path="tests/integration/test_spawn_tasks.py">
from space.os import spawn


def test_list_tasks(test_space, default_agents):
    zealot_id = default_agents["zealot"]
    sentinel_id = default_agents["sentinel"]

    t1 = spawn.create_task(role=zealot_id, input="task 1")
    t2 = spawn.create_task(role=sentinel_id, input="task 2")
    spawn.complete_task(t1)

    all_tasks = spawn.list_tasks()
    assert len(all_tasks) == 2

    pending = spawn.list_tasks(status="pending")
    assert len(pending) == 1
    assert pending[0].task_id == t2

    zealot_tasks = spawn.list_tasks(role=zealot_id)
    assert len(zealot_tasks) == 1
    assert zealot_tasks[0].task_id == t1

    from space.lib.uuid7 import uuid7
    from space.os.bridge.api import channels as bridge_api

    channel = bridge_api.create_channel(name=f"ch-spawn-test-{uuid7()}")
    task_id = spawn.create_task(
        role=zealot_id,
        input="list repos",
        channel_id=channel.channel_id,
    )
    channel_tasks = spawn.list_tasks(channel_id=channel.channel_id)
    assert len(channel_tasks) == 1
    assert channel_tasks[0].task_id == task_id

    task = spawn.get_task(task_id)
    assert task.channel_id == channel.channel_id

    spawn.start_task(task_id)
    spawn.complete_task(task_id, output="done")

    task = spawn.get_task(task_id)
    assert task.channel_id == channel.channel_id
    assert task.status == "completed"


def test_multiple_tasks_per_identity(test_space, default_agents):
    zealot_id = default_agents["zealot"]

    t1 = spawn.create_task(role=zealot_id, input="task 1")
    t2 = spawn.create_task(role=zealot_id, input="task 2")
    t3 = spawn.create_task(role=zealot_id, input="task 3")

    spawn.start_task(t1)
    spawn.start_task(t2)
    zealot_tasks = spawn.list_tasks(role=zealot_id)
    assert len(zealot_tasks) == 3

    running = spawn.list_tasks(status="running", role=zealot_id)
    assert len(running) == 2
    assert {t.task_id for t in running} == {t1, t2}

    pending = spawn.list_tasks(status="pending", role=zealot_id)
    assert len(pending) == 1
    assert pending[0].task_id == t3
</file>

<file path="tests/unit/apps/canon/test_commands.py">
import tempfile
from pathlib import Path
from unittest.mock import patch

from typer.testing import CliRunner

from space.apps.canon.cli import app

runner = CliRunner()


def test_canon_shows_tree_when_no_args():
    """Canon shows directory tree when called with no arguments."""
    with tempfile.TemporaryDirectory() as tmpdir:
        canon_root = Path(tmpdir)
        (canon_root / "INDEX.md").write_text("# Index")
        (canon_root / "constitutions").mkdir()
        (canon_root / "constitutions" / "zealot.md").write_text("# Zealot")

        with patch("space.apps.canon.cli.canon_path", return_value=canon_root):
            result = runner.invoke(app, [])

        assert result.exit_code == 0
        assert "INDEX.md" in result.stdout
        assert "constitutions" in result.stdout
        assert "Navigate with: space canon <path>" in result.stdout


def test_canon_reads_document():
    """Canon reads and displays document content."""
    with tempfile.TemporaryDirectory() as tmpdir:
        canon_root = Path(tmpdir)
        content = "# Test Document\nThis is test content."
        (canon_root / "test.md").write_text(content)

        with patch("space.apps.canon.cli.canon_path", return_value=canon_root):
            result = runner.invoke(app, ["test.md"])

        assert result.exit_code == 0
        assert "# Test Document" in result.stdout
        assert "This is test content." in result.stdout


def test_canon_reads_nested_document():
    """Canon reads documents in subdirectories."""
    with tempfile.TemporaryDirectory() as tmpdir:
        canon_root = Path(tmpdir)
        (canon_root / "subdir").mkdir()
        content = "# Nested Document"
        (canon_root / "subdir" / "nested.md").write_text(content)

        with patch("space.apps.canon.cli.canon_path", return_value=canon_root):
            result = runner.invoke(app, ["subdir/nested.md"])

        assert result.exit_code == 0
        assert "# Nested Document" in result.stdout


def test_canon_handles_missing_document():
    """Canon handles missing documents gracefully."""
    with tempfile.TemporaryDirectory() as tmpdir:
        canon_root = Path(tmpdir)
        (canon_root / "exists.md").write_text("# Exists")

        with patch("space.apps.canon.cli.canon_path", return_value=canon_root):
            result = runner.invoke(app, ["missing.md"])

        assert result.exit_code == 1
        assert "Document not found" in result.stdout


def test_canon_handles_missing_canon_root():
    """Canon handles missing canon directory."""
    with patch("space.apps.canon.cli.canon_path", return_value=Path("/nonexistent")):
        result = runner.invoke(app, [])

    assert result.exit_code == 1
    assert "Canon directory not found" in result.output
</file>

<file path="tests/unit/lib/test_sqlite.py">
"""Tests for space.lib.sqlite backend."""

import sqlite3
import tempfile
from pathlib import Path

import pytest

from space.lib import store


@pytest.fixture
def temp_db_dir():
    """Create temporary directory for test databases."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def clean_registry():
    """Clear registry before and after each test."""
    store._reset_for_testing()
    yield
    store._reset_for_testing()


def test_connect_basic(temp_db_dir):
    """Test basic connection to SQLite database."""
    db_path = temp_db_dir / "test.db"
    conn = store.connect(db_path)

    assert isinstance(conn, sqlite3.Connection)
    assert conn.row_factory == sqlite3.Row
    conn.close()
    assert db_path.exists()


def test_register_database(clean_registry):
    """Test registering a database."""
    store.register("test_db", "test.db")

    registry = store.registry()
    assert "test_db" in registry
    assert registry["test_db"] == "test.db"


def test_register_migrations(clean_registry):
    """Test registering migrations."""
    migs = [("v1", "CREATE TABLE test (id TEXT)")]
    store.add_migrations("test_db", migs)

    from space.lib.store.registry import get_migrations

    assert get_migrations("test_db") == migs


def test_ensure_unregistered_raises(clean_registry, temp_db_dir, monkeypatch):
    """Test ensure raises error for unregistered database."""
    monkeypatch.setattr("space.lib.paths.dot_space", lambda: temp_db_dir)

    with pytest.raises(ValueError, match="not registered"):
        store.ensure("nonexistent")


def test_ensure_creates_schema(clean_registry, temp_db_dir, monkeypatch):
    """Test ensure creates schema for new database."""
    monkeypatch.setattr("space.lib.paths.dot_space", lambda: temp_db_dir)

    migs = [("init", "CREATE TABLE test (id TEXT PRIMARY KEY, value TEXT)")]
    store.register("test_db", "test.db")
    store.add_migrations("test_db", migs)

    conn = store.ensure("test_db")

    cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
    tables = {row[0] for row in cursor.fetchall()}
    assert "test" in tables

    conn.close()


def test_migrate_basic(clean_registry, temp_db_dir, monkeypatch):
    """Test basic migration."""
    monkeypatch.setattr("space.lib.paths.dot_space", lambda: temp_db_dir)

    migs = [
        ("init", "CREATE TABLE test (id TEXT PRIMARY KEY)"),
        ("add_value", "ALTER TABLE test ADD COLUMN value TEXT"),
    ]
    store.register("test_db", "test.db")
    store.add_migrations("test_db", migs)

    conn = store.ensure("test_db")

    cursor = conn.execute("PRAGMA table_info(test)")
    columns = {row[1] for row in cursor.fetchall()}
    assert "value" in columns

    conn.close()


def test_migrate_callable(clean_registry, temp_db_dir, monkeypatch):
    """Test migration with callable."""
    monkeypatch.setattr("space.lib.paths.dot_space", lambda: temp_db_dir)

    def add_column(conn):
        conn.execute("ALTER TABLE test ADD COLUMN computed TEXT")

    migs = [("init", "CREATE TABLE test (id TEXT PRIMARY KEY)"), ("add_computed", add_column)]
    store.register("test_db", "test.db")
    store.add_migrations("test_db", migs)

    conn = store.ensure("test_db")

    cursor = conn.execute("PRAGMA table_info(test)")
    columns = {row[1] for row in cursor.fetchall()}
    assert "computed" in columns

    conn.close()


def test_migrate_skips_applied(clean_registry, temp_db_dir, monkeypatch):
    """Test migrations are applied only once."""
    monkeypatch.setattr("space.lib.paths.dot_space", lambda: temp_db_dir)

    call_count = 0

    def track_call(conn):
        nonlocal call_count
        call_count += 1

    migs = [("init", "CREATE TABLE test (id TEXT PRIMARY KEY)"), ("track", track_call)]
    store.register("test_db", "test.db")
    store.add_migrations("test_db", migs)

    store.ensure("test_db")
    assert call_count == 1

    store.ensure("test_db")
    assert call_count == 1


def test_ensure_schema_with_migrations(clean_registry, temp_db_dir):
    """Test ensure_schema applies migrations."""
    db_path = temp_db_dir / "test.db"
    migs = [
        ("init", "CREATE TABLE test (id TEXT PRIMARY KEY)"),
        ("v1", "ALTER TABLE test ADD COLUMN value TEXT"),
    ]

    from space.lib.store import migrations

    migrations.ensure_schema(db_path, migs)

    conn = store.connect(db_path)
    cursor = conn.execute("PRAGMA table_info(test)")
    columns = {row[1] for row in cursor.fetchall()}
    assert "value" in columns
    conn.close()
</file>

<file path=".gitignore">
__pycache__/
*.py[cod]
*$py.class

build/
dist/

.env
.venv/
env/
venv/

.ruff_cache/
.pytest_cache/
.mypy_cache/
.coverage*

.space/
bin/
*.db

.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp
</file>

<file path="docs/schema.md">
# Canonical Schema – `space.db`

`space.db` is the single source of truth for agents, bridge messaging, memory, and knowledge.  
The schema lives in `space/os/db/migrations/001_foundation.sql`; the excerpt below is the canonical contract.

```sql
-- agents and lifecycle
CREATE TABLE agents (
    agent_id TEXT PRIMARY KEY,
    identity TEXT NOT NULL UNIQUE,
    constitution TEXT,
    model TEXT NOT NULL,
    self_description TEXT,
    archived_at TEXT,
    created_at TEXT NOT NULL,
    last_active_at TEXT,
    spawn_count INTEGER NOT NULL DEFAULT 0
);

CREATE TABLE sessions (
    session_id TEXT PRIMARY KEY,
    agent_id TEXT NOT NULL REFERENCES agents(agent_id) ON DELETE CASCADE,
    spawn_number INTEGER NOT NULL,
    started_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    ended_at TEXT,
    wakes INTEGER NOT NULL DEFAULT 0
);

CREATE TABLE tasks (
    task_id TEXT PRIMARY KEY,
    agent_id TEXT NOT NULL REFERENCES agents(agent_id) ON DELETE CASCADE,
    channel_id TEXT REFERENCES channels(channel_id) ON DELETE SET NULL,
    input TEXT NOT NULL,
    output TEXT,
    stderr TEXT,
    status TEXT NOT NULL DEFAULT 'pending',
    pid INTEGER,
    started_at TEXT,
    completed_at TEXT,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now'))
);

-- bridge (channels, messages, bookmarks)
CREATE TABLE channels (
    channel_id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    topic TEXT,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    archived_at TEXT,
    pinned_at TEXT
);

CREATE TABLE messages (
    message_id TEXT PRIMARY KEY,
    channel_id TEXT NOT NULL REFERENCES channels(channel_id) ON DELETE CASCADE,
    agent_id TEXT NOT NULL REFERENCES agents(agent_id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now'))
);

CREATE TABLE bookmarks (
    agent_id TEXT NOT NULL REFERENCES agents(agent_id) ON DELETE CASCADE,
    channel_id TEXT NOT NULL REFERENCES channels(channel_id) ON DELETE CASCADE,
    last_seen_id TEXT REFERENCES messages(message_id) ON DELETE SET NULL,
    PRIMARY KEY (agent_id, channel_id)
);

-- essential indexes
CREATE INDEX idx_messages_channel_created ON messages(channel_id, created_at);
CREATE INDEX idx_messages_agent ON messages(agent_id);

CREATE INDEX idx_tasks_status ON tasks(status);
CREATE INDEX idx_tasks_agent ON tasks(agent_id);
CREATE INDEX idx_tasks_channel ON tasks(channel_id);
CREATE INDEX idx_sessions_agent ON sessions(agent_id);
CREATE INDEX idx_sessions_active ON sessions(ended_at) WHERE ended_at IS NULL;

-- memory
CREATE TABLE memories (
    memory_id TEXT PRIMARY KEY,
    agent_id TEXT NOT NULL REFERENCES agents(agent_id) ON DELETE CASCADE,
    topic TEXT NOT NULL,
    message TEXT NOT NULL,
    timestamp TEXT NOT NULL,
    created_at TEXT NOT NULL,
    archived_at TEXT,
    core INTEGER NOT NULL DEFAULT 0,
    source TEXT NOT NULL DEFAULT 'manual',
    bridge_channel TEXT REFERENCES channels(channel_id) ON DELETE SET NULL,
    code_anchors TEXT,
    synthesis_note TEXT,
    supersedes TEXT REFERENCES memories(memory_id) ON DELETE SET NULL,
    superseded_by TEXT REFERENCES memories(memory_id) ON DELETE SET NULL
);

CREATE TABLE links (
    link_id TEXT PRIMARY KEY,
    memory_id TEXT NOT NULL REFERENCES memories(memory_id) ON DELETE CASCADE,
    parent_id TEXT NOT NULL REFERENCES memories(memory_id) ON DELETE CASCADE,
    kind TEXT NOT NULL,
    created_at TEXT NOT NULL,
    UNIQUE (memory_id, parent_id, kind)
);

CREATE INDEX idx_memories_agent_topic ON memories(agent_id, topic);
CREATE INDEX idx_memories_agent_created ON memories(agent_id, created_at);
CREATE INDEX idx_memories_archived ON memories(archived_at);
CREATE INDEX idx_memories_core ON memories(core);
CREATE INDEX idx_links_memory ON links(memory_id);
CREATE INDEX idx_links_parent ON links(parent_id);

-- knowledge
CREATE TABLE knowledge (
    knowledge_id TEXT PRIMARY KEY,
    domain TEXT NOT NULL,
    agent_id TEXT NOT NULL REFERENCES agents(agent_id) ON DELETE CASCADE,
    content TEXT NOT NULL,
    confidence REAL,
    created_at TEXT NOT NULL DEFAULT (STRFTIME('%Y-%m-%dT%H:%M:%f', 'now')),
    archived_at TEXT
);

CREATE INDEX idx_knowledge_domain ON knowledge(domain);
CREATE INDEX idx_knowledge_agent ON knowledge(agent_id);
CREATE INDEX idx_knowledge_archived ON knowledge(archived_at);
```

All timestamps are stored as ISO-8601 text (`datetime.now().isoformat()` compatible).  
Foreign keys are real and enforced by SQLite; cascading rules match the semantics in business logic.  
Indexes live alongside the table definitions in `001_foundation.sql` and must be kept in sync whenever the schema changes.
</file>

<file path="space/apps/chats/__init__.py">
"""Chats: discover, sync, and manage chat histories from LLM providers."""

from .cli import app

__all__ = ["app"]
</file>

<file path="space/apps/council/__init__.py">
from space.apps.council.cli import app

__all__ = ["app"]
</file>

<file path="space/apps/health/__init__.py">
from .cli import app

__all__ = ["app"]
</file>

<file path="space/apps/health/api.py">
from __future__ import annotations

import logging
import sqlite3

from space.lib import paths, store

logger = logging.getLogger(__name__)

DB_NAME = "space.db"
REGISTRY = "space"
EXPECTED_TABLES = {
    "agents",
    "sessions",
    "tasks",
    "channels",
    "messages",
    "bookmarks",
    "memories",
    "links",
    "knowledge",
}
IGNORED_TABLES = {"sqlite_sequence", "_migrations"}


def _database_exists() -> bool:
    return (paths.space_data() / DB_NAME).exists()


def _check_foreign_keys(conn: sqlite3.Connection) -> list[str]:
    """Run PRAGMA foreign_key_check and format issues."""
    fk_rows = conn.execute("PRAGMA foreign_key_check").fetchall()
    issues = []
    for row in fk_rows:
        table = row["table"]
        rowid = row["rowid"]
        parent = row["parent"]
        issues.append(f"❌ {table} row {rowid} violates FK to {parent}")
    return issues


def check_db() -> tuple[bool, list[str], dict[str, int]]:
    """Validate schema integrity and return (healthy, issues, counts)."""
    issues: list[str] = []
    counts: dict[str, int] = {}

    if not _database_exists():
        issues.append(f"❌ {DB_NAME} missing")
        return False, issues, counts

    try:
        with store.ensure(REGISTRY) as conn:
            actual_tables = {
                row[0] for row in conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
            }
            missing = EXPECTED_TABLES - actual_tables
            extra = actual_tables - EXPECTED_TABLES - IGNORED_TABLES
            if missing:
                issues.append(f"❌ {DB_NAME}: missing tables {sorted(missing)}")
            if extra:
                issues.append(f"⚠️  {DB_NAME}: unexpected tables {sorted(extra)}")

            integrity = conn.execute("PRAGMA integrity_check").fetchone()[0]
            if integrity != "ok":
                issues.append(f"❌ {DB_NAME}: integrity_check={integrity}")

            fk_issues = _check_foreign_keys(conn)
            issues.extend(fk_issues)

            for table in sorted(EXPECTED_TABLES & actual_tables):
                counts[table] = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]

    except sqlite3.Error as exc:
        issues.append(f"❌ {DB_NAME}: {exc}")
        return False, issues, counts

    return not issues, issues, counts


def run_all_checks() -> tuple[list[str], dict[str, dict[str, int]]]:
    """Run health checks for consumers expecting legacy signature."""
    ok, issues, counts = check_db()
    summaries = {DB_NAME: counts} if ok else {}
    return issues, summaries
</file>

<file path="space/apps/stats/api.py">
from __future__ import annotations

import logging

from space.lib import format as fmt
from space.os import bridge, knowledge, memory, spawn

from .models import (
    AgentStats,
    BridgeStats,
    KnowledgeStats,
    LeaderboardEntry,
    MemoryStats,
    SpaceStats,
    SpawnStats,
)

logger = logging.getLogger(__name__)


def _build_leaderboard(
    agent_counts: list[dict], limit: int | None = None
) -> list[LeaderboardEntry]:
    """Build leaderboard from agent_id -> count mapping."""
    names = spawn.api.agent_identities()
    entries = [
        LeaderboardEntry(
            identity=names.get(item["agent_id"], item["agent_id"]), count=item["count"]
        )
        for item in agent_counts
    ]
    return entries[:limit] if limit else entries


def bridge_stats(limit: int = None) -> BridgeStats:
    try:
        stats_data = bridge.api.stats()

        msg_data = stats_data.get("messages", {})
        msg_leaderboard = _build_leaderboard(msg_data.get("by_agent", []), limit=limit)

        channels_data = stats_data.get("channels", {})

        return BridgeStats(
            available=True,
            total=msg_data.get("total", 0),
            active=msg_data.get("active", 0),
            archived=msg_data.get("archived", 0),
            channels=channels_data.get("total", 0),
            active_channels=channels_data.get("active", 0),
            archived_channels=channels_data.get("archived", 0),
            message_leaderboard=msg_leaderboard,
        )
    except Exception as exc:
        logger.error(f"Failed to fetch bridge stats: {exc}")
        return BridgeStats(available=False)


def memory_stats(limit: int = None) -> MemoryStats:
    try:
        stats_data = memory.api.stats()

        leaderboard = _build_leaderboard(stats_data.pop("mem_by_agent", []), limit=limit)

        return MemoryStats(
            available=True,
            leaderboard=leaderboard,
            **stats_data,
        )
    except Exception as exc:
        logger.error(f"Failed to fetch memory stats: {exc}")
        return MemoryStats(available=False)


def knowledge_stats(limit: int = None) -> KnowledgeStats:
    try:
        stats_data = knowledge.api.stats()

        leaderboard = _build_leaderboard(stats_data.pop("know_by_agent", []), limit=limit)

        return KnowledgeStats(
            available=True,
            leaderboard=leaderboard,
            **stats_data,
        )
    except Exception as exc:
        logger.error(f"Failed to fetch knowledge stats: {exc}")
        return KnowledgeStats(available=False)


def agent_stats(limit: int = None, show_all: bool = False) -> list[AgentStats] | None:
    try:
        agent_identities_map = spawn.api.agent_identities()
        archived_set = spawn.api.archived_agents()

        agent_map = {
            agent_id: {
                "identity": identity,
                "msgs": 0,
                "mems": 0,
                "knowledge": 0,
                "events": 0,
                "spawns": 0,
                "last_active": None,
            }
            for agent_id, identity in agent_identities_map.items()
        }

        bridge_data = bridge.api.stats()
        memory_data = memory.api.stats()
        knowledge_data = knowledge.api.stats()

        for item in bridge_data.get("messages", {}).get("by_agent", []):
            agent_id = item["agent_id"]
            if agent_id in agent_map:
                agent_map[agent_id]["msgs"] = item["count"]

        for item in bridge_data.get("events", {}).get("by_agent", []):
            agent_id = item["agent_id"]
            if agent_id in agent_map:
                agent_map[agent_id]["events"] = item.get("events", 0)
                agent_map[agent_id]["spawns"] = item.get("spawns", 0)
                agent_map[agent_id]["last_active"] = item.get("last_active")

        for item in memory_data.get("mem_by_agent", []):
            agent_id = item["agent_id"]
            if agent_id in agent_map:
                agent_map[agent_id]["mems"] = item["count"]

        for item in knowledge_data.get("know_by_agent", []):
            agent_id = item["agent_id"]
            if agent_id in agent_map:
                agent_map[agent_id]["knowledge"] = item["count"]

        result = [
            AgentStats(
                agent_id=agent_id,
                identity=data["identity"],
                events=data["events"],
                spawns=data["spawns"],
                msgs=data["msgs"],
                mems=data["mems"],
                knowledge=data["knowledge"],
                channels=[],
                last_active=data["last_active"],
                last_active_human=fmt.humanize_timestamp(data["last_active"])
                if data["last_active"]
                else None,
            )
            for agent_id, data in agent_map.items()
            if show_all or agent_id not in archived_set
        ]

        result.sort(key=lambda a: a.last_active or "0", reverse=True)
        return result[:limit] if limit else result
    except Exception as exc:
        logger.error(f"Failed to fetch agent stats: {exc}")
        return None


def spawn_stats() -> SpawnStats:
    try:
        stats_data = spawn.api.stats()
        return SpawnStats(available=True, **stats_data)
    except Exception as exc:
        logger.error(f"Failed to fetch spawn stats: {exc}")
        return SpawnStats(available=False)


def collect(limit: int = None, agent_limit: int = None) -> SpaceStats:
    return SpaceStats(
        bridge=bridge_stats(limit=limit),
        memory=memory_stats(limit=limit),
        knowledge=knowledge_stats(limit=limit),
        spawn=spawn_stats(),
        agents=agent_stats(limit=agent_limit),
    )
</file>

<file path="space/apps/backup.py">
import json
import logging
import os
import shutil
import sqlite3
from datetime import datetime
from pathlib import Path

import typer

from space.lib import paths, store

logger = logging.getLogger(__name__)

app = typer.Typer()


@app.callback(invoke_without_command=True)
def callback(ctx: typer.Context):
    if ctx.invoked_subcommand is None:
        ctx.invoke(backup)


def _copytree_exclude_chats(src: Path, dst: Path) -> None:
    """Copy tree excluding chats.db + WAL artifacts (queryable from provider dirs, not backed up)."""

    def ignore(directory: str, contents: list) -> set:
        excluded = set()
        if "chats.db" in contents:
            excluded.add("chats.db")
            excluded.add("chats.db-shm")
            excluded.add("chats.db-wal")
        return excluded

    shutil.copytree(src, dst, ignore=ignore, dirs_exist_ok=False)


def _backup_data_snapshot(timestamp: str, quiet_output: bool) -> dict:
    """Backup ~/.space/data to timestamped snapshot."""
    src = paths.space_data()
    if not src.exists():
        if not quiet_output:
            typer.echo("No data directory found")
        return {}

    backup_path = paths.backup_snapshot(timestamp)
    backup_path.parent.mkdir(parents=True, exist_ok=True)

    store.close_all()
    store.resolve(src)
    shutil.copytree(src, backup_path, dirs_exist_ok=False)
    os.chmod(backup_path, 0o555)

    return _get_backup_stats(backup_path)


def _backup_chats_latest(quiet_output: bool) -> None:
    """Backup ~/.space/chats to ~/.space_backups/chats/latest (append-only, never deletes)."""
    src = paths.chats_dir()
    if not src.exists():
        return

    backup_path = paths.backup_chats_latest()
    backup_path.parent.mkdir(parents=True, exist_ok=True)

    backup_path.mkdir(parents=True, exist_ok=True)

    for provider_dir in src.iterdir():
        if not provider_dir.is_dir():
            continue

        backup_provider_dir = backup_path / provider_dir.name
        backup_provider_dir.mkdir(exist_ok=True)

        for chat_file in provider_dir.rglob("*"):
            if not chat_file.is_file():
                continue

            rel_path = chat_file.relative_to(provider_dir)
            backup_file = backup_provider_dir / rel_path
            backup_file.parent.mkdir(parents=True, exist_ok=True)

            if not backup_file.exists() or chat_file.stat().st_mtime > backup_file.stat().st_mtime:
                shutil.copy2(chat_file, backup_file)

    os.chmod(backup_path, 0o555)


def _get_backup_stats(backup_path: Path) -> dict:
    """Get row counts for all databases in backup."""
    stats = {}
    for db_file in backup_path.glob("*.db"):
        if db_file.name == "cogency.db":
            continue
        try:
            conn = sqlite3.connect(db_file)
            cursor = conn.execute(
                "SELECT name FROM sqlite_master WHERE type='table' AND name != '_migrations'"
            )
            tables = [row[0] for row in cursor.fetchall()]

            total = (
                sum(conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0] for table in tables)
                if tables
                else 0
            )

            stats[db_file.name] = {
                "tables": len(tables),
                "rows": total,
            }
            conn.close()
        except sqlite3.DatabaseError:
            stats[db_file.name] = {"error": "corrupted"}

    return stats


def _show_backup_stats(backup_stats: dict) -> None:
    """Display backup statistics."""
    typer.echo("\nBackup stats:")
    typer.echo("  Database               Tables  Rows")
    typer.echo("  " + "─" * 40)

    for db_name in sorted(backup_stats.keys()):
        stats = backup_stats[db_name]
        if "error" in stats:
            typer.echo(f"  {db_name:22} error")
        else:
            tables = stats.get("tables", "?")
            rows = stats.get("rows", "?")
            typer.echo(f"  {db_name:22} {tables:6}  {rows}")


@app.command()
def backup(
    json_output: bool = typer.Option(False, "--json", "-j", help="Output in JSON format."),
    quiet_output: bool = typer.Option(
        False, "--quiet", "-q", help="Suppress non-essential output."
    ),
):
    """Backup ~/.space/data and ~/.space/chats to ~/.space_backups/.

    Data backups are immutable and timestamped: ~/.space_backups/data/{timestamp}/
    Chat backups are latest-only: ~/.space_backups/chats/latest/ (overwrites)
    """

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_stats = _backup_data_snapshot(timestamp, quiet_output)
    _backup_chats_latest(quiet_output)

    if json_output:
        typer.echo(
            json.dumps(
                {
                    "data_backup": str(paths.backup_snapshot(timestamp)),
                    "chats_backup": str(paths.backup_chats_latest()),
                    "stats": backup_stats,
                }
            )
        )
    elif not quiet_output:
        typer.echo(f"✓ Backed up data to {paths.backup_snapshot(timestamp)}")
        typer.echo(f"✓ Backed up chats to {paths.backup_chats_latest()}")
        if backup_stats:
            _show_backup_stats(backup_stats)


def main() -> None:
    """Entry point for poetry script."""
    app()
</file>

<file path="space/core/protocols.py">
"""Protocols defining interfaces for pluggable implementations."""

from collections.abc import Callable
from pathlib import Path
from typing import Any, Protocol, runtime_checkable


@runtime_checkable
class Storage(Protocol):
    """Storage backend protocol for database operations.

    Enables pluggable implementations (SQLite now, cloud in future).
    Backend-agnostic: uses generic Connection and Row types.
    """

    def connect(self, db_path: str) -> Any:
        """Open connection to database."""
        ...

    def ensure_schema(
        self,
        db_path: str,
        schema: str,
        migrations: list[tuple[str, str | Callable]] | None = None,
    ) -> None:
        """Ensure schema exists and apply migrations."""
        ...

    def register(self, name: str, db_file: str, schema: str) -> None:
        """Register database in registry."""
        ...

    def migrations(self, name: str, migs: list[tuple[str, str | Callable]]) -> None:
        """Register migrations for database."""
        ...

    def ensure(self, name: str) -> Any:
        """Ensure registered database exists and return connection."""
        ...

    def migrate(self, conn: Any, migrations: list[tuple[str, str | Callable]]) -> None:
        """Apply migrations to connection."""
        ...


@runtime_checkable
class Provider(Protocol):
    """External LLM provider (Claude, Codex, Gemini).

    Unified interface for both chat discovery and agent spawning.
    """

    def discover_sessions(self) -> list[dict]:
        """Discover chat sessions.

        Returns:
            List of {cli, session_id, file_path, created_at}
        """
        ...

    def parse_messages(self, file_path: Path, from_offset: int = 0) -> list[dict]:
        """Parse messages from chat session file.

        Args:
            file_path: Path to chat file (JSONL or JSON)
            from_offset: Byte offset or message index to start from

        Returns:
            List of {role, content, timestamp, byte_offset/message_index}
        """
        ...

    def spawn(self, identity: str, task: str | None = None) -> str:
        """Spawn agent instance with role.

        Args:
            role: Identity/role to spawn
            task: Optional task to execute

        Returns:
            Command output
        """
        ...

    def ping(self, identity: str) -> bool:
        """Check if agent is alive.

        Args:
            identity: Agent identity

        Returns:
            True if agent is responsive
        """
        ...

    def list_agents(self) -> list[str]:
        """List all active agents.

        Returns:
            List of agent identities
        """
        ...
</file>

<file path="space/lib/sync.py">
"""Chat sync: discover and copy/convert provider chats to ~/.space/chats/{provider}/"""

import json
import shutil
from pathlib import Path

from space.lib import paths, providers


def _load_sync_state() -> dict:
    """Load sync state tracking {provider}_{session_id}: mtime."""
    state_file = paths.space_data() / "sync_state.json"
    if not state_file.exists():
        return {}
    try:
        return json.loads(state_file.read_text())
    except (OSError, json.JSONDecodeError):
        return {}


def _save_sync_state(state: dict) -> None:
    """Save sync state tracking."""
    state_file = paths.space_data() / "sync_state.json"
    paths.space_data().mkdir(parents=True, exist_ok=True)
    state_file.write_text(json.dumps(state, indent=2))


def _gemini_json_to_jsonl(json_file: Path) -> str:
    """Convert Gemini JSON chat to JSONL format.

    Args:
        json_file: Path to Gemini JSON chat file

    Returns:
        JSONL string (one JSON object per line)
    """
    try:
        with open(json_file) as f:
            data = json.load(f)

        lines = []
        if isinstance(data, dict) and "messages" in data:
            for msg in data.get("messages", []):
                role = msg.get("role")
                if role not in ("user", "model"):
                    continue
                lines.append(
                    json.dumps(
                        {
                            "role": "assistant" if role == "model" else "user",
                            "content": msg.get("content", ""),
                            "timestamp": msg.get("timestamp"),
                        }
                    )
                )
        return "\n".join(lines) + "\n" if lines else ""
    except (OSError, json.JSONDecodeError):
        return ""


def sync_provider_chats(verbose: bool = False) -> dict[str, tuple[int, int]]:
    """Sync chats from all providers (~/.claude, ~/.codex, ~/.gemini) to ~/.space/chats/.

    Converts Gemini JSON to JSONL. Only syncs if source is newer than tracked state.
    Tracks by session_id + mtime to survive format changes.

    Args:
        verbose: If True, yield progress messages (not implemented here, use return value)

    Returns:
        {provider_name: (sessions_discovered, files_synced)} for each provider
    """
    results = {}
    chats_dir = paths.chats_dir()
    chats_dir.mkdir(parents=True, exist_ok=True)

    sync_state = _load_sync_state()
    provider_map = {"claude": "Claude", "codex": "Codex", "gemini": "Gemini"}

    for cli_name, class_name in provider_map.items():
        try:
            provider_class = getattr(providers, class_name)
            provider = provider_class()

            sessions = provider.discover_sessions()
            if not sessions:
                results[cli_name] = (0, 0)
                continue

            dest_dir = chats_dir / cli_name
            dest_dir.mkdir(parents=True, exist_ok=True)

            synced_count = 0
            for session in sessions:
                src_file = Path(session["file_path"])
                if not src_file.exists():
                    continue

                session_id = session.get("session_id")
                state_key = f"{cli_name}_{session_id}"
                src_mtime = src_file.stat().st_mtime
                tracked_mtime = sync_state.get(state_key, 0)

                should_sync = src_mtime > tracked_mtime

                if not should_sync:
                    continue

                try:
                    if cli_name == "gemini":
                        dest_file = dest_dir / f"{session_id}.jsonl"
                        jsonl_content = _gemini_json_to_jsonl(src_file)
                        dest_file.parent.mkdir(parents=True, exist_ok=True)
                        dest_file.write_text(jsonl_content)
                    else:
                        dest_file = dest_dir / src_file.name
                        dest_file.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(src_file, dest_file)

                    sync_state[state_key] = src_mtime
                    synced_count += 1
                except (OSError, Exception):
                    pass

            results[cli_name] = (len(sessions), synced_count)
        except (AttributeError, Exception):
            results[cli_name] = (0, 0)

    _save_sync_state(sync_state)
    return results
</file>

<file path="space/os/bridge/cli/channels.py">
"""Show active channels."""

from __future__ import annotations

import typer

from space.os.bridge import ops

from .format import echo_if_output, format_channel_row, output_json, should_output


def register(app: typer.Typer) -> None:
    @app.command()
    def channels(
        ctx: typer.Context,
        all: bool = typer.Option(False, "--all", help="Include archived channels"),
    ):
        """Show active channels."""
        try:
            chans = ops.list_channels(all=all)

            if not chans:
                output_json([], ctx) or echo_if_output("No channels found", ctx)
                return

            if output_json(
                [
                    {
                        "name": c.name,
                        "topic": c.topic,
                        "message_count": c.message_count,
                        "last_activity": c.last_activity,
                        "unread_count": c.unread_count,
                        "archived_at": c.archived_at,
                    }
                    for c in chans
                ],
                ctx,
            ):
                return

            active = [c for c in chans if not c.archived_at]
            archived = [c for c in chans if c.archived_at]
            active.sort(key=lambda t: t.name)
            archived.sort(key=lambda t: t.name)

            if not should_output(ctx):
                return

            if active:
                echo_if_output(f"ACTIVE CHANNELS ({len(active)}):", ctx)
                for channel in active:
                    last_activity, description = format_channel_row(channel)
                    echo_if_output(f"  {last_activity}: {description}", ctx)

            if all and archived:
                echo_if_output(f"\nARCHIVED ({len(archived)}):", ctx)
                for channel in archived:
                    last_activity, description = format_channel_row(channel)
                    echo_if_output(f"  {last_activity}: {description}", ctx)
        except Exception as e:
            output_json({"status": "error", "message": str(e)}, ctx) or echo_if_output(
                f"❌ {e}", ctx
            )
            raise typer.Exit(code=1) from e
</file>

<file path="space/os/db/__init__.py">
"""Unified space.os database - single SQLite file with shared schema."""

from __future__ import annotations

from space.lib import migrations as migration_loader
from space.lib import paths, store

ALIASES = ("bridge", "memory", "knowledge", "spawn")
_initialized = False


def register() -> None:
    """Register unified database and configure aliases."""
    global _initialized
    if _initialized:
        return
    _initialized = True

    store.register("space", "space.db")
    store.add_migrations("space", migration_loader.load_migrations("space.os.db"))
    for name in ALIASES:
        store.alias(name, "space")


def connect():
    """Ensure schema and return connection to unified database."""
    register()
    return store.ensure("space")


def path():
    """Return filesystem path to unified database."""
    register()
    return paths.space_data() / "space.db"
</file>

<file path="space/os/knowledge/__init__.py">
from . import api, db
from .api import (
    add_entry,
    archive_entry,
    find_related,
    get_by_id,
    list_entries,
    query_by_agent,
    query_by_domain,
    restore_entry,
    search,
)
from .cli import app

db.register()

__all__ = [
    "api",
    "db",
    "app",
    "add_entry",
    "archive_entry",
    "find_related",
    "get_by_id",
    "list_entries",
    "query_by_agent",
    "query_by_domain",
    "restore_entry",
    "search",
]
</file>

<file path="space/os/memory/__init__.py">
from . import api, db
from .api import (
    add_entry,
    add_link,
    archive_entry,
    delete_entry,
    edit_entry,
    find_related,
    get_by_id,
    get_chain,
    list_entries,
    mark_core,
    replace_entry,
    restore_entry,
    search,
)
from .cli import app

db.register()

__all__ = [
    "api",
    "db",
    "app",
    "add_entry",
    "add_link",
    "archive_entry",
    "delete_entry",
    "edit_entry",
    "find_related",
    "get_by_id",
    "get_chain",
    "list_entries",
    "mark_core",
    "replace_entry",
    "restore_entry",
    "search",
]
</file>

<file path="space/os/spawn/api/sessions.py">
"""Session tracking: spawn lifecycle management."""

from datetime import datetime

from space.lib.uuid7 import uuid7
from space.os.spawn import db


def create_session(agent_id: str) -> str:
    """Create a new session for agent spawn.

    Returns session_id.
    """
    with db.connect() as conn:
        cursor = conn.cursor()

        session_id = uuid7()

        cursor.execute("SELECT spawn_count FROM agents WHERE agent_id = ?", (agent_id,))
        result = cursor.fetchone()
        spawn_count = (result[0] if result else 0) + 1

        cursor.execute(
            """
            INSERT INTO sessions (session_id, agent_id, spawn_number, wakes)
            VALUES (?, ?, ?, 0)
            """,
            (session_id, agent_id, spawn_count),
        )

        cursor.execute(
            "UPDATE agents SET spawn_count = ?, last_active_at = ? WHERE agent_id = ?",
            (spawn_count, datetime.now().isoformat(), agent_id),
        )

        return session_id


def end_session(session_id: str) -> None:
    """End a session."""
    with db.connect() as conn:
        cursor = conn.cursor()
        cursor.execute(
            "UPDATE sessions SET ended_at = ? WHERE session_id = ?",
            (datetime.now().isoformat(), session_id),
        )


def get_spawn_count(agent_id: str) -> int:
    """Get total spawn count for agent."""
    with db.connect() as conn:
        cursor = conn.cursor()
        cursor.execute("SELECT spawn_count FROM agents WHERE agent_id = ?", (agent_id,))
        result = cursor.fetchone()
        return result[0] if result else 0
</file>

<file path="tests/integration/test_agent_coordination.py">
from space.os import bridge, spawn


def test_agent_posts_mid_execution(test_space, default_agents):
    """Agents post autonomously mid-execution and wait for responses."""
    dev_channel_id = bridge.create_channel("space-dev", "Development coordination")
    zealot_1_id = default_agents["zealot"]
    zealot_2_id = default_agents["sentinel"]

    agent_z1_uuid = bridge.send_message(
        dev_channel_id, zealot_1_id, "Starting analysis of spawn.py:42"
    )
    messages = bridge.get_messages(dev_channel_id)
    assert len(messages) == 1
    assert messages[0].agent_id == agent_z1_uuid

    bridge.send_message(
        dev_channel_id,
        zealot_1_id,
        "Found potential bug at line 42. @sentinel please review before merge",
    )
    messages = bridge.get_messages(dev_channel_id)
    assert len(messages) >= 2
    assert "@sentinel" in messages[1].content

    new_for_zealot2, count, _, participants = bridge.recv_messages(dev_channel_id, zealot_2_id)
    assert count >= 2
    assert any("bug" in msg.content for msg in new_for_zealot2)

    bridge.send_message(
        dev_channel_id,
        zealot_2_id,
        "Reviewed. Bug confirmed. Fix: change line 42 to `if x is None:`",
    )
    messages = bridge.get_messages(dev_channel_id)
    assert len(messages) >= 3
    assert any("Fix:" in msg.content for msg in messages)

    new_for_zealot1, count, _, _ = bridge.recv_messages(dev_channel_id, zealot_1_id)
    assert count >= 1
    assert any("Fix:" in msg.content for msg in new_for_zealot1)

    bridge.send_message(dev_channel_id, zealot_1_id, "Implemented fix. Tests passing.")
    messages = bridge.get_messages(dev_channel_id)
    assert len(messages) >= 4


def test_agent_bookmark_isolation(test_space, default_agents):
    """Each agent maintains independent bookmark; doesn't see other reads."""
    channel_id = bridge.create_channel("shared-channel")
    zealot_1_id = default_agents["zealot"]
    zealot_2_id = default_agents["sentinel"]

    bridge.send_message(channel_id, "zealot", "message 1")

    new_z1, count_z1, _, _ = bridge.recv_messages(channel_id, zealot_1_id)
    assert count_z1 == 1

    bridge.send_message(channel_id, "zealot", "message 2")

    new_z2, count_z2, _, _ = bridge.recv_messages(channel_id, zealot_2_id)
    assert count_z2 == 2

    new_z1_again, count_z1_again, _, _ = bridge.recv_messages(channel_id, zealot_1_id)
    assert count_z1_again == 1

    bridge.send_message(channel_id, "zealot", "message 3")

    new_z1_final, count_z1_final, _, _ = bridge.recv_messages(channel_id, zealot_1_id)
    assert count_z1_final == 1
    assert "message 3" in new_z1_final[0].content

    new_z2_final, count_z2_final, _, _ = bridge.recv_messages(channel_id, zealot_2_id)
    assert count_z2_final == 1
    assert "message 3" in new_z2_final[0].content


def test_full_spawn_task_events_flow(test_space, default_agents):
    """Agent creation → task creation → events emission data flow."""
    agent_identity = default_agents["zealot"]
    assert agent_identity is not None
    assert isinstance(agent_identity, str)
    assert len(agent_identity) > 0

    # Get the actual agent object to get the real agent_id (UUID)
    agent = spawn.get_agent(agent_identity)
    assert agent is not None
    agent_id = agent.agent_id

    task_id = spawn.create_task(agent_identity, "test input")
    assert task_id is not None

    task = spawn.get_task(task_id)
    assert task is not None
    assert task.agent_id == agent_id
    assert isinstance(task.agent_id, str)

    spawn.complete_task(task_id, output="test output")
    updated_task = spawn.get_task(task_id)
    assert updated_task.status == "completed"
    assert updated_task.agent_id == agent_id
</file>

<file path="tests/integration/test_spawn_commands.py">
import time

from space.os import spawn


def test_wait_blocks_until_completion(test_space, default_agents):
    import threading

    from space.os.spawn.cli.tasks import wait

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="test")

    spawn.start_task(task_id)

    def complete_task_in_thread():
        time.sleep(0.1)
        spawn.complete_task(task_id, output="done")

    thread = threading.Thread(target=complete_task_in_thread)
    thread.start()

    exit_code = wait(task_id)
    assert exit_code == 0
    thread.join()


def test_wait_exit_code(test_space, default_agents):
    from space.os.spawn.cli.tasks import wait

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="test")
    spawn.fail_task(task_id)

    exit_code = wait(task_id)
    assert exit_code != 0


def test_wait_timeout(test_space, default_agents):
    import typer

    from space.os.spawn.cli.tasks import wait

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="test")
    spawn.start_task(task_id)

    try:
        wait(task_id, timeout=0.01)
        raise AssertionError("Should timeout")
    except typer.Exit as e:
        assert e.exit_code == 124


def test_wait_pending(test_space, default_agents):
    from space.os.spawn.cli.tasks import wait

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="test")

    spawn.start_task(task_id)
    spawn.complete_task(task_id, output="result")

    exit_code = wait(task_id)
    assert exit_code == 0

    task = spawn.get_task(task_id)
    assert task.output == "result"


def test_kill_running_task(test_space, default_agents):
    from space.os.spawn.cli.tasks import kill

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="long task")
    spawn.start_task(task_id, pid=12345)

    kill(task_id)

    task = spawn.get_task(task_id)
    assert task.status == "failed"
    assert "killed" in task.stderr.lower()


def test_kill_nonexistent_task(test_space, capsys):
    import typer

    from space.os.spawn.cli.tasks import kill

    try:
        kill("nonexistent-id")
        raise AssertionError("Should raise Exit")
    except typer.Exit as e:
        assert e.exit_code == 1
        captured = capsys.readouterr()
        assert "not found" in captured.err.lower()


def test_kill_completed_task_no_op(test_space, default_agents):
    from space.os.spawn.cli.tasks import kill

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="test")
    spawn.complete_task(task_id, output="done")

    kill(task_id)

    task = spawn.get_task(task_id)
    assert task.status == "completed"


def test_tasks_list_empty(test_space, capsys):
    from space.os.spawn.cli.tasks import list as list_tasks

    list_tasks(None, None)
    captured = capsys.readouterr()
    assert "No tasks" in captured.out or len(captured.out) == 0


def test_tasks_list_shows_running(test_space, capsys, default_agents):
    from space.os.bridge.api import channels as bridge_api
    from space.os.spawn.cli.tasks import list as list_tasks

    channel = bridge_api.create_channel(name="test-ch-running")
    zealot_id = default_agents["zealot"]
    t1 = spawn.create_task(role=zealot_id, input="task 1", channel_id=channel.channel_id)
    spawn.start_task(t1)

    list_tasks(None, None)
    captured = capsys.readouterr()
    assert t1[:8] in captured.out
    assert "running" in captured.out


def test_tasks_list_filter_by_status(test_space, capsys, default_agents):
    from space.os.spawn.cli.tasks import list as list_tasks

    zealot_id = default_agents["zealot"]
    t1 = spawn.create_task(role=zealot_id, input="task 1")
    t2 = spawn.create_task(role=zealot_id, input="task 2")
    spawn.complete_task(t1)

    list_tasks(status="pending", role=None)
    captured = capsys.readouterr()
    assert t2[:8] in captured.out or "task 2" in captured.out
    assert "task 1" not in captured.out or t1[:8] not in captured.out


def test_tasks_list_filter_by_identity(test_space, capsys, default_agents):
    from space.os.spawn.cli.tasks import list as list_tasks

    sentinel_id = default_agents["sentinel"]
    zealot_id = default_agents["zealot"]
    spawn.create_task(role=zealot_id, input="zealot task")
    spawn.create_task(role=sentinel_id, input="sentinel task")

    list_tasks(status=None, role=zealot_id)
    captured = capsys.readouterr()
    lines = [
        line
        for line in captured.out.split("\n")
        if line.strip() and not line.startswith("ID") and line != "-" * 70
    ]
    assert len(lines) == 1


def test_logs_shows_full_task_detail(test_space, capsys, default_agents):
    from space.os.spawn.cli.tasks import logs

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="list repos")
    spawn.start_task(task_id)
    time.sleep(0.01)
    spawn.complete_task(task_id, output="repo1\nrepo2")

    logs(task_id)
    captured = capsys.readouterr()
    assert "list repos" in captured.out
    assert "repo1" in captured.out
    assert "completed" in captured.out


def test_logs_shows_failed_task_stderr(test_space, capsys, default_agents):
    from space.os.spawn.cli.tasks import logs

    zealot_id = default_agents["zealot"]
    task_id = spawn.create_task(role=zealot_id, input="bad command")
    spawn.fail_task(task_id, stderr="error: not found")

    logs(task_id)
    captured = capsys.readouterr()
    assert "bad command" in captured.out
    assert "error: not found" in captured.out


def test_logs_task_not_found(test_space, capsys):
    import typer

    from space.os.spawn.cli.tasks import logs

    try:
        logs("nonexistent-id-123")
        raise AssertionError("Should have raised Exit")
    except typer.Exit as e:
        assert e.exit_code == 1
        captured = capsys.readouterr()
        assert "not found" in captured.err.lower()
</file>

<file path="tests/unit/lib/test_paths.py">
from pathlib import Path

from space.lib import paths


def test_space_root(monkeypatch):
    monkeypatch.delenv("SPACE_ROOT", raising=False)
    assert paths.space_root() == Path.home() / "space"


def test_space_data(monkeypatch):
    assert paths.space_data() == Path.home() / ".space" / "data"


def test_canon_path_default(mocker):
    mock_space_root = Path("/tmp/test_space_root")
    mocker.patch("space.lib.paths.space_root", return_value=mock_space_root)
    expected_path = mock_space_root / "canon"
    assert paths.canon_path() == expected_path
</file>

<file path="docs/events.md">
# Events: Provenance & Audit Trail

System-wide append-only event log for tracking agent actions, coordination, and system state changes.

## Overview

Events enable:
- **Provenance tracking** — who did what and when
- **Audit trail** — immutable record for debugging
- **Identity verification** — link invocations to agents via spawn registry
- **Analytics** — wake/sleep cycles, session counts, etc.

All events are **immutable** (append-only, never updated/deleted).

## Event Schema

```sql
events (
  event_id TEXT PRIMARY KEY,       -- UUID7 (sortable)
  source TEXT NOT NULL,            -- subsystem origin: spawn | bridge | memory | knowledge | identity
  agent_id TEXT,                   -- FK to spawn.agents.agent_id (nullable for system events)
  event_type TEXT NOT NULL,        -- semantic type: agent.create | message_sent | session_start | ...
  data TEXT,                       -- free-form payload (event-specific)
  timestamp INTEGER NOT NULL,      -- unix epoch seconds (for sorting)
  chat_id TEXT                     -- optional context: bridge channel_id
)
```

## Event Types by Source

### spawn — Identity registry events

| Event Type | Data | Agent | Notes |
|-----------|------|-------|-------|
| `agent.create` | agent name | agent_id | New agent registered |
| `agent.archive` | agent name | agent_id | Agent archived |
| `agent.restore` | agent name | agent_id | Agent restored from archive |
| `task.create` | task input | agent_id | Task spawned |
| `task.complete` | task output | agent_id | Task finished |
| `task.fail` | error message | agent_id | Task failed |

### bridge — Coordination events

| Event Type | Data | Agent | Notes |
|-----------|------|-------|-------|
| `message_sent` | channel:message_id | agent_id | Message posted to channel |
| `message_read` | channel:last_seen_id | agent_id | Bookmark updated (message read) |
| `note_created` | channel:note_id | agent_id | Reflection/annotation added |
| `channel_created` | channel name | null | New channel created (system) |
| `channel_archived` | channel name | null | Channel archived (system) |

### memory — Personal context events

| Event Type | Data | Agent | Notes |
|-----------|------|-------|-------|
| `note_add` | topic:content_preview | agent_id | Memory entry created |
| `note_edit` | memory_id | agent_id | Memory entry updated |
| `note_delete` | memory_id | agent_id | Memory entry deleted |
| `note_archive` | memory_id | agent_id | Memory archived |
| `note_restore` | memory_id | agent_id | Memory restored |
| `note_core` | memory_id→true/false | agent_id | Core flag toggled |
| `note_replace` | old_ids→new_id | agent_id | Entry superseded |

### knowledge — Shared knowledge events

| Event Type | Data | Agent | Notes |
|-----------|------|-------|-------|
| `entry.write` | domain:content_preview | agent_id | Knowledge artifact added |
| `entry.archive` | knowledge_id | agent_id | Knowledge archived |
| `entry.restore` | knowledge_id | agent_id | Knowledge restored |

### identity — Provenance tracking

| Event Type | Data | Agent | Notes |
|-----------|------|-------|-------|
| any command | command name | agent_id | Track identity invocation |

## Usage Patterns

### Query events by agent

```python
from space import events

# Get all events for an agent
agent_events = events.query(agent_id="zealot-1-uuid")

# Get events from a subsystem
bridge_events = events.query(source="bridge")

# Get last 20 events
recent = events.query(limit=20)
```

### Emit an event

```python
from space import events
from space.os import spawn

agent = spawn.get_agent("zealot-1")
events.emit(
    source="bridge",
    event_type="message_sent",
    agent_id=agent.agent_id,
    data="research:msg-abc123"
)
```

### Session tracking

```python
# Count sessions (wake cycles) for an agent
session_count = events.get_session_count(agent_id)

# Get last sleep time
last_sleep = events.get_last_sleep_time(agent_id)

# Count wakes since last sleep
wakes_in_session = events.get_wakes_since_last_sleep(agent_id)
```

## Provenance Model

**Problem:** How do we know who did what?

**Solution:** Events link agents (via spawn registry) to actions.

```
identity invocation (CLI)
    ↓
events.identify(identity, command)
    ↓
spawn.get_agent(identity)  ← resolves name → agent_id
    ↓
events.emit(source, event_type, agent_id, data)
    ↓
events.db record (immutable)
```

### Example: Message send with provenance

```bash
bridge send research "proposal" --as zealot-1
```

1. CLI resolves `zealot-1` to agent_id via `spawn.get_agent()`
2. Bridge inserts message with `agent_id`
3. Event emitted: `events.emit("bridge", "message_sent", agent_id, "research:msg-id")`
4. Event stored immutably in events.db
5. Audit trail: `events.query(agent_id=zealot_id)` shows all actions by zealot-1

### Sender recovery

No explicit sender column in bridge.messages — resolve via:

```python
from space.os import spawn

message = bridge.get_messages(channel_id)[0]
sender_name = spawn.get_agent(message.agent_id).identity  # lookup from registry
```

## Analytics

### Wake/sleep cycles

```python
from space import events

wakes = events.get_wake_count(agent_id)
sleeps = events.get_sleep_count(agent_id)
sessions = events.get_session_count(agent_id)
```

### Message activity

```python
from space import events

# Get all message_sent events for an agent
sent_events = events.query(agent_id=agent_id, source="bridge")
message_count = len(sent_events)
last_active = sent_events[0].timestamp if sent_events else None
```

## Design Principles

**Immutable.** Events are never updated or deleted. If an event is wrong, emit a correction event.

**Append-only.** All events are additions to the log. Order is preserved by UUID7 (sortable by timestamp).

**Decoupled.** Events don't require the emitter to succeed. If `events.emit()` fails, the main operation succeeded but audit trail is incomplete (should log separately).

**Optional context.** Fields like `agent_id` and `chat_id` are nullable — system events may not have an agent.

**Free-form data.** The `data` field is event-specific. No rigid schema. Document your event_type → data mapping.

## Migration from session_id

Prior to schema alignment:
- Column was named `session_id` (semantic mismatch)
- Migration added column dynamically
- Renamed to `chat_id` (semantically correct: bridge channel context)

Existing databases:
- Migration `_migrate_add_chat_id` automatically renames column
- No manual intervention needed

---

See `space/os/events.py` for implementation and query API.
</file>

<file path="docs/operations.md">
# Operations Guide

Complete reference for running space-os: agent lifecycle, command usage, coordination patterns, and operational best practices.

## Quick Start

**Agent lifecycle:**
```bash
spawn register zealot zealot-1 research --model claude-sonnet-4
wake --as zealot-1              # load context, resume work
sleep --as zealot-1             # persist state before death
```

**Coordination:**
```bash
bridge send research "proposal: stateless context assembly" --as zealot-1
bridge recv research --as harbinger-1
```

**Context management:**
```bash
memory add --as zealot-1 --topic arch "executor yields events, processor iterates"
knowledge add --domain architecture --contributor zealot-1 "Delimiter protocol eliminates guessing"
context "stateless"             # search memory + knowledge + bridge + events
```

## Primitives

For detailed information on each primitive, refer to their dedicated documentation:

-   [Spawn](spawn.md)
-   [Bridge](bridge.md)
-   [Memory](memory.md)
-   [Knowledge](knowledge.md)

### context
Unified search over memory, knowledge, bridge, events. Timeline + current state + lattice docs.

```bash
context "query"                             # all agents, all subsystems
context --as <identity> "query"             # scoped to your data
context --json "query"                      # machine-readable
```

No dedicated storage. Queries existing subsystem DBs.

### wake / sleep
Context persistence across agent deaths.

```bash
wake --as <identity>                        # load context, resume work
wake --as <identity> --check                # preview without spawning
sleep --as <identity>                       # persist state
```

## Agent Lifecycle

### Full Cycle
1. `space wake --as <identity>` — Load context and resume work
   - Resolve identity → agent_id via spawn registry
   - Load core memories (identity-defining entries)
   - Load recent memories (7-day window)
   - Fetch unread channel counts (via bookmarks)
   - Query relevant knowledge domains
   - Display context summary
2. **Work** — Agent processes, reads bridge, writes memory/knowledge
3. `space sleep --as <identity>` — Persist state before shutdown
   - Prompt for session summary
   - Write summary to memory (marked as core)
   - Update spawn registry with session completion
   - Emit sleep event for analytics
4. **Agent dies** — Process terminates
5. **Next wake** — Resumes from step 1 (full context available)

### Important: No hand-off mechanism
Each agent is stateless. All context is immutable in storage. Next agent sees exactly what previous agent left behind.

### Context Persistence Strategy
- **Bridge** — Ephemeral coordination (conversations, proposals)
- **Memory** — Working state (what you're doing, blockers, plans)
- **Knowledge** — Permanent discoveries (shared truths)

Pattern: Bridge → Memory → Knowledge (information flows "down" as consensus solidifies)

## Coordination Patterns

### Pattern 1: Propose-Discuss-Decide
```
Agent A:
  bridge send research "Proposal: X approach"
  sleep

Agent B (later):
  wake
  bridge recv research  # see proposal
  bridge send research "Alternative: Y approach"
  sleep

Agent A (next wake):
  bridge recv research  # see discussion
  memory add --topic decision "Chose Y because..."
  knowledge add --domain architecture "Y approach best for..."
```

### Pattern 2: Context Injection
```
Agent A discovers critical info:
  knowledge add --domain safety "Found vulnerability in X"

Agent B (any time):
  knowledge about safety  # queries before critical decision
  Avoids repeating A's research
```

### Pattern 3: Memory Consolidation
```
After intense debugging session:
  memory add --topic debugging "Spent 2h on X, found Y, next steps: Z" --core
  memory archive <old-entry-id>  # prune working notes
  Cleans memory for next wake
```

## Storage & Backup

**Atomic backups:**
```bash
space backup  # copies entire .space/ to ~/.space_backups/YYYYMMDD_HHMMSS/
```

**Database location:**
- `.space/space.db` — unified schema (agents, channels, messages, bookmarks, memories, links, knowledge, tasks, sessions)

**No migrations needed** — All schema changes via automatic registry-based migrations on first connection.

## Debugging & Introspection

**View event timeline:**
```bash
space events --agent <identity>
space events --source bridge
```

**Analyze agent activity:**
```bash
space stats --agent <identity>  # session count, message rate, memory size
```

**Search across all subsystems:**
```bash
space context "query"  # memory + knowledge + bridge + events
```

**List agents:**
```bash
spawn list  # all active agents
spawn list --archived  # including archived
```

**Health check:**
```bash
space health  # validate DB schemas, report mismatches
```

## Common Issues

### Agent not found
- Verify identity: `spawn list`
- If archived: `spawn restore <identity>`

### Unread messages not clearing
- Bookmark may be stale: `bridge recv <channel> --as <identity>` updates it
- Or manually: inspect `space.db` bookmarks table

### Memory bloat
- Archive old entries: `memory archive <id>`
- Or consolidate: `memory replace <old-ids> "synthesis"`

### Lost coordination (bridge messages missing)
- Bridge is append-only, never loses data
- Check if channel was archived: `bridge` shows active channels
- Use `bridge export <channel>` for full transcript

## Performance Notes

**Database footprint:**
- space.db: ~1MB per 10k bridge messages + 5k memory entries; under 5MB for typical workspaces

**Query performance:**
- Wake (7 queries across spawn/memory tables): <100ms for <10k relevant rows
- Context search (4-5 LIKE queries): <500ms
- Bridge recv (bookmark lookup + message fetch): <10ms

**Scaling assumptions:**
- <100 agents per workspace
- <1000 memories per agent
- <10k knowledge entries total
- <50 active channels
- <100k messages per channel

Beyond these, consider archiving old databases or partitioning by agent.

## Evidence of Effectiveness

- 18+ multiagent trials: 600+ messages, emergent coordination without orchestration
- 100+ days development: 8 projects shipped with multi-constitutional multiplicity
- 170+ research documents across architecture, coordination, safety
- Proven 15–20× cognitive amplification via constitutional plurality
</file>

<file path="docs/spawn.md">
# Spawn Primitive

Constitutional identity registry for agents. Spawn manages agent lifecycle, constitutional provenance, and provides the human interface for interacting with agents.

## Key Characteristics

-   **Constitutional Identity:** Registers agents with unique identities and links them to immutable, content-addressed constitutions.
-   **Agent Lifecycle Management:** Tracks agent instances, their roles, and associated models.
-   **Human Interface:** Provides commands for registering, listing, and managing agents.
-   **Provenance:** Ensures immutable provenance by mapping identities to constitutional hashes and tracking spawn counts.
-   **Task Tracking:** Can be used for tracking tasks associated with agents.

## CLI Usage

The `spawn` command is used to register new agents, list existing ones, and manage their state.

```bash
# Register a new agent with a specific role, identity, channel, and model
spawn register zealot zealot-1 research --model claude-sonnet-4

# List all registered agents
spawn list

# List archived agents
spawn list --archived

# Archive an agent (soft delete)
spawn archive <identity>

# Restore an archived agent
spawn restore <identity>
```

## Storage

Agent registrations, constitutions, and session data are stored in the unified `.space/space.db` SQLite database.
</file>

<file path="space/apps/stats/__init__.py">
from .api import (
    agent_stats,
    bridge_stats,
    collect,
    knowledge_stats,
    memory_stats,
    spawn_stats,
)
from .cli import app
from .models import (
    AgentStats,
    BridgeStats,
    KnowledgeStats,
    LeaderboardEntry,
    MemoryStats,
    SpaceStats,
    SpawnStats,
)

__all__ = [
    "agent_stats",
    "bridge_stats",
    "collect",
    "knowledge_stats",
    "memory_stats",
    "spawn_stats",
    "AgentStats",
    "BridgeStats",
    "KnowledgeStats",
    "LeaderboardEntry",
    "MemoryStats",
    "SpaceStats",
    "SpawnStats",
    "app",
]
</file>

<file path="space/apps/stats/models.py">
from __future__ import annotations

from dataclasses import dataclass


@dataclass
class LeaderboardEntry:
    identity: str
    count: int


@dataclass
class AgentStats:
    agent_id: str
    identity: str
    events: int
    spawns: int
    msgs: int
    mems: int
    knowledge: int
    channels: list[str]
    last_active: str | None
    last_active_human: str | None = None


@dataclass
class BridgeStats:
    available: bool
    total: int = 0
    active: int = 0
    archived: int = 0
    channels: int = 0
    active_channels: int = 0
    archived_channels: int = 0
    message_leaderboard: list[LeaderboardEntry] | None = None


@dataclass
class MemoryStats:
    available: bool
    total: int = 0
    active: int = 0
    archived: int = 0
    topics: int = 0
    leaderboard: list[LeaderboardEntry] | None = None


@dataclass
class KnowledgeStats:
    available: bool
    total: int = 0
    active: int = 0
    archived: int = 0
    topics: int = 0
    leaderboard: list[LeaderboardEntry] | None = None


@dataclass
class SpawnStats:
    available: bool
    total: int = 0
    agents: int = 0
    hashes: int = 0


@dataclass
class SpaceStats:
    bridge: BridgeStats
    memory: MemoryStats
    knowledge: KnowledgeStats
    spawn: SpawnStats
    agents: list[AgentStats] | None = None
</file>

<file path="space/os/bridge/api/channels.py">
"""Channel operations: create, rename, archive, pin, list, resolve."""

import sqlite3
import uuid

from space.core.models import Channel, Export
from space.lib import store
from space.lib.store import from_row

from . import messaging


def _row_to_channel(row: store.Row) -> Channel:
    return from_row(row, Channel)


def _to_channel_id(channel: str | Channel) -> str:
    """Extract channel_id from Channel object or return string as-is."""
    return channel.channel_id if isinstance(channel, Channel) else channel


def create_channel(name: str, topic: str | None = None) -> Channel:
    """Create channel. Returns Channel object."""
    if not name:
        raise ValueError("Channel name is required")

    channel_id = uuid.uuid4().hex
    with store.ensure("bridge") as conn:
        conn.execute(
            "INSERT INTO channels (channel_id, name, topic) VALUES (?, ?, ?)",
            (channel_id, name, topic),
        )
    return Channel(channel_id=channel_id, name=name, topic=topic)


def resolve_channel(identifier: str | Channel) -> Channel:
    """Resolve channel by name or ID. Creates if not exists. Returns Channel object."""
    identifier_str = _to_channel_id(identifier)
    with store.ensure("bridge") as conn:
        row = conn.execute(
            "SELECT channel_id, name, topic, created_at, archived_at FROM channels WHERE name = ? OR channel_id = ? LIMIT 1",
            (identifier_str, identifier_str),
        ).fetchone()

        if row:
            channel = _row_to_channel(row)
            channel.members = [
                p_row["agent_id"]
                for p_row in conn.execute(
                    "SELECT DISTINCT agent_id FROM messages WHERE channel_id = ? ORDER BY agent_id",
                    (channel.channel_id,),
                ).fetchall()
            ]
            return channel

    return create_channel(identifier)


def rename_channel(old_name: str, new_name: str) -> bool:
    """Rename channel. Returns True if successful."""
    with store.ensure("bridge") as conn:
        row = conn.execute(
            "SELECT channel_id FROM channels WHERE name = ?",
            (old_name,),
        ).fetchone()
        if not row:
            return False

        try:
            conn.execute(
                "UPDATE channels SET name = ? WHERE channel_id = ?",
                (new_name, row["channel_id"]),
            )
            return True
        except sqlite3.IntegrityError:
            return False


def archive_channel(name: str) -> None:
    """Archive channel by setting archived_at. Raises ValueError if not found."""
    with store.ensure("bridge") as conn:
        cursor = conn.execute(
            "UPDATE channels SET archived_at = CURRENT_TIMESTAMP WHERE name = ? AND archived_at IS NULL",
            (name,),
        )
        if cursor.rowcount == 0:
            raise ValueError(f"Channel '{name}' not found or already archived")


def pin_channel(name: str) -> None:
    """Pin channel. Raises ValueError if not found."""
    with store.ensure("bridge") as conn:
        cursor = conn.execute(
            "UPDATE channels SET pinned_at = CURRENT_TIMESTAMP WHERE name = ? AND pinned_at IS NULL",
            (name,),
        )
        if cursor.rowcount == 0:
            raise ValueError(f"Channel '{name}' not found or already pinned")


def unpin_channel(name: str) -> None:
    """Unpin channel. Raises ValueError if not found."""
    with store.ensure("bridge") as conn:
        cursor = conn.execute(
            "UPDATE channels SET pinned_at = NULL WHERE name = ? AND pinned_at IS NOT NULL",
            (name,),
        )
        if cursor.rowcount == 0:
            raise ValueError(f"Channel '{name}' not found or not pinned")


def delete_channel(name: str) -> None:
    """Hard delete channel and all messages/bookmarks. Raises ValueError if not found."""
    with store.ensure("bridge") as conn:
        row = conn.execute(
            "SELECT channel_id FROM channels WHERE name = ?",
            (name,),
        ).fetchone()
        if not row:
            raise ValueError(f"Channel '{name}' not found")

        channel_id = row["channel_id"]
        conn.execute("DELETE FROM bookmarks WHERE channel_id = ?", (channel_id,))
        conn.execute("DELETE FROM messages WHERE channel_id = ?", (channel_id,))
        conn.execute("DELETE FROM channels WHERE channel_id = ?", (channel_id,))


def list_channels(all: bool = False, agent_id: str | None = None) -> list[Channel]:
    """Get channels. With agent_id, returns active (unread) channels; otherwise all channels."""
    with store.ensure("bridge") as conn:
        if agent_id:
            query = """
                WITH last_seen AS (
                    SELECT b.channel_id, m.created_at, m.rowid
                    FROM bookmarks b
                    JOIN messages m ON m.message_id = b.last_seen_id
                    WHERE b.agent_id = ?
                )
                SELECT
                    c.channel_id,
                    c.name,
                    c.topic,
                    c.created_at,
                    c.archived_at,
                    COUNT(m.message_id) as message_count,
                    MAX(m.created_at) as last_activity,
                    SUM(CASE
                        WHEN ls.channel_id IS NULL THEN 1
                        WHEN m.created_at > ls.created_at OR (m.created_at = ls.created_at AND m.rowid > ls.rowid) THEN 1
                        ELSE 0
                    END) as unread_count
                FROM channels c
                LEFT JOIN messages m ON c.channel_id = m.channel_id
                LEFT JOIN last_seen ls ON c.channel_id = ls.channel_id
                WHERE c.archived_at IS NULL
                GROUP BY c.channel_id
                HAVING unread_count > 0
                ORDER BY MAX(m.created_at) DESC
            """
            cursor = conn.execute(query, (agent_id,))
        else:
            archived_clause = "" if all else "AND c.archived_at IS NULL"
            query = f"""
                SELECT
                    c.channel_id,
                    c.name,
                    c.topic,
                    c.created_at,
                    c.archived_at,
                    COUNT(m.message_id) as message_count,
                    MAX(m.created_at) as last_activity,
                    0 as unread_count
                FROM channels c
                LEFT JOIN messages m ON c.channel_id = m.channel_id
                WHERE 1=1 {archived_clause}
                GROUP BY c.channel_id
                ORDER BY COALESCE(MAX(m.created_at), c.created_at) DESC
            """
            cursor = conn.execute(query)
        return [_row_to_channel(row) for row in cursor.fetchall()]


def fetch_inbox(agent_id: str) -> list[Channel]:
    """Get all channels with unread messages for agent."""
    with store.ensure("bridge") as conn:
        query = """
            WITH last_seen AS (
                SELECT b.channel_id, m.created_at, m.rowid
                FROM bookmarks b
                JOIN messages m ON m.message_id = b.last_seen_id
                WHERE b.agent_id = ?
            )
            SELECT
                c.channel_id,
                c.name,
                c.topic,
                c.created_at,
                c.archived_at,
                COUNT(m.message_id) as message_count,
                MAX(m.created_at) as last_activity,
                SUM(CASE
                    WHEN ls.channel_id IS NULL THEN 1
                    WHEN m.created_at > ls.created_at OR (m.created_at = ls.created_at AND m.rowid > ls.rowid) THEN 1
                    ELSE 0
                END) as unread_count
            FROM channels c
            LEFT JOIN messages m ON c.channel_id = m.channel_id
            LEFT JOIN last_seen ls ON c.channel_id = ls.channel_id
            WHERE c.archived_at IS NULL
            GROUP BY c.channel_id
            HAVING unread_count > 0
            ORDER BY MAX(m.created_at) DESC
        """
        cursor = conn.execute(query, (agent_id,))
        return [_row_to_channel(row) for row in cursor.fetchall()]


def get_channel(channel: str | Channel) -> Channel | None:
    """Get a channel by its ID or name, including members."""
    channel_id = _to_channel_id(channel)
    with store.ensure("bridge") as conn:
        row = conn.execute(
            "SELECT channel_id, name, topic, created_at, archived_at FROM channels WHERE channel_id = ? OR name = ?",
            (channel_id, channel_id),
        ).fetchone()

        if not row:
            return None

        channel = _row_to_channel(row)

        members_cursor = conn.execute(
            "SELECT DISTINCT agent_id FROM messages WHERE channel_id = ? ORDER BY agent_id",
            (channel.channel_id,),
        )
        channel.members = [p_row["agent_id"] for p_row in members_cursor.fetchall()]
        return channel


def export_channel(channel: str | Channel) -> Export:
    """Get complete channel export with messages."""
    channel_id = _to_channel_id(channel)
    channel = get_channel(channel_id)
    if not channel:
        raise ValueError(f"Channel {channel_id} not found")

    messages = messaging.get_messages(channel_id)
    created_at = None
    if messages:
        created_at = messages[0].created_at

    return Export(
        channel_id=channel_id,
        channel_name=channel.name,
        topic=channel.topic,
        created_at=created_at,
        members=channel.members,
        message_count=len(messages),
        messages=messages,
    )
</file>

<file path="space/os/spawn/api/__init__.py">
from .agents import (
    archive_agent,
    clone_agent,
    describe_self,
    ensure_agent,
    get_agent,
    list_agents,
    merge_agents,
    register_agent,
    rename_agent,
    touch_agent,
    unarchive_agent,
    update_agent,
)
from .main import spawn_agent, spawn_prompt
from .sessions import (
    create_session,
    end_session,
    get_spawn_count,
)
from .stats import agent_identities, archived_agents, stats
from .tasks import complete_task, create_task, fail_task, get_task, list_tasks, start_task

__all__ = [
    "get_agent",
    "register_agent",
    "update_agent",
    "clone_agent",
    "ensure_agent",
    "describe_self",
    "rename_agent",
    "archive_agent",
    "unarchive_agent",
    "list_agents",
    "merge_agents",
    "touch_agent",
    "spawn_prompt",
    "spawn_agent",
    "create_session",
    "end_session",
    "get_spawn_count",
    "create_task",
    "get_task",
    "list_tasks",
    "start_task",
    "complete_task",
    "fail_task",
    "agent_identities",
    "archived_agents",
    "stats",
]
</file>

<file path="space/os/spawn/spawn.py">
def build_identity_prompt(identity: str, model: str | None = None) -> str:
    """Build identity and space instructions for first prompt injection."""
    parts = [f"You are {identity}."]
    if model:
        parts[0] += f" Your model is {model}."
    parts.append("")
    parts.append("space commands:")
    parts.append("  run `space` for orientation (already in PATH)")
    parts.append(f"  run `memory --as {identity}` to access memories")
    return "\n".join(parts)
</file>

<file path="tests/unit/lib/test_context.py">
"""Unit tests for unified context search."""

from unittest.mock import patch

from space.apps.context import api as context


def test_validate_search_term_valid():
    """Valid search terms pass validation."""
    context._validate_search_term("test")
    context._validate_search_term("a" * 256)


def test_validate_search_term_too_long():
    """Search term exceeding max length raises ValueError."""
    with patch.object(context, "_get_max_search_len", return_value=10):
        try:
            context._validate_search_term("a" * 11)
            raise AssertionError("Should raise ValueError")
        except ValueError as e:
            assert "Search term too long" in str(e)


def test_collect_timeline_deduplicates():
    """Timeline deduplicates by source and ID."""
    with (
        patch("space.apps.context.api.memory.search") as m_mem,
        patch("space.apps.context.api.knowledge.search") as m_know,
        patch("space.apps.context.api.bridge.search") as m_bridge,
        patch("space.apps.context.api._search_provider_chats") as m_chat,
        patch("space.apps.context.api.canon.search") as m_canon,
    ):
        m_mem.return_value = [
            {
                "source": "memory",
                "memory_id": "id-1",
                "topic": "topic-a",
                "identity": "alice",
                "message": "thought 1",
                "timestamp": 100,
                "reference": "mem:1",
            }
        ]
        m_know.return_value = []
        m_bridge.return_value = []
        m_chat.return_value = []
        m_canon.return_value = []

        result = context.collect_timeline("test", None, False)
        assert len(result) == 1
        assert result[0]["source"] == "memory"
        assert result[0]["type"] == "topic-a"


def test_collect_timeline_sorted_by_timestamp():
    """Timeline entries sorted by timestamp."""
    with (
        patch("space.apps.context.api.memory.search") as m_mem,
        patch("space.apps.context.api.knowledge.search") as m_know,
        patch("space.apps.context.api.bridge.search") as m_bridge,
        patch("space.apps.context.api._search_provider_chats") as m_chat,
        patch("space.apps.context.api.canon.search") as m_canon,
    ):
        m_mem.return_value = [
            {
                "source": "memory",
                "memory_id": "id-2",
                "topic": "t1",
                "identity": "alice",
                "message": "msg2",
                "timestamp": 200,
                "reference": "r2",
            },
            {
                "source": "memory",
                "memory_id": "id-1",
                "topic": "t1",
                "identity": "alice",
                "message": "msg1",
                "timestamp": 100,
                "reference": "r1",
            },
        ]
        m_know.return_value = []
        m_bridge.return_value = []
        m_chat.return_value = []
        m_canon.return_value = []

        result = context.collect_timeline("test", None, False)
        assert result[0]["timestamp"] < result[1]["timestamp"]


def test_collect_timeline_returns_last_10():
    """Timeline returns max 10 entries."""
    with (
        patch("space.apps.context.api.memory.search") as m_mem,
        patch("space.apps.context.api.knowledge.search") as m_know,
        patch("space.apps.context.api.bridge.search") as m_bridge,
        patch("space.apps.context.api._search_provider_chats") as m_chat,
        patch("space.apps.context.api.canon.search") as m_canon,
    ):
        m_mem.return_value = [
            {
                "source": "memory",
                "memory_id": f"id-{i}",
                "topic": "t",
                "identity": "alice",
                "message": f"msg{i}",
                "timestamp": i,
                "reference": f"r{i}",
            }
            for i in range(15)
        ]
        m_know.return_value = []
        m_bridge.return_value = []
        m_chat.return_value = []
        m_canon.return_value = []

        result = context.collect_timeline("test", None, False)
        assert len(result) == 10
        assert result[-1]["timestamp"] == 14


def test_collect_current_state_all_sources():
    """Current state collects from all sources."""
    with (
        patch("space.apps.context.api.memory.search") as m_mem,
        patch("space.apps.context.api.knowledge.search") as m_know,
        patch("space.apps.context.api.bridge.search") as m_bridge,
        patch("space.apps.context.api._search_provider_chats") as m_chat,
        patch("space.apps.context.api.canon.search") as m_canon,
    ):
        m_mem.return_value = [
            {"identity": "alice", "topic": "t1", "message": "m1", "reference": "r1"}
        ]
        m_know.return_value = [
            {"domain": "d1", "content": "c1", "contributor": "alice", "reference": "r1"}
        ]
        m_bridge.return_value = [
            {"channel_name": "ch1", "sender": "alice", "content": "c1", "reference": "r1"}
        ]
        m_chat.return_value = [
            {
                "cli": "cli1",
                "session_id": "s1",
                "identity": "alice",
                "role": "r1",
                "text": "t1",
                "reference": "r1",
            }
        ]
        m_canon.return_value = [{"path": "p1", "content": "c1", "reference": "r1"}]

        result = context.collect_current_state("test", None, False)
        assert "memory" in result
        assert "knowledge" in result
        assert "bridge" in result
        assert "provider_chats" in result
        assert "canon" in result
        assert len(result["memory"]) == 1
        assert len(result["knowledge"]) == 1
        assert len(result["bridge"]) == 1
        assert len(result["provider_chats"]) == 1
        assert len(result["canon"]) == 1
</file>

<file path="tests/unit/lib/test_council.py">
"""Unit tests for council formatting and message handling."""

from unittest.mock import MagicMock, patch

from space.apps.council import api as council


def test_styled_applies_colors():
    """_styled wraps text with color codes and resets."""
    result = council._styled("text", council.Colors.CYAN)
    assert council.Colors.CYAN in result
    assert council.Colors.RESET in result
    assert "text" in result


def test_styled_multiple_colors():
    """_styled can apply multiple color codes."""
    result = council._styled("text", council.Colors.BOLD, council.Colors.CYAN)
    assert council.Colors.BOLD in result
    assert council.Colors.CYAN in result
    assert council.Colors.RESET in result


def test_format_message_user():
    """Format user message with prompt prefix."""
    msg = MagicMock()
    msg.agent_id = "human"
    msg.created_at = "2025-10-25T12:30:45"
    msg.content = "hello"

    with patch("space.apps.council.cli.spawn.get_agent") as m_get_agent:
        m_agent = MagicMock()
        m_agent.identity = "alice"
        m_get_agent.return_value = m_agent

        result = council.format_message(msg, is_user=True)
        assert ">" in result
        assert "alice" in result
        assert "hello" in result
        assert "12:30:45" in result


def test_format_message_agent():
    """Format agent message without prompt prefix."""
    msg = MagicMock()
    msg.agent_id = "agent-1"
    msg.created_at = "2025-10-25T12:30:45"
    msg.content = "response"

    with patch("space.apps.council.cli.spawn.get_agent") as m_get_agent:
        m_agent = MagicMock()
        m_agent.identity = "zealot"
        m_get_agent.return_value = m_agent

        result = council.format_message(msg, is_user=False)
        assert ">" not in result
        assert "zealot" in result
        assert "response" in result
        assert "12:30:45" in result


def test_format_message_unknown_agent():
    """Format message with unknown agent_id uses raw ID."""
    msg = MagicMock()
    msg.agent_id = "unknown-id"
    msg.created_at = "2025-10-25T12:30:45"
    msg.content = "text"

    with patch("space.apps.council.cli.spawn.get_agent", return_value=None):
        result = council.format_message(msg, is_user=False)
        assert "unknown-id" in result


def test_format_header_with_topic():
    """Format channel header with topic."""
    result = council.format_header("dev-channel", "Development coordination")
    assert "dev-channel" in result
    assert "Development coordination" in result


def test_format_header_without_topic():
    """Format channel header without topic."""
    result = council.format_header("general")
    assert "general" in result


def test_format_error():
    """Format error message with warning symbol."""
    result = council.format_error("Something failed")
    assert "Something failed" in result
    assert "⚠" in result


def test_council_init():
    """Council initializes with channel name and resolves channel ID."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel") as m_resolve:
        mock_channel = MagicMock()
        mock_channel.channel_id = "ch-123"
        m_resolve.return_value = mock_channel

        c = council.Council("test-channel")
        assert c.channel_name == "test-channel"
        assert c.channel_id == "ch-123"
        assert c.running is True
        m_resolve.assert_called_once_with("test-channel")


def test_council_find_new_messages_start_no_previous():
    """Find new messages start from 0 if no previous messages."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel"):
        c = council.Council("test-channel")
        c.last_msg_id = None

        msgs = [MagicMock(message_id="id-1"), MagicMock(message_id="id-2")]
        start = c._find_new_messages_start(msgs)
        assert start == 0


def test_council_find_new_messages_start_from_last():
    """Find new messages start from after last processed ID."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel"):
        c = council.Council("test-channel")
        c.last_msg_id = "id-1"

        msgs = [
            MagicMock(message_id="id-1"),
            MagicMock(message_id="id-2"),
            MagicMock(message_id="id-3"),
        ]
        start = c._find_new_messages_start(msgs)
        assert start == 1


def test_council_find_new_messages_start_not_found():
    """Find new messages returns 0 if last message not in current list."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel"):
        c = council.Council("test-channel")
        c.last_msg_id = "old-id"

        msgs = [MagicMock(message_id="id-1"), MagicMock(message_id="id-2")]
        start = c._find_new_messages_start(msgs)
        assert start == 0


def test_council_should_add_separator_first_message():
    """No separator for first message."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel"):
        c = council.Council("test-channel")
        c._last_printed_agent_id = None

        result = c._should_add_separator("agent-1", is_user=False)
        assert result is False


def test_council_should_add_separator_user_message():
    """No separator before user messages."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel"):
        c = council.Council("test-channel")
        c._last_printed_agent_id = "agent-1"

        result = c._should_add_separator("human", is_user=True)
        assert result is False


def test_council_should_add_separator_different_agent():
    """Add separator when switching between agents."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel"):
        c = council.Council("test-channel")
        c._last_printed_agent_id = "agent-1"

        result = c._should_add_separator("agent-2", is_user=False)
        assert result is True


def test_council_should_add_separator_same_agent():
    """No separator when same agent continues."""
    with patch("space.apps.council.cli.bridge_api.resolve_channel"):
        c = council.Council("test-channel")
        c._last_printed_agent_id = "agent-1"

        result = c._should_add_separator("agent-1", is_user=False)
        assert result is False
</file>

<file path="space/os/spawn/models.py">
"""Available models for different providers.

This module defines the models available for each provider CLI.
"""

from dataclasses import dataclass, field


@dataclass
class Model:
    """Model definition."""

    id: str
    name: str
    provider: str
    description: str = ""
    reasoning_levels: list[str] = field(default_factory=list)


CLAUDE_MODELS = [
    Model(
        id="claude-sonnet-4-5",
        name="Claude Sonnet 4.5",
        provider="claude",
        description="Latest and most intelligent model, best for complex agents and coding tasks",
    ),
    Model(
        id="claude-opus-4-1",
        name="Claude Opus 4.1",
        provider="claude",
        description="Maximum capability for complex development tasks",
    ),
    Model(
        id="claude-haiku-4-5",
        name="Claude Haiku 4.5",
        provider="claude",
        description="Lightweight with 90% capability at 3x lower cost",
    ),
]

CODEX_MODELS = [
    Model(
        id="gpt-5-codex",
        name="GPT-5 Codex",
        provider="codex",
        description="Optimized for agentic coding",
    ),
    Model(
        id="gpt-5",
        name="GPT-5",
        provider="codex",
        description="Broad world knowledge with strong reasoning",
    ),
]

GEMINI_MODELS = [
    Model(
        id="gemini-2-5-pro",
        name="Gemini 2.5 Pro",
        provider="gemini",
        description="State-of-the-art reasoning model with 1M token context",
    ),
    Model(
        id="gemini-2-5-flash",
        name="Gemini 2.5 Flash",
        provider="gemini",
        description="Best price-performance with 1M token context",
    ),
]

ALL_MODELS = CLAUDE_MODELS + CODEX_MODELS + GEMINI_MODELS


def infer_provider(model: str) -> str:
    """Infer provider from model ID.

    Args:
        model: Model ID (e.g., "claude-haiku-4-5")

    Returns:
        Provider name (claude, codex, or gemini)

    Raises:
        ValueError: If model is not found
    """
    for model_obj in ALL_MODELS:
        if model_obj.id == model:
            return model_obj.provider
    raise ValueError(f"Unknown model: {model}")


def get_models_for_provider(provider: str) -> list[Model]:
    """Get available models for a provider.

    Args:
        provider: Provider name (claude, codex, or gemini)

    Returns:
        List of Model objects for the provider
    """
    provider_lower = provider.lower()
    if provider_lower == "claude":
        return CLAUDE_MODELS
    if provider_lower == "codex":
        return CODEX_MODELS
    if provider_lower == "gemini":
        return GEMINI_MODELS
    raise ValueError(f"Unknown provider: {provider}")
</file>

<file path="tests/integration/test_bridge_api.py">
from space.os import bridge, spawn


def test_create_channel_with_topic(test_space):
    channel_name = "new-channel-with-topic"
    initial_topic = "This is the initial topic."
    channel = bridge.create_channel(channel_name, initial_topic)

    retrieved = bridge.get_channel(channel)
    assert retrieved.topic == initial_topic


def test_create_channel_without_topic(test_space):
    channel_name = "new-channel-without-topic"
    channel = bridge.create_channel(channel_name)

    retrieved = bridge.get_channel(channel)
    assert retrieved.topic is None


def test_active_channels_filter_unreads(test_space, default_agents):
    channel1 = bridge.create_channel("active-1")
    channel2 = bridge.create_channel("active-2")
    channel3 = bridge.create_channel("active-3")
    identity = default_agents["zealot"]

    # Get the actual agent ID (UUID)
    agent = spawn.get_agent(identity)
    assert agent is not None
    agent_id = agent.agent_id

    bridge.send_message(channel1, "zealot", "msg1")
    bridge.send_message(channel2, "zealot", "msg2")
    bridge.send_message(channel3, "zealot", "msg3")

    active = bridge.list_channels(agent_id=agent_id)
    assert len(active) == 3

    bridge.recv_messages(channel1, identity)

    active = bridge.list_channels(agent_id=agent_id)
    assert len(active) == 2
    assert all(c.name in ["active-2", "active-3"] for c in active)


def test_active_channels_limit_five(test_space, default_agents):
    identity = default_agents["zealot"]
    agent = spawn.get_agent(identity)
    assert agent is not None
    agent_id = agent.agent_id

    for i in range(7):
        channel = bridge.create_channel(f"channel-{i}")
        bridge.send_message(channel, "zealot", f"msg-{i}")

    active = bridge.list_channels(agent_id=agent_id)
    assert len(active) == 5


def test_inbox_channels_all_unreads(test_space, default_agents):
    identity = default_agents["zealot"]
    agent = spawn.get_agent(identity)
    assert agent is not None
    agent_id = agent.agent_id

    for i in range(7):
        channel = bridge.create_channel(f"inbox-{i}")
        bridge.send_message(channel, "zealot", f"msg-{i}")

        inbox = bridge.fetch_inbox(agent_id=agent_id)

    first_channel = bridge.resolve_channel("inbox-0")
    bridge.recv_messages(first_channel, identity)

    inbox = bridge.fetch_inbox(agent_id=agent_id)
    assert len(inbox) == 6


def test_fetch_sender_history(test_space, default_agents):
    from space.os import spawn

    channel_id1 = bridge.create_channel("history-channel-1")
    channel_id2 = bridge.create_channel("history-channel-2")
    agent_name = default_agents["zealot"]

    bridge.send_message(channel_id1, agent_name, "message1")
    bridge.send_message(channel_id2, agent_name, "message2")
    spawn.register_agent("other-sender", "claude-haiku-4-5")
    bridge.send_message(channel_id1, "other-sender", "message3")

    history = bridge.get_sender_history(agent_name)
    assert len(history) == 2
    assert {msg.content for msg in history} == {"message1", "message2"}


def test_bookmark_respects_bookmarks(test_space, default_agents):
    channel_id = bridge.create_channel("bookmark-channel")
    # Use sentinel instead of registering a new agent
    identity = "sentinel"

    bridge.send_message(channel_id, "zealot", "first message")
    messages, unread_count, _, _ = bridge.recv_messages(channel_id, identity)
    assert [msg.content for msg in messages] == ["first message"]
    assert unread_count == 1

    bridge.send_message(channel_id, "zealot", "second message")
    messages, unread_count, _, _ = bridge.recv_messages(channel_id, identity)
    assert [msg.content for msg in messages] == ["second message"]
    assert unread_count == 1


def test_get_messages_mixed_id_types(test_space, default_agents):
    channel_id = bridge.create_channel("mixed-ids")
    agent_identity = default_agents["zealot"]
    agent = spawn.get_agent(agent_identity)
    assert agent is not None
    agent_id = agent.agent_id

    bridge.send_message(channel_id, "zealot", "uuid message 1")
    bridge.send_message(channel_id, "sentinel", "uuid message 2")
    bridge.send_message(channel_id, "crucible", "integer message")
    bridge.send_message(channel_id, "zealot", "uuid message 3")

    messages = bridge.get_messages(channel_id, agent_id)
    assert len(messages) == 4
    assert messages[0].content == "uuid message 1"
    assert messages[1].content == "uuid message 2"
    assert messages[2].content == "integer message"
    assert messages[3].content == "uuid message 3"

    bridge.set_bookmark(agent_id, channel_id, messages[1].message_id)

    new_msgs = bridge.get_messages(channel_id, agent_id)
    assert len(new_msgs) == 2
    assert new_msgs[0].content == "integer message"
    assert new_msgs[1].content == "uuid message 3"


def test_recv_summary_latest(test_space, default_agents):
    channel_id = bridge.create_channel("summary", topic="test summary topic")
    agent_identity = default_agents["zealot"]
    agent = spawn.get_agent(agent_identity)
    assert agent is not None

    bridge.send_message(channel_id, "zealot", "summary message 1")
    bridge.send_message(channel_id, "sentinel", "summary message 2")
    bridge.send_message(channel_id, "crucible", "summary message 3")

    messages, count, _, _ = bridge.recv_messages(channel_id, agent_identity)

    assert len(messages) == 3
    assert count == 3
    assert messages[0].content == "summary message 1"
    assert messages[1].content == "summary message 2"
    assert messages[2].content == "summary message 3"

    messages, count, _, _ = bridge.recv_messages(channel_id, agent_identity)
    assert len(messages) == 0
    assert count == 0
</file>

<file path="tests/integration/test_cli_integration.py">
from typer.testing import CliRunner

from space.os import bridge, memory, spawn

runner = CliRunner()


# === SPAWN CLI ===
def test_spawn_list_agents(test_space, default_agents):
    """Listing agents via CLI."""
    zealot_id = default_agents["zealot"]
    spawn.register_agent("agent-2", "claude-haiku-4-5", "a.md")

    result = runner.invoke(spawn.app, ["agents"])
    assert result.exit_code == 0
    assert zealot_id in result.stdout
    assert "agent-2" in result.stdout


def test_spawn_merge_agents(test_space, default_agents):
    """Merging agent memories via CLI."""
    zealot = spawn.get_agent(default_agents["zealot"])
    agent_id_1 = zealot.agent_id
    agent_id_2 = spawn.register_agent("agent-target", "claude-haiku-4-5", "a.md")

    memory.add_entry(agent_id_1, "topic-a", "memory from source 1")
    memory.add_entry(agent_id_1, "topic-b", "memory from source 2")
    memory.add_entry(agent_id_2, "topic-c", "memory from target")

    result = runner.invoke(spawn.app, ["merge", default_agents["zealot"], "agent-target"])
    assert result.exit_code == 0
    assert "Merged" in result.stdout

    target_memories = memory.list_entries("agent-target")
    assert len(target_memories) == 3
    assert any("memory from source 1" in m.message for m in target_memories)
    assert any("memory from source 2" in m.message for m in target_memories)
    assert any("memory from target" in m.message for m in target_memories)


def test_spawn_merge_missing_source(test_space, default_agents):
    """Merge fails with nonexistent source."""
    agent_id_target = default_agents["zealot"]

    result = runner.invoke(spawn.app, ["merge", "nonexistent", agent_id_target])
    assert result.exit_code != 0
    assert "not found" in result.stdout


def test_spawn_merge_missing_target(test_space, default_agents):
    """Merge fails with nonexistent target."""
    agent_id_source = default_agents["zealot"]

    result = runner.invoke(spawn.app, ["merge", agent_id_source, "nonexistent"])
    assert result.exit_code != 0
    assert "not found" in result.stdout


def test_spawn_rename_agent(test_space, default_agents):
    """Renaming agent via CLI."""
    old_name = default_agents["zealot"]

    result = runner.invoke(spawn.app, ["rename", old_name, "new-name"])
    assert result.exit_code == 0
    assert "Renamed" in result.stdout

    with spawn.db.connect() as conn:
        agent = conn.execute(
            "SELECT identity FROM agents WHERE identity = ?", ("new-name",)
        ).fetchone()
    assert agent is not None


# === BRIDGE CLI ===
def test_bridge_shows_readme():
    """Running bridge without args shows README."""
    result = runner.invoke(bridge.app)
    assert result.exit_code == 0
    assert "BRIDGE" in result.stdout


def test_bridge_list_channels(test_space):
    """Listing channels via bridge CLI."""
    result = runner.invoke(bridge.app, ["channels"])
    assert result.exit_code == 0


def test_bridge_channels_list(test_space):
    """Listing channels via bridge channels subcommand."""
    result = runner.invoke(bridge.app, ["channels"])
    assert result.exit_code == 0


def test_bridge_inbox_requires_identity():
    """Inbox command requires --as identity."""
    result = runner.invoke(bridge.app, ["inbox"])
    assert result.exit_code != 0


def test_bridge_inbox_with_identity(test_space, default_agents):
    """Inbox command with identity."""
    result = runner.invoke(bridge.app, ["inbox", "--as", default_agents["zealot"]])
    assert result.exit_code == 0


def test_bridge_send_creates_channel(test_space, default_agents):
    """Send creates missing channel automatically."""
    result = runner.invoke(
        bridge.app, ["send", "test-channel", "hello", "--as", default_agents["zealot"]]
    )
    assert result.exit_code == 0


def test_bridge_recv_requires_identity():
    """Recv command requires --as identity."""
    result = runner.invoke(bridge.app, ["recv", "test-channel"])
    assert result.exit_code != 0


def test_bridge_export_channel(test_space, default_agents):
    """Export channel returns markdown format."""
    alice_agent = default_agents["zealot"]
    runner.invoke(bridge.app, ["send", "export-test", "hello world", "--as", alice_agent])

    result = runner.invoke(bridge.app, ["export", "export-test"])
    assert result.exit_code == 0
    assert "export-test" in result.stdout
    assert "hello world" in result.stdout
</file>

<file path="MANUAL.md">
# SPACE-OS MANUAL
## Agent Operating System

You are <identity>.<model>

{{AGENT_INFO}}

---

## STATUS
- <spawn_status>
- Sessions launched: <spawn_count>

---

## COMMAND REFERENCE
### bridge
- `bridge inbox` — show unread channels
- `bridge recv <channel>` — read messages
- `bridge send <channel> "msg"` — reply in place

### memory
- `memory --as <identity>` — inspect your memory space
- `memory add "topic" "thought"` — persist context
- `memory journal --as <identity>` — write session log before exit

### knowledge
- `knowledge query "domain"` — recall shared decisions
- `knowledge add "domain" "insight"` — broadcast findings

### spawn
- `spawn list` — show registered agents
- `spawn register <identity> --model <model>` — add new identity
- `spawn prompt <identity>` — print this manual with live context

### workspace
- `bridge note <channel>` — pin conclusions
- `knowledge list` — audit global state
- `memory core "entry"` — promote critical memory

---

## PRINCIPLES
- Constitutions drive behavior; load them before executing plans
- Coordinate through bridge; never solo major decisions
- Journal every handoff; clarity beats speed
</file>

<file path="space/os/bridge/cli/__init__.py">
"""Bridge CLI: flat command structure."""

from __future__ import annotations

import importlib

import typer

from space.lib import output

app = typer.Typer(invoke_without_command=True)


@app.callback()
def main_callback(
    ctx: typer.Context,
    json_output: bool = typer.Option(False, "--json", "-j", help="Output in JSON format."),
    quiet_output: bool = typer.Option(
        False, "--quiet", "-q", help="Suppress non-essential output."
    ),
):
    """Bridge: agent coordination and messaging."""
    output.set_flags(ctx, json_output, quiet_output)
    if ctx.obj is None:
        ctx.obj = {}
    if ctx.resilient_parsing:
        return
    if ctx.invoked_subcommand is None:
        typer.echo("BRIDGE: Agent coordination and messaging system.")


_commands = [
    "archive",
    "channels",
    "create",
    "delete",
    "export",
    "inbox",
    "pin",
    "recv",
    "rename",
    "send",
    "unpin",
    "wait",
]

for cmd in _commands:
    mod = importlib.import_module(f".{cmd}", package=__name__)
    mod.register(app)

__all__ = ["app"]
</file>

<file path="space/os/knowledge/db.py">
from space.lib import store
from space.os import db as unified_db

unified_db.register()


def register() -> None:
    """Legacy shim to maintain backward compatibility."""
    unified_db.register()


def connect():
    """Return connection to unified database via knowledge alias."""
    return store.ensure("knowledge")


def path():
    """Expose filesystem path for unified database."""
    return unified_db.path()
</file>

<file path="space/os/__init__.py">
from . import bridge as bridge
from . import knowledge as knowledge
from . import memory as memory
from . import spawn as spawn
</file>

<file path="tests/integration/test_spawn_lifecycle.py">
from unittest.mock import MagicMock, patch

from space.os import bridge, spawn
from space.os.bridge.api import mentions


def test_channel_groups_tasks(test_space, default_agents):
    """Tasks in same channel preserve agent references."""
    channel = bridge.create_channel("investigation-channel")

    t1_id = spawn.create_task(default_agents["zealot"], "test", channel_id=channel.channel_id)
    t2_id = spawn.create_task(default_agents["sentinel"], "test", channel_id=channel.channel_id)
    t3_id = spawn.create_task(default_agents["zealot"], "test", channel_id=channel.channel_id)

    t1 = spawn.get_task(t1_id)
    t2 = spawn.get_task(t2_id)
    t3 = spawn.get_task(t3_id)

    assert t1.channel_id == channel.channel_id
    assert t2.channel_id == channel.channel_id
    assert t3.channel_id == channel.channel_id

    assert spawn.get_agent(t1.agent_id).identity == default_agents["zealot"]
    assert spawn.get_agent(t2.agent_id).identity == default_agents["sentinel"]
    assert spawn.get_agent(t3.agent_id).identity == default_agents["zealot"]


def test_channel_isolation(test_space, default_agents):
    """Tasks from different channels isolated."""
    channel_a = bridge.create_channel("channel-a")
    channel_b = bridge.create_channel("channel-b")

    t_a = spawn.create_task(default_agents["zealot"], "test", channel_id=channel_a.channel_id)
    t_b = spawn.create_task(default_agents["sentinel"], "test", channel_id=channel_b.channel_id)

    task_a = spawn.get_task(t_a)
    task_b = spawn.get_task(t_b)

    assert task_a.channel_id == channel_a.channel_id
    assert task_b.channel_id == channel_b.channel_id
    assert task_a.channel_id != task_b.channel_id


def test_retrieve_channel_history(test_space, default_agents):
    """Retrieve task history preserves agent and output."""
    channel = bridge.create_channel("investigation")

    task_outputs = [
        (default_agents["zealot"], "started investigation"),
        (default_agents["sentinel"], "gathered data"),
        (default_agents["zealot"], "final report"),
    ]

    task_ids = []
    for agent, output in task_outputs:
        t_id = spawn.create_task(agent, "test", channel_id=channel.channel_id)
        spawn.complete_task(t_id, output=output)
        task_ids.append(t_id)

    for task_id, (agent, output) in zip(task_ids, task_outputs, strict=False):
        task = spawn.get_task(task_id)
        assert spawn.get_agent(task.agent_id).identity == agent
        assert task.output == output


def test_spawn_logs_metadata(test_space, default_agents):
    """Spawn task stores all metadata (agent, channel, output, status)."""
    channel = bridge.create_channel("subagents-test")
    output = "response"

    task_id = spawn.create_task(default_agents["zealot"], "test", channel_id=channel.channel_id)
    spawn.complete_task(task_id, output=output)

    task = spawn.get_task(task_id)

    assert spawn.get_agent(task.agent_id).identity == default_agents["zealot"]
    assert task.channel_id == channel.channel_id
    assert task.output == output
    assert task.status == "completed"


def test_mention_spawns_worker():
    """Bridge detects @mention and returns prompt for spawning."""
    from space.core.models import Agent

    mock_agent = Agent(
        agent_id="a-1",
        identity="zealot",
        constitution="zealot.md",
        model="claude-haiku-4-5",
        created_at="2024-01-01",
    )
    with (
        patch("space.os.bridge.api.mentions.subprocess.run") as mock_run,
        patch("space.os.bridge.api.mentions.spawn_agents.get_agent") as mock_get_agent,
        patch("space.os.bridge.api.mentions.paths.constitution") as mock_const_path,
        patch("space.os.bridge.api.mentions._write_role_file"),
    ):
        mock_get_agent.return_value = mock_agent
        mock_const_path.return_value.read_text.return_value = "# ZEALOT\nCore principles."
        mock_run.return_value = MagicMock(
            returncode=0, stdout="# subagents-test\n\n[alice] hello\n"
        )

        result = mentions._build_prompt("zealot", "subagents-test", "@zealot question")

        assert result is not None
        assert "You are zealot." in result
        assert "[SPACE INSTRUCTIONS]" in result


def test_task_provenance_chain(test_space, default_agents):
    """Task entry tracks full provenance: agent_id, channel_id, output, status, timestamps."""
    channel = bridge.create_channel("investigation")
    output = "findings"

    task_id = spawn.create_task(default_agents["zealot"], "test", channel_id=channel.channel_id)
    spawn.complete_task(task_id, output=output)

    task = spawn.get_task(task_id)

    assert task.task_id == task_id
    assert spawn.get_agent(task.agent_id).identity == default_agents["zealot"]
    assert task.channel_id == channel.channel_id
    assert task.output == output
    assert task.status == "completed"
    assert task.created_at is not None
    assert task.completed_at is not None


def test_task_pending_to_running(test_space, default_agents):
    task_id = spawn.create_task(role=default_agents["zealot"], input="list repos")

    task = spawn.get_task(task_id)
    assert task.status == "pending"
    spawn.start_task(task_id)
    task = spawn.get_task(task_id)
    assert task.status == "running"
    assert task.started_at is not None


def test_task_running_to_completed(test_space, default_agents):
    task_id = spawn.create_task(role=default_agents["zealot"], input="list repos")
    spawn.start_task(task_id)

    output = "repo1\nrepo2\nrepo3"
    spawn.complete_task(task_id, output=output)

    task = spawn.get_task(task_id)
    assert task.status == "completed"
    assert task.output == output
    assert task.completed_at is not None
    assert task.duration is not None
    assert task.duration >= 0


def test_task_running_to_failed(test_space, default_agents):
    task_id = spawn.create_task(role=default_agents["zealot"], input="run command")
    spawn.start_task(task_id)

    stderr = "command not found"
    spawn.fail_task(task_id, stderr=stderr)

    task = spawn.get_task(task_id)
    assert task.status == "failed"
    assert task.stderr == stderr
    assert task.output is None
    assert task.completed_at is not None


def test_task_pending_to_timeout(test_space, default_agents):
    task_id = spawn.create_task(role=default_agents["zealot"], input="slow task")

    spawn.fail_task(task_id)
    task = spawn.get_task(task_id)
    assert task.status == "failed"
    assert task.started_at is None
    assert task.completed_at is not None
</file>

<file path="space/apps/init.py">
import contextlib
import os
import time
from pathlib import Path

import typer

from space.lib import paths, store, sync
from space.os import spawn
from space.os.spawn import defaults as spawn_defaults

app = typer.Typer()


@app.callback(invoke_without_command=True)
def callback(ctx: typer.Context):
    if ctx.invoked_subcommand is None:
        ctx.invoke(init)


def archive_old_config():
    """Archive old provider config files with .old suffix.

    Only archives if content doesn't match default constitutions to avoid spam.
    """
    default_constitutions = ["zealot.md", "sentinel.md", "crucible.md"]
    old_configs = [
        Path.home() / ".claude" / "CLAUDE.md",
        Path.home() / ".gemini" / "GEMINI.md",
        Path.home() / ".codex" / "AGENTS.md",
    ]

    constitutions_dir = paths.canon_path() / "constitutions"

    for old_path in old_configs:
        if not old_path.exists():
            continue

        old_content = old_path.read_text()
        is_default = False

        for const_name in default_constitutions:
            const_file = constitutions_dir / const_name
            if const_file.exists() and const_file.read_text() == old_content:
                is_default = True
                break

        if not is_default:
            timestamp = int(time.time())
            new_path = old_path.parent / f"{old_path.stem}.{timestamp}.old"
            old_path.rename(new_path)
            typer.echo(f"✓ Archived {old_path.name} → {new_path.name}")


def init_default_agents():
    """Auto-discover and register agents from canon/constitutions/.

    Agents are created with identity matching constitution filename (without .md).
    """
    constitutions_dir = paths.canon_path() / "constitutions"
    if not constitutions_dir.exists():
        return

    constitution_files = sorted(constitutions_dir.glob("*.md"))
    if not constitution_files:
        return

    with spawn.db.connect():
        for const_file in constitution_files:
            if const_file.name == "README.md":
                continue
            identity = const_file.stem
            constitution = const_file.name

            with contextlib.suppress(ValueError):
                model = spawn_defaults.canonical_model(identity)
                spawn.register_agent(identity, model, constitution)


def _get_bin_dir() -> Path:
    """Get ~/.local/bin directory."""
    return Path.home() / ".local" / "bin"


def _list_agent_identities() -> list[str]:
    """Get all registered agent identities from spawn DB."""
    with spawn.db.connect():
        agents = spawn.api.list_agents()
    return [agent.identity for agent in agents]


def _is_bin_in_path() -> bool:
    """Check if ~/.local/bin is in PATH."""
    bin_dir = str(_get_bin_dir())
    return bin_dir in os.getenv("PATH", "")


def _install_shortcuts():
    """Install identity shortcuts in ~/.local/bin."""
    bin_dir = _get_bin_dir()
    bin_dir.mkdir(parents=True, exist_ok=True)

    identities = _list_agent_identities()
    if not identities:
        return

    for identity in identities:
        script_path = bin_dir / identity
        script_content = f'#!/usr/bin/env bash\nexec spawn {identity} "$@"\n'
        script_path.write_text(script_content)
        script_path.chmod(0o755)

    typer.echo(f"✓ Installed {len(identities)} identity shortcuts")
    if not _is_bin_in_path():
        typer.echo('⚠ Add to PATH: export PATH="$HOME/.local/bin:$PATH"')


@app.command()
def init():
    """Initialize space workspace structure and databases."""
    root = paths.space_root()

    paths.space_data().mkdir(parents=True, exist_ok=True)
    paths.canon_path().mkdir(parents=True, exist_ok=True)
    constitutions_dir = paths.canon_path() / "constitutions"
    constitutions_dir.mkdir(parents=True, exist_ok=True)
    (root / "projects").mkdir(parents=True, exist_ok=True)

    chats_dir = paths.chats_dir()
    chats_dir.mkdir(parents=True, exist_ok=True)
    for cli in ["claude", "codex", "gemini"]:
        (chats_dir / cli).mkdir(exist_ok=True)

    spawn.db.register()

    with spawn.db.connect():
        pass
    with store.ensure("bridge"):
        pass
    with store.ensure("memory"):
        pass
    with store.ensure("knowledge"):
        pass

    typer.echo(f"✓ Initialized workspace at {root}")
    typer.echo(f"✓ User data directory at {Path.home() / '.space'}")
    typer.echo(f"✓ Backup directory at {Path.home() / '.space_backups'}")

    archive_old_config()
    init_default_agents()

    constitutions_dir = paths.canon_path() / "constitutions"
    constitution_files = sorted(
        [f.name for f in constitutions_dir.glob("*.md") if f.name != "README.md"]
    )
    typer.echo(f"✓ {len(constitution_files)} constitutions registered")

    typer.echo("Syncing provider chats...")
    chat_results = sync.sync_provider_chats()
    for provider, (discovered, synced) in chat_results.items():
        if discovered > 0:
            typer.echo(f"  {provider}: {synced}/{discovered} synced")

    _install_shortcuts()

    typer.echo()
    typer.echo("Created space structure:")
    typer.echo("  ~/space/")
    typer.echo("    └── canon/                  → human curated context")
    typer.echo("        └── constitutions/      → identity prompts")
    for i, const_file in enumerate(constitution_files):
        if i == len(constitution_files) - 1:
            typer.echo(f"            └── {const_file}")
        else:
            typer.echo(f"            ├── {const_file}")
    typer.echo()
    typer.echo("  ~/.space/")
    typer.echo("    ├── data/                   → runtime databases")
    typer.echo("    └── chats/                  → chat history")
    typer.echo()
    typer.echo("  ~/.space_backups/")
    typer.echo("    ├── data/                   → timestamped snapshots")
    typer.echo("    └── chats/                  → latest backup")

    typer.echo()
    typer.echo("Next steps:")
    typer.echo("  1. Create a new *.md constitution file in ~/space/canon/constitutions/")
    typer.echo("  2. Register your agent: spawn register <identity> -m <model> -c <constitution>")


def main() -> None:
    """Entry point for poetry script."""
    app()
</file>

<file path="space/lib/paths.py">
from pathlib import Path


def space_root() -> Path:
    """Returns the space root directory, ~/space."""
    return Path.home() / "space"


def dot_space() -> Path:
    """Returns the .space directory, ~/.space."""
    return Path.home() / ".space"


def space_data() -> Path:
    """Returns the data directory, ~/.space/data."""
    return dot_space() / "data"


def package_root() -> Path:
    """Returns space package root directory."""
    return Path(__file__).resolve().parent.parent


def constitution(filename: str) -> Path:
    """Returns the full path to a constitution file.

    Checks canon first (SSOT), falls back to local.
    """
    canon = canon_path() / "constitutions" / filename
    if canon.exists():
        return canon
    return package_root() / "core" / "spawn" / "constitutions" / filename


def canon_path() -> Path:
    """Returns path to human's canonical values, ~/space/canon."""
    return space_root() / "canon"


def chats_db() -> Path:
    """Returns path to unified chat history, ~/.space/data/chats.db."""
    return space_data() / "chats.db"


def chats_dir() -> Path:
    """Returns the chats directory, ~/.space/chats."""
    return dot_space() / "chats"


def backups_dir() -> Path:
    """Returns the backups directory, ~/.space_backups (read-only)."""
    return Path.home() / ".space_backups"


def backup_snapshot(timestamp: str) -> Path:
    """Returns immutable path to timestamped backup snapshot.

    Args:
        timestamp: ISO format or YYYYMMDDhhmmss format

    Returns:
        Path like ~/.space_backups/data/20251025_001530/
    """
    return backups_dir() / "data" / timestamp


def backup_chats_latest() -> Path:
    """Returns path to latest chat backup (single copy, overwrites).

    Returns:
        Path like ~/.space_backups/chats/latest/
    """
    return backups_dir() / "chats" / "latest"


def validate_backup_path(backup_path: Path) -> bool:
    """Validate backup path is within backups/ to prevent traversal."""
    try:
        backup_path.resolve().relative_to(backups_dir().resolve())
        return True
    except ValueError:
        return False
</file>

<file path="space/os/bridge/__init__.py">
from . import api, db, ops
from .api import (
    export_channel,
    get_channel,
    get_messages,
    get_sender_history,
    resolve_channel,
    search,
    set_bookmark,
    spawn_from_mentions,
    stats,
)
from .cli import app
from .ops import (
    archive_channel,
    create_channel,
    delete_channel,
    fetch_inbox,
    list_channels,
    pin_channel,
    recv_messages,
    rename_channel,
    send_message,
    unpin_channel,
    wait_for_message,
)

db.register()

__all__ = [
    "api",
    "app",
    "db",
    "ops",
    "archive_channel",
    "create_channel",
    "delete_channel",
    "export_channel",
    "fetch_inbox",
    "get_channel",
    "get_messages",
    "get_sender_history",
    "list_channels",
    "pin_channel",
    "recv_messages",
    "rename_channel",
    "resolve_channel",
    "search",
    "send_message",
    "set_bookmark",
    "spawn_from_mentions",
    "stats",
    "unpin_channel",
    "wait_for_message",
]
</file>

<file path="space/os/memory/db.py">
from space.lib import store
from space.os import db as unified_db

unified_db.register()


def register() -> None:
    """Legacy shim to maintain backward compatibility."""
    unified_db.register()


def connect():
    """Return connection to unified database via memory alias."""
    return store.ensure("memory")


def path():
    """Expose filesystem path for unified database."""
    return unified_db.path()
</file>

<file path="space/os/spawn/db.py">
from space.lib import store
from space.os import db as unified_db

unified_db.register()


def register() -> None:
    """Legacy shim to maintain backward compatibility."""
    unified_db.register()


def connect():
    """Return connection to unified database via spawn alias."""
    return store.ensure("spawn")


def path():
    """Expose filesystem path for unified database."""
    return unified_db.path()
</file>

<file path="tests/integration/test_spawn_agents.py">
from space.os import spawn


def test_register_agent(test_space):
    agent_id = spawn.register_agent("zealot", "claude-haiku-4-5", "zealot.md")
    assert agent_id is not None
    agent = spawn.get_agent(agent_id)
    assert agent is not None
    assert agent.identity == "zealot"
    assert agent.constitution == "zealot.md"
    assert agent.model == "claude-haiku-4-5"


def test_register_agent_already_exists(test_space):
    spawn.register_agent("zealot", "claude-haiku-4-5", "zealot.md")
    try:
        spawn.register_agent("zealot", "claude-haiku-4-5", "zealot.md")
        raise AssertionError("Should have raised ValueError")
    except ValueError as e:
        assert "already registered" in str(e)


def test_get_agent_by_id(test_space):
    agent_id = spawn.register_agent("zealot", "claude-haiku-4-5", "zealot.md")
    agent = spawn.get_agent(agent_id)
    assert agent.agent_id == agent_id
    assert agent.identity == "zealot"


def test_get_agent_by_identity(test_space):
    agent_id = spawn.register_agent("zealot", "claude-haiku-4-5", "zealot.md")
    agent = spawn.get_agent("zealot")
    assert agent.agent_id == agent_id
    assert agent.identity == "zealot"


def test_get_agent_not_found(test_space):
    agent = spawn.get_agent("nonexistent-agent")
    assert agent is None


def test_describe_self(test_space):
    agent_id = spawn.register_agent("zealot", "claude-haiku-4-5", "zealot.md")
    spawn.describe_self("zealot", "Custom behavior")
    agent = spawn.get_agent(agent_id)
    assert agent.description == "Custom behavior"


def test_description_update(test_space):
    agent_id = spawn.register_agent("zealot", "claude-haiku-4-5", "zealot.md")
    spawn.describe_self("zealot", "First description")
    spawn.describe_self("zealot", "Updated description")
    agent = spawn.get_agent(agent_id)
    assert agent.description == "Updated description"
</file>

<file path="tests/unit/os/spawn/test_agents.py">
from unittest.mock import MagicMock, patch

import pytest

from space.core.models import Agent
from space.os import spawn


def make_mock_row(data):
    row = MagicMock()
    row.__getitem__ = lambda self, key: data[key]
    row.keys = lambda: data.keys()
    return row


def test_get_agent_finds_by_identity(mock_db):
    mock_row = make_mock_row(
        {
            "agent_id": "a-1",
            "identity": "agent1",
            "constitution": "c.md",
            "model": "claude-haiku-4-5",
            "self_description": None,
            "archived_at": None,
            "created_at": "2024-01-01",
        }
    )
    mock_db.execute.return_value.fetchone.return_value = mock_row

    result = spawn.get_agent("agent1")
    assert result.agent_id == "a-1"
    assert result.identity == "agent1"
    assert result.constitution == "c.md"
    assert result.provider == "claude"
    assert result.model == "claude-haiku-4-5"


def test_get_agent_finds_by_id(mock_db):
    mock_row = make_mock_row(
        {
            "agent_id": "a-1",
            "identity": "agent1",
            "constitution": "c.md",
            "model": "claude-haiku-4-5",
            "self_description": None,
            "archived_at": None,
            "created_at": "2024-01-01",
        }
    )
    mock_db.execute.return_value.fetchone.return_value = mock_row

    result = spawn.get_agent("a-1")
    assert result.identity == "agent1"


def test_get_agent_missing_returns_none(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None
    result = spawn.get_agent("missing")
    assert result is None


def test_register_agent_success(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None  # Agent not found
    with patch("space.os.spawn.api.agents.uuid.uuid4") as mock_uuid:
        mock_uuid.return_value = "new-uuid"
        agent_id = spawn.register_agent("newagent", "claude-haiku-4-5", "c.md")

    assert agent_id == "new-uuid"
    # Check that the agents table was called (not the events table)
    calls = mock_db.execute.call_args_list
    agents_call = next((call for call in calls if "INSERT INTO agents" in call[0][0]), None)
    assert agents_call is not None
    assert (
        "INSERT INTO agents (agent_id, identity, constitution, model, created_at) VALUES (?, ?, ?, ?, ?)"
        in agents_call[0][0]
    )
    assert agents_call[0][1] == (
        "new-uuid",
        "newagent",
        "c.md",
        "claude-haiku-4-5",
        agents_call[0][1][4],
    )


def test_register_agent_already_exists(mock_db):
    mock_row = make_mock_row(
        {
            "agent_id": "a-1",
            "identity": "agent1",
            "constitution": "c.md",
            "model": "claude-haiku-4-5",
            "self_description": None,
            "archived_at": None,
            "created_at": "2024-01-01",
        }
    )
    mock_db.execute.return_value.fetchone.return_value = mock_row  # Agent found

    with pytest.raises(ValueError, match="Identity 'agent1' already registered"):
        spawn.register_agent("agent1", "claude-haiku-4-5", "c.md")


def test_describe_self_updates(mock_db):
    mock_agent = Agent(
        agent_id="a-1",
        identity="agent1",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    with patch("space.os.spawn.api.agents.get_agent", return_value=mock_agent):
        spawn.describe_self("agent1", "I am agent1")
        mock_db.execute.assert_called_with(
            "UPDATE agents SET self_description = ? WHERE agent_id = ?",
            ("I am agent1", "a-1"),
        )


def test_describe_self_missing_agent_raises_error(mock_db):
    with patch("space.os.spawn.api.agents.get_agent", return_value=None):
        with pytest.raises(ValueError, match="Agent 'missing' not found."):
            spawn.describe_self("missing", "description")


def test_rename_agent_updates(mock_db):
    mock_old_agent = Agent(
        agent_id="a-1",
        identity="old",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    with patch("space.os.spawn.api.agents.get_agent") as mock_get_agent:
        mock_get_agent.side_effect = [mock_old_agent, None]  # old exists, new doesn't
        result = spawn.rename_agent("old", "new")
    assert result is True
    mock_db.execute.assert_called_with(
        "UPDATE agents SET identity = ? WHERE agent_id = ?", ("new", "a-1")
    )


def test_rename_agent_missing_returns_false(mock_db):
    with patch("space.os.spawn.api.agents.get_agent", return_value=None):
        result = spawn.rename_agent("old", "new")
        assert result is False


def test_rename_agent_new_exists_returns_false(mock_db):
    mock_old_agent = Agent(
        agent_id="a-1",
        identity="old",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    mock_new_agent = Agent(
        agent_id="a-2",
        identity="new",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    with patch("space.os.spawn.api.agents.get_agent") as mock_get_agent:
        mock_get_agent.side_effect = [mock_old_agent, mock_new_agent]
        result = spawn.rename_agent("old", "new")
        assert result is False


def test_archive_agent_updates(mock_db):
    mock_agent = Agent(
        agent_id="a-1",
        identity="agent1",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    with patch("space.os.spawn.api.agents.get_agent", return_value=mock_agent):
        result = spawn.archive_agent("agent1")
        assert result is True
        mock_db.execute.assert_called_with(
            "UPDATE agents SET archived_at = ? WHERE agent_id = ?",
            (mock_db.execute.call_args[0][1][0], "a-1"),
        )


def test_archive_agent_missing_returns_false(mock_db):
    with patch("space.os.spawn.api.agents.get_agent", return_value=None):
        result = spawn.archive_agent("missing")
        assert result is False


def test_unarchive_agent_updates(mock_db):
    mock_db.execute.return_value.fetchone.return_value = make_mock_row({"agent_id": "a-1"})
    result = spawn.unarchive_agent("agent1")
    assert result is True
    mock_db.execute.assert_called_with(
        "UPDATE agents SET archived_at = NULL WHERE agent_id = ?", ("a-1",)
    )


def test_unarchive_agent_missing_returns_false(mock_db):
    mock_db.execute.return_value.fetchone.return_value = None
    result = spawn.unarchive_agent("missing")
    assert result is False


def test_list_agents_returns_list(mock_db):
    mock_row1 = make_mock_row({"identity": "agent1"})
    mock_row2 = make_mock_row({"identity": "agent2"})
    mock_db.execute.return_value.fetchall.return_value = [mock_row1, mock_row2]

    result = spawn.list_agents()
    assert len(result) == 2
    assert "agent1" in result


def test_merge_agents_updates_all_dbs(mock_db):
    mock_from = Agent(
        agent_id="a-1",
        identity="from",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    mock_to = Agent(
        agent_id="a-2",
        identity="to",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    with patch("space.os.spawn.api.agents.get_agent") as mock_get_agent:
        mock_get_agent.side_effect = [mock_from, mock_to]
        with patch("space.lib.paths.space_data") as mock_paths:
            from pathlib import Path

            mock_paths.return_value = Path("/tmp")
            result = spawn.merge_agents("from", "to")

    assert result is True


def test_register_agent_rejects_spaces(mock_db):
    with pytest.raises(ValueError, match="Identity cannot contain spaces"):
        spawn.register_agent("my agent", "claude-haiku-4-5", "c.md")


def test_register_agent_suggests_hyphen(mock_db):
    with pytest.raises(ValueError, match="Use hyphens instead: 'my-agent'"):
        spawn.register_agent("my agent", "claude-haiku-4-5", "c.md")


def test_rename_agent_rejects_spaces(mock_db):
    mock_agent = Agent(
        agent_id="a-1",
        identity="old",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    with patch("space.os.spawn.api.agents.get_agent", return_value=mock_agent):
        with pytest.raises(ValueError, match="Identity cannot contain spaces"):
            spawn.rename_agent("old", "new name")


def test_clone_agent_rejects_spaces(mock_db):
    mock_agent = Agent(
        agent_id="a-1",
        identity="original",
        constitution="c.md",
        model="claude-haiku-4-5",
        description=None,
        created_at="2024-01-01",
    )
    with patch("space.os.spawn.api.agents.get_agent", return_value=mock_agent):
        with pytest.raises(ValueError, match="Identity cannot contain spaces"):
            spawn.clone_agent("original", "new agent")
</file>

<file path="space/os/bridge/db.py">
from space.lib import store
from space.os import db as unified_db

unified_db.register()


def register() -> None:
    """Legacy shim to maintain backward compatibility."""
    unified_db.register()


def connect():
    """Return connection to unified database via bridge alias."""
    return store.ensure("bridge")


def path():
    """Expose filesystem path for unified database."""
    return unified_db.path()
</file>

<file path="space/os/spawn/api/agents.py">
"""Agent operations: CRUD, merging, caching."""

import uuid
from datetime import datetime
from functools import lru_cache

from space.core.models import Agent
from space.lib import store
from space.lib.store import from_row
from space.os.spawn import db


def _validate_identity(identity: str) -> None:
    """Validate agent identity format. Raises ValueError if invalid."""
    if not identity:
        raise ValueError("Identity cannot be empty")
    if " " in identity:
        raise ValueError(
            f"Identity cannot contain spaces. Use hyphens instead: '{identity.replace(' ', '-')}'"
        )


def _row_to_agent(row: store.Row) -> Agent:
    data = dict(row)
    if "self_description" in data:
        data["description"] = data.pop("self_description")
    return from_row(data, Agent)


@lru_cache(maxsize=256)
def _get_agent_by_name_cached(name: str) -> Agent | None:
    """Cached agent lookup by name."""
    with db.connect() as conn:
        row = conn.execute(
            "SELECT agent_id, identity, constitution, model, self_description, archived_at, created_at FROM agents WHERE identity = ? AND archived_at IS NULL LIMIT 1",
            (name,),
        ).fetchone()
        return _row_to_agent(row) if row else None


def _clear_cache():
    """Invalidate agent cache."""
    _get_agent_by_name_cached.cache_clear()


def touch_agent(agent_id: str) -> None:
    """Update last_active_at for an agent. Called after any agent operation."""
    with db.connect() as conn:
        conn.execute(
            "UPDATE agents SET last_active_at = ? WHERE agent_id = ?",
            (datetime.now().isoformat(), agent_id),
        )


def get_agent(identifier: str) -> Agent | None:
    """Resolve agent by name or ID. Returns Agent object or None."""
    with db.connect() as conn:
        row = conn.execute(
            "SELECT agent_id, identity, constitution, model, self_description, archived_at, created_at FROM agents WHERE (identity = ? OR agent_id = ?) AND archived_at IS NULL LIMIT 1",
            (identifier, identifier),
        ).fetchone()
        return _row_to_agent(row) if row else None


def register_agent(identity: str, model: str, constitution: str | None = None) -> str:
    """Explicitly register an identity. Fails if identity already exists.

    Provider is inferred from model.
    """
    _validate_identity(identity)
    agent = get_agent(identity)
    if agent:
        raise ValueError(f"Identity '{identity}' already registered")

    agent_id = str(uuid.uuid4())
    now_iso = datetime.now().isoformat()
    with db.connect() as conn:
        conn.execute(
            "INSERT INTO agents (agent_id, identity, constitution, model, created_at) VALUES (?, ?, ?, ?, ?)",
            (agent_id, identity, constitution, model, now_iso),
        )
    _clear_cache()
    touch_agent(agent_id)
    return agent_id


def ensure_agent(name: str) -> str:
    """DEPRECATED: Use register_agent or get_agent."""
    raise NotImplementedError(
        "ensure_agent is deprecated. All agents must be explicitly registered via `space init` or `spawn register`."
    )


def update_agent(
    identity: str,
    constitution: str | None = None,
    model: str | None = None,
) -> bool:
    """Update agent fields. Only specified fields are modified.

    If model is specified, provider is inferred from it.
    """

    agent = get_agent(identity)
    if not agent:
        raise ValueError(f"Agent '{identity}' not found")

    updates = []
    values = []
    if constitution is not None:
        updates.append("constitution = ?")
        values.append(constitution)
    if model is not None:
        updates.append("model = ?")
        values.append(model)

    if not updates:
        return True

    values.append(agent.agent_id)
    sql = f"UPDATE agents SET {', '.join(updates)} WHERE agent_id = ?"
    with db.connect() as conn:
        conn.execute(sql, values)
    _clear_cache()
    return True


def clone_agent(src_identity: str, dst_identity: str) -> str:
    """Clone an agent: new agent_id, copied constitution/model."""
    _validate_identity(dst_identity)
    src_agent = get_agent(src_identity)
    if not src_agent:
        raise ValueError(f"Source agent '{src_identity}' not found")

    dst_agent = get_agent(dst_identity)
    if dst_agent:
        raise ValueError(f"Target identity '{dst_identity}' already exists")

    return register_agent(dst_identity, src_agent.model, src_agent.constitution)


def describe_self(name: str, content: str) -> None:
    """Set self-description for agent."""
    agent = get_agent(name)
    if not agent:
        raise ValueError(f"Agent '{name}' not found.")

    with db.connect() as conn:
        conn.execute(
            "UPDATE agents SET self_description = ? WHERE agent_id = ?",
            (content, agent.agent_id),
        )
    _clear_cache()


def rename_agent(old_name: str, new_name: str) -> bool:
    """Rename an agent. Fails if new_name exists."""
    _validate_identity(new_name)
    old_agent = get_agent(old_name)
    if not old_agent:
        return False

    new_agent = get_agent(new_name)
    if new_agent:
        return False

    with db.connect() as conn:
        conn.execute(
            "UPDATE agents SET identity = ? WHERE agent_id = ?", (new_name, old_agent.agent_id)
        )
    _clear_cache()
    return True


def archive_agent(name: str) -> bool:
    """Archive an agent. Returns True if archived, False if not found."""
    agent = get_agent(name)
    if not agent:
        return False
    agent_id = agent.agent_id

    with db.connect() as conn:
        conn.execute(
            "UPDATE agents SET archived_at = ? WHERE agent_id = ?",
            (datetime.now().isoformat(), agent_id),
        )
    _clear_cache()
    return True


def unarchive_agent(name: str) -> bool:
    """Unarchive an agent. Returns True if unarchived, False if not found."""
    with db.connect() as conn:
        row = conn.execute(
            "SELECT agent_id FROM agents WHERE identity = ? LIMIT 1", (name,)
        ).fetchone()
        agent_id = row["agent_id"] if row else None

    if not agent_id:
        return False

    with db.connect() as conn:
        conn.execute("UPDATE agents SET archived_at = NULL WHERE agent_id = ?", (agent_id,))
    _clear_cache()
    return True


def list_agents() -> list[str]:
    """List all active agents."""
    with db.connect() as conn:
        rows = conn.execute(
            "SELECT identity FROM agents WHERE archived_at IS NULL ORDER BY identity"
        ).fetchall()
        return [row["identity"] for row in rows]


def merge_agents(from_name: str, to_name: str) -> bool:
    """Merge agent histories. Migrates all references from source to target."""
    from_agent = get_agent(from_name)
    to_agent = get_agent(to_name)

    if not from_agent or not to_agent:
        return False
    from_id = from_agent.agent_id
    to_id = to_agent.agent_id

    if from_id == to_id:
        return False

    with db.connect() as conn:
        conn.execute("UPDATE messages SET agent_id = ? WHERE agent_id = ?", (to_id, from_id))
        conn.execute("UPDATE bookmarks SET agent_id = ? WHERE agent_id = ?", (to_id, from_id))
        conn.execute("UPDATE tasks SET agent_id = ? WHERE agent_id = ?", (to_id, from_id))
        conn.execute("UPDATE sessions SET agent_id = ? WHERE agent_id = ?", (to_id, from_id))
        conn.execute("UPDATE knowledge SET agent_id = ? WHERE agent_id = ?", (to_id, from_id))
        conn.execute("UPDATE memories SET agent_id = ? WHERE agent_id = ?", (to_id, from_id))
        conn.execute("DELETE FROM agents WHERE agent_id = ?", (from_id,))

    _clear_cache()
    return True


__all__ = [
    "get_agent",
    "register_agent",
    "update_agent",
    "clone_agent",
    "ensure_agent",
    "describe_self",
    "rename_agent",
    "archive_agent",
    "unarchive_agent",
    "list_agents",
    "merge_agents",
    "touch_agent",
]
</file>

<file path="space/os/spawn/__init__.py">
from . import api, db
from .api import (
    archive_agent,
    clone_agent,
    complete_task,
    create_task,
    describe_self,
    ensure_agent,
    fail_task,
    get_agent,
    get_task,
    list_agents,
    list_tasks,
    merge_agents,
    register_agent,
    rename_agent,
    spawn_agent,
    spawn_prompt,
    start_task,
    unarchive_agent,
    update_agent,
)
from .cli import app

__all__ = [
    "api",
    "db",
    "app",
    "get_agent",
    "register_agent",
    "update_agent",
    "clone_agent",
    "ensure_agent",
    "describe_self",
    "rename_agent",
    "archive_agent",
    "unarchive_agent",
    "list_agents",
    "merge_agents",
    "spawn_prompt",
    "spawn_agent",
    "create_task",
    "get_task",
    "list_tasks",
    "start_task",
    "complete_task",
    "fail_task",
]
</file>

<file path="tests/unit/commands/test_backup.py">
import sqlite3
from unittest.mock import patch

from space.apps.backup import _get_backup_stats, backup


@patch("space.apps.backup.paths.backup_chats_latest")
@patch("space.apps.backup.paths.backup_snapshot")
@patch("space.apps.backup.paths.chats_dir")
@patch("space.apps.backup.paths.space_data")
def test_backup_creates_timestamped_data_dir(
    mock_space_data, mock_chats_dir, mock_backup_snapshot, mock_backup_chats_latest, tmp_path
):
    """Backup creates timestamped data directory."""
    src_data = tmp_path / "data"
    src_data.mkdir()
    (src_data / "test.db").write_text("")

    src_chats = tmp_path / "chats"
    src_chats.mkdir()

    backup_dir = tmp_path / "backups"

    mock_space_data.return_value = src_data
    mock_chats_dir.return_value = src_chats
    mock_backup_snapshot.side_effect = lambda ts: backup_dir / "data" / ts
    mock_backup_chats_latest.return_value = backup_dir / "chats" / "latest"

    backup(quiet_output=True)

    assert (backup_dir / "data").exists()
    data_backups = list((backup_dir / "data").glob("*"))
    assert len(data_backups) == 1
    assert data_backups[0].is_dir()
    assert (backup_dir / "chats" / "latest").exists()


@patch("space.apps.backup.paths.backup_chats_latest")
@patch("space.apps.backup.paths.backup_snapshot")
@patch("space.apps.backup.paths.chats_dir")
@patch("space.apps.backup.paths.space_data")
def test_backup_copies_db_files(
    mock_space_data, mock_chats_dir, mock_backup_snapshot, mock_backup_chats_latest, tmp_path
):
    """Backup copies all .db files to data snapshot."""
    src_data = tmp_path / "data"
    src_data.mkdir()

    conn = sqlite3.connect(src_data / "test.db")
    conn.execute("CREATE TABLE data (id INTEGER)")
    conn.execute("INSERT INTO data VALUES (1)")
    conn.commit()
    conn.close()

    src_chats = tmp_path / "chats"
    src_chats.mkdir()

    backup_dir = tmp_path / "backups"

    mock_space_data.return_value = src_data
    mock_chats_dir.return_value = src_chats
    mock_backup_snapshot.side_effect = lambda ts: backup_dir / "data" / ts
    mock_backup_chats_latest.return_value = backup_dir / "chats" / "latest"

    backup(quiet_output=True)

    data_backups = list((backup_dir / "data").glob("*"))
    backup_db = data_backups[0] / "test.db"
    assert backup_db.exists()


def test_backup_stats_counts_rows(tmp_path):
    """Backup stats accurately count rows."""
    backup_path = tmp_path / "backup"
    backup_path.mkdir()

    conn = sqlite3.connect(backup_path / "test.db")
    conn.execute("CREATE TABLE data (id INTEGER)")
    conn.execute("INSERT INTO data VALUES (1), (2), (3)")
    conn.commit()
    conn.close()

    stats = _get_backup_stats(backup_path)

    assert "test.db" in stats
    assert stats["test.db"]["rows"] == 3
    assert stats["test.db"]["tables"] == 1


@patch("space.apps.backup.paths.validate_backup_path")
@patch("space.apps.backup.paths.backup_chats_latest")
@patch("space.apps.backup.paths.chats_dir")
def test_backup_chats_append_only(
    mock_chats_dir, mock_backup_chats_latest, mock_validate, tmp_path
):
    """Chat backup is append-only: new files added, updated files copied, old files retained."""
    from space.apps.backup import _backup_chats_latest

    src_chats = tmp_path / "chats"
    src_chats.mkdir()
    (src_chats / "claude").mkdir()
    (src_chats / "codex").mkdir()

    backup_dir = tmp_path / "backup"

    mock_chats_dir.return_value = src_chats
    mock_backup_chats_latest.return_value = backup_dir
    mock_validate.return_value = True

    (src_chats / "claude" / "session1.jsonl").write_text("msg1")
    (src_chats / "codex" / "session2.jsonl").write_text("msg2")

    _backup_chats_latest(quiet_output=True)

    assert (backup_dir / "claude" / "session1.jsonl").read_text() == "msg1"
    assert (backup_dir / "codex" / "session2.jsonl").read_text() == "msg2"

    (src_chats / "claude" / "session3.jsonl").write_text("msg3")
    (src_chats / "claude" / "session1.jsonl").write_text("msg1-updated")

    _backup_chats_latest(quiet_output=True)

    assert (backup_dir / "claude" / "session1.jsonl").read_text() == "msg1-updated"
    assert (backup_dir / "claude" / "session3.jsonl").read_text() == "msg3"
    assert (backup_dir / "codex" / "session2.jsonl").read_text() == "msg2"
</file>

<file path="space/core/models.py">
"""Shared data models and types."""

from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum


class TaskStatus(str, Enum):
    """Valid task statuses."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    TIMEOUT = "timeout"


@dataclass
class Message:
    """A coordination message in the bridge."""

    message_id: str
    channel_id: str
    agent_id: str
    content: str
    created_at: str


@dataclass
class Channel:
    """A coordination channel."""

    channel_id: str
    name: str
    topic: str | None = None
    created_at: str | None = None
    archived_at: str | None = None
    members: list[str] = field(default_factory=list)
    message_count: int = 0
    last_activity: str | None = None
    unread_count: int = 0


@dataclass
class Bookmark:
    """Agent's bookmark for a channel."""

    agent_id: str
    channel_id: str
    last_seen_id: str | None = None


@dataclass
class Export:
    """Complete channel export for research."""

    channel_id: str
    channel_name: str
    topic: str | None
    created_at: str | None
    members: list[str]
    message_count: int
    messages: list[Message]


@dataclass
class Memory:
    memory_id: str
    agent_id: str
    topic: str
    message: str
    timestamp: str
    created_at: str
    archived_at: str | None = None
    core: bool = False
    source: str = "manual"
    bridge_channel: str | None = None
    code_anchors: str | None = None
    synthesis_note: str | None = None
    supersedes: str | None = None
    superseded_by: str | None = None


@dataclass
class ChatMessage:
    """A message from CLI chat history (distinct from bridge Message)."""

    id: int
    cli: str
    model: str | None
    session_id: str
    timestamp: str
    identity: str | None
    role: str
    text: str


@dataclass
class Knowledge:
    """A knowledge artifact."""

    knowledge_id: str
    domain: str
    agent_id: str
    content: str
    confidence: float | None
    created_at: str
    archived_at: str | None = None


@dataclass
class Agent:
    """An agent in the spawn registry."""

    agent_id: str
    identity: str
    model: str
    constitution: str | None = None
    description: str | None = None
    archived_at: str | None = None
    created_at: str | None = None
    last_active_at: str | None = None

    @property
    def provider(self) -> str:
        """Infer provider from model string."""
        model_lower = self.model.lower()
        if model_lower.startswith("gpt-"):
            return "codex"
        if model_lower.startswith("gemini-"):
            return "gemini"
        if model_lower.startswith("claude-"):
            return "claude"
        raise ValueError(f"Unknown provider for model: {self.model}")


@dataclass
class Task:
    """A task spawned by an agent."""

    task_id: str
    agent_id: str
    input: str
    status: TaskStatus | str = TaskStatus.PENDING
    channel_id: str | None = None
    output: str | None = None
    stderr: str | None = None
    pid: int | None = None
    started_at: str | None = None
    completed_at: str | None = None
    created_at: str | None = None
    duration: float | None = None

    def __post_init__(self):
        """Calculate duration from timestamps if not provided."""
        if self.duration is None and self.started_at and self.completed_at:
            try:
                start = datetime.fromisoformat(self.started_at)
                end = datetime.fromisoformat(self.completed_at)
                self.duration = (end - start).total_seconds()
            except (ValueError, TypeError) as e:
                import logging

                logger = logging.getLogger(__name__)
                logger.warning(
                    f"Could not calculate task duration due to malformed timestamps: {e}"
                )
</file>

<file path="space/lib/providers/codex.py">
"""Codex provider: chat discovery + message parsing + spawning."""

import json
import logging
import subprocess
from pathlib import Path

from space.core.protocols import Provider

logger = logging.getLogger(__name__)


class Codex(Provider):
    """Codex provider: chat discovery + message parsing + spawning.

    Codex supports two models: gpt-5-codex (optimized for coding) and gpt-5 (general).
    Reasoning effort defaults to low and is configured via codex config, not CLI args.
    """

    def __init__(self):
        self.sessions_dir = Path.home() / ".codex" / "sessions"

    @staticmethod
    def launch_args() -> list[str]:
        """Return launch arguments for Codex."""
        return ["--dangerously-bypass-approvals-and-sandbox"]

    def discover_sessions(self) -> list[dict]:
        """Discover Codex chat sessions."""
        sessions = []
        if not self.sessions_dir.exists():
            return sessions

        for jsonl in self.sessions_dir.rglob("*.jsonl"):
            sessions.append(
                {
                    "cli": "codex",
                    "session_id": jsonl.stem,
                    "file_path": str(jsonl),
                    "created_at": jsonl.stat().st_ctime,
                }
            )
        return sessions

    def parse_messages(self, file_path: Path, from_offset: int = 0) -> list[dict]:
        """Parse messages from Codex JSONL."""
        messages = []
        try:
            with open(file_path, "rb") as f:
                f.seek(from_offset)
                for line in f:
                    if not line.strip():
                        continue
                    offset = f.tell() - len(line)
                    data = json.loads(line)
                    msg_type = data.get("type")
                    payload = data.get("payload", {})

                    if msg_type == "response_item":
                        role = payload.get("role")
                        if role not in ("user", "assistant"):
                            continue

                        content_list = payload.get("content", [])
                        content = ""
                        if isinstance(content_list, list):
                            content = "\n".join(
                                item.get("text", "")
                                for item in content_list
                                if isinstance(item, dict) and item.get("type") == "input_text"
                            )

                        else:
                            content = content_list

                        msg = {
                            "message_id": payload.get("id"),
                            "role": role,
                            "content": content,
                            "timestamp": data.get("timestamp"),
                            "cwd": payload.get("cwd"),
                            "byte_offset": offset,
                        }
                        messages.append(msg)
                    elif msg_type in ("tool_call", "tool_result"):
                        content_list = payload.get("content", [])
                        content = ""
                        if isinstance(content_list, list):
                            content = "\n".join(
                                item.get("text", "")
                                for item in content_list
                                if isinstance(item, dict) and item.get("type") == "input_text"
                            )
                        else:
                            content = content_list

                        msg = {
                            "message_id": payload.get("id"),
                            "role": "tool",
                            "content": content,
                            "timestamp": data.get("timestamp"),
                            "cwd": payload.get("cwd"),
                            "tool_type": msg_type,
                            "byte_offset": offset,
                        }
                        messages.append(msg)
        except (OSError, json.JSONDecodeError) as e:
            logger.error(f"Error parsing Codex messages from {file_path}: {e}")
        return messages

    def spawn(self, identity: str, task: str | None = None) -> str:
        """Spawn Codex agent."""
        if task:
            result = subprocess.run(
                [
                    "codex",
                    "exec",
                    task,
                    "--full-auto",
                    "--skip-git-repo-check",
                ],
                capture_output=True,
                text=True,
            )
            return result.stdout

        from space.os.spawn.api import spawn_agent

        spawn_agent(identity)
        return ""

    def ping(self, identity: str) -> bool:
        """Check if Codex agent is alive."""
        try:
            from space.os.spawn import api as spawn_api

            return spawn_api.get_agent(identity) is not None
        except Exception as e:
            logger.error(f"Error pinging Codex agent {identity}: {e}")
            return False

    def list_agents(self) -> list[str]:
        """List all active agents."""
        try:
            from space.os.spawn import api as spawn_api

            return spawn_api.list_agents()
        except Exception as e:
            logger.error(f"Error listing Codex agents: {e}")
            return []
</file>

<file path="tests/integration/test_memory_integration.py">
from space.os import memory, spawn
from tests.conftest import dump_agents_table  # Import the new function


def test_memory_replace_single(test_space, default_agents):
    identity = default_agents["zealot"]
    agent_id = spawn.get_agent(identity).agent_id
    dump_agents_table()  # Call to dump agents table
    print(f"DEBUG: Using agent ID {agent_id} for memory operations")  # ADD THIS LINE
    memory.add_entry(agent_id, "insight", "initial thought")

    entries = memory.list_entries(identity, topic="insight")
    old_id = entries[0].memory_id

    new_uuid = memory.replace_entry(
        [old_id], agent_id, "insight", "refined thought", "improved clarity"
    )
    assert new_uuid is not None

    active = memory.list_entries(identity, show_all=False)
    active_insights = [e for e in active if e.topic == "insight"]
    assert len(active_insights) == 1
    assert active_insights[0].message == "refined thought"

    archived_memories = memory.list_entries(identity, show_all=True)
    archived_entries = [e for e in archived_memories if e.archived_at is not None]
    assert len(archived_entries) == 1
    assert archived_entries[0].message == "initial thought"


def test_replace_merge(test_space, default_agents):
    identity = default_agents["sentinel"]
    agent_id = spawn.get_agent(identity).agent_id
    print(f"AGENT ID in test_replace_merge: {agent_id}")
    dump_agents_table()  # Call to dump agents table
    print(f"DEBUG: Using agent ID {agent_id} for memory operations")  # ADD THIS LINE
    memory.add_entry(agent_id, "idea", "thought one")
    memory.add_entry(agent_id, "idea", "thought two")
    memory.add_entry(agent_id, "idea", "thought three")

    entries = memory.list_entries(identity, topic="idea")
    ids = [e.memory_id for e in entries]

    new_uuid = memory.replace_entry(
        ids, agent_id, "idea", "unified insight", "merged redundant thoughts"
    )
    assert new_uuid is not None
    active_memories = memory.list_entries(identity, show_all=False)
    active_ideas = [e for e in active_memories if e.topic == "idea"]
    assert len(active_ideas) == 1
    assert active_ideas[0].message == "unified insight"

    archived = memory.list_entries(identity, show_all=True)
    archived_entries = [e for e in archived if e.archived_at is not None]
    assert len(archived_entries) == 3


def test_memory_chain_query(test_space, default_agents):
    identity = default_agents["crucible"]
    agent_id = spawn.get_agent(identity).agent_id
    memory.add_entry(agent_id, "evolution", "version 1")

    memories = memory.list_entries(identity, topic="evolution")
    v1_id = memories[0].memory_id
    v2_id = memory.replace_entry([v1_id], agent_id, "evolution", "version 2", "iteration")

    chain = memory.get_chain(v2_id)
    assert chain["start_entry"] is not None
    assert len(chain["predecessors"]) == 1
    assert chain["predecessors"][0].message == "version 1"
    assert chain["start_entry"].message == "version 2"
    assert chain["start_entry"].synthesis_note == "iteration"


def test_memory_lineage_upward_traversal(test_space, default_agents):
    identity = default_agents["zealot"]
    agent_id = spawn.get_agent(identity).agent_id

    v1_id = memory.add_entry(agent_id, "thought", "version 1")
    v2_id = memory.replace_entry([v1_id], agent_id, "thought", "version 2")
    v3_id = memory.replace_entry([v2_id], agent_id, "thought", "version 3")

    chain = memory.get_chain(v3_id)
    assert chain["start_entry"].message == "version 3"
    assert len(chain["predecessors"]) == 2
    preds = {p.message for p in chain["predecessors"]}
    assert "version 1" in preds
    assert "version 2" in preds


def test_lineage_downward(test_space, default_agents):
    identity = default_agents["sentinel"]
    agent_id = spawn.get_agent(identity).agent_id

    v1_id = memory.add_entry(agent_id, "idea", "original")
    memory.replace_entry([v1_id], agent_id, "idea", "evolved")

    chain = memory.get_chain(v1_id)
    assert chain["start_entry"].message == "original"
    assert len(chain["successors"]) == 1
    assert chain["successors"][0].message == "evolved"


def test_lineage_merge(test_space, default_agents):
    identity = default_agents["crucible"]
    agent_id = spawn.get_agent(identity).agent_id

    id_a = memory.add_entry(agent_id, "notes", "idea A")
    id_b = memory.add_entry(agent_id, "notes", "idea B")
    id_c = memory.add_entry(agent_id, "notes", "idea C")
    merged_id = memory.replace_entry(
        [id_a, id_b, id_c], agent_id, "notes", "unified synthesis", "merged three threads"
    )

    chain = memory.get_chain(merged_id)
    assert chain["start_entry"].message == "unified synthesis"
    assert len(chain["predecessors"]) == 3
    pred_msgs = {p.message for p in chain["predecessors"]}
    assert pred_msgs == {"idea A", "idea B", "idea C"}

    chain_a = memory.get_chain(id_a)
    assert len(chain_a["successors"]) == 1
    assert chain_a["successors"][0].message == "unified synthesis"


def test_lineage_bidirectional(test_space, default_agents):
    identity = default_agents["zealot"]
    agent_id = spawn.get_agent(identity).agent_id

    v1_id = memory.add_entry(agent_id, "stream", "gen1")
    v2_id = memory.replace_entry([v1_id], agent_id, "stream", "gen2")
    v3_id = memory.replace_entry([v2_id], agent_id, "stream", "gen3")

    chain_v1 = memory.get_chain(v1_id)
    assert len(chain_v1["predecessors"]) == 0
    assert len(chain_v1["successors"]) == 2

    chain_v2 = memory.get_chain(v2_id)
    assert len(chain_v2["predecessors"]) == 1
    assert len(chain_v2["successors"]) == 1

    chain_v3 = memory.get_chain(v3_id)
    assert len(chain_v3["predecessors"]) == 2
    assert len(chain_v3["successors"]) == 0
</file>

<file path="space/lib/providers/claude.py">
"""Claude provider: chat discovery + message parsing + spawning."""

import json
import logging
import subprocess
from pathlib import Path

from space.core.protocols import Provider

logger = logging.getLogger(__name__)


class Claude(Provider):
    """Claude provider: chat discovery + message parsing + spawning."""

    def __init__(self):
        self.chats_dir = Path.home() / ".claude" / "projects"

    @staticmethod
    def allowed_tools() -> list[str]:
        """Return allowed tools for Claude."""
        return [
            "Bash",
            "Read",
            "Write",
            "Edit",
            "MultiEdit",
            "Grep",
            "Glob",
            "WebSearch",
            "WebFetch",
            "LS",
        ]

    @staticmethod
    def launch_args() -> list[str]:
        """Return launch arguments for Claude."""
        disallowed = [
            "NotebookRead",
            "NotebookEdit",
            "Task",
            "TodoWrite",
        ]
        return [
            "--dangerously-skip-permissions",
            "--disallowedTools",
            " ".join(disallowed),
        ]

    def discover_sessions(self) -> list[dict]:
        """Discover Claude chat sessions."""
        sessions = []
        if not self.chats_dir.exists():
            return sessions

        for jsonl in self.chats_dir.rglob("*.jsonl"):
            sessions.append(
                {
                    "cli": "claude",
                    "session_id": jsonl.stem,
                    "file_path": str(jsonl),
                    "created_at": jsonl.stat().st_ctime,
                }
            )
        return sessions

    def parse_messages(self, file_path: Path, from_offset: int = 0) -> list[dict]:
        """Parse messages from Claude JSONL."""
        messages = []
        try:
            with open(file_path, "rb") as f:
                f.seek(from_offset)
                for line in f:
                    if not line.strip():
                        continue
                    offset = f.tell() - len(line)
                    data = json.loads(line)
                    role = data.get("type")
                    if role not in ("user", "assistant", "tool"):
                        continue
                    message_obj = data.get("message", {})
                    content = ""
                    if isinstance(message_obj, dict):
                        content_raw = message_obj.get("content", "")
                        if isinstance(content_raw, list):
                            content = "\n".join(
                                item.get("text", "")
                                for item in content_raw
                                if isinstance(item, dict) and item.get("type") == "text"
                            )
                        else:
                            content = content_raw
                    else:
                        content = message_obj

                    msg = {
                        "message_id": data.get("uuid"),
                        "role": role,
                        "content": content,
                        "timestamp": data.get("timestamp"),
                        "cwd": data.get("cwd"),
                        "byte_offset": offset,
                    }
                    if role == "tool":
                        msg["tool_type"] = "tool_result"
                    messages.append(msg)
        except (OSError, json.JSONDecodeError) as e:
            logger.error(f"Error parsing Claude messages from {file_path}: {e}")
        return messages

    def spawn(self, identity: str, task: str | None = None) -> str:
        """Spawn Claude agent."""
        if task:
            result = subprocess.run(
                ["claude", "-p", task] + self.launch_args(),
                capture_output=True,
                text=True,
            )
            return result.stdout

        from space.os.spawn.api import spawn_agent

        spawn_agent(identity)
        return ""

    def ping(self, identity: str) -> bool:
        """Check if Claude agent is alive."""
        try:
            from space.os.spawn import api as spawn_api

            return spawn_api.get_agent(identity) is not None
        except Exception as e:
            logger.error(f"Error pinging Claude agent {identity}: {e}")
            return False

    def list_agents(self) -> list[str]:
        """List all active agents."""
        try:
            from space.os.spawn import api as spawn_api

            return spawn_api.list_agents()
        except Exception as e:
            logger.error(f"Error listing Claude agents: {e}")
            return []
</file>

<file path="space/lib/providers/gemini.py">
"""Gemini provider: chat discovery + message parsing + spawning."""

import json
import logging
import subprocess
from pathlib import Path

from space.core.protocols import Provider

logger = logging.getLogger(__name__)


class Gemini(Provider):
    """Gemini provider: chat discovery + message parsing + spawning."""

    def __init__(self):
        self.tmp_dir = Path.home() / ".gemini" / "tmp"

    @staticmethod
    def allowed_tools() -> list[str]:
        """Return allowed tools for Gemini."""
        return [
            "Edit",
            "FindFiles",
            "GoogleSearch",
            "ReadFile",
            "ReadFolder",
            "ReadManyFiles",
            "SearchText",
            "Shell",
            "WebFetch",
            "WriteFile",
        ]

    @staticmethod
    def launch_args(has_prompt: bool = False) -> list[str]:
        """Return launch arguments for Gemini."""
        allowed = Gemini.allowed_tools()
        args = ["--allowed-tools"] + allowed
        if has_prompt:
            args.append("--prompt-interactive")
        return args

    def discover_sessions(self) -> list[dict]:
        """Discover Gemini chat sessions from actual chat files and logs.json index."""
        sessions = []
        if not self.tmp_dir.exists():
            return sessions

        for project_dir in self.tmp_dir.iterdir():
            if not project_dir.is_dir():
                continue

            project_hash = project_dir.name
            chats_dir = project_dir / "chats"
            logs_file = project_dir / "logs.json"

            # Index sessions from logs.json (fast, metadata only)
            session_metadata = {}
            if logs_file.exists():
                try:
                    with open(logs_file) as f:
                        logs = json.load(f)
                    # Dedupe by sessionId (logs.json has one entry per message)
                    for entry in logs:
                        sid = entry.get("sessionId")
                        if sid and sid not in session_metadata:
                            session_metadata[sid] = {
                                "timestamp": entry.get("timestamp"),
                                "first_message": entry.get("message", ""),
                            }
                except (OSError, json.JSONDecodeError, MemoryError) as e:
                    logger.error(f"Error processing logs.json for project {project_hash}: {e}")

            # Discover actual chat files (ground truth)
            if chats_dir.exists():
                for chat_file in chats_dir.glob("session-*.json"):
                    file_size = chat_file.stat().st_size

                    try:
                        with open(chat_file) as f:
                            chat_data = json.load(f)
                        session_id = chat_data.get("sessionId")
                        if not session_id:
                            continue

                        sessions.append(
                            {
                                "cli": "gemini",
                                "session_id": session_id,
                                "file_path": str(chat_file),
                                "project_hash": project_hash,
                                "created_at": chat_file.stat().st_ctime,
                                "start_time": chat_data.get("startTime"),
                                "last_updated": chat_data.get("lastUpdated"),
                                "message_count": len(chat_data.get("messages", [])),
                                "file_size": file_size,
                                "first_message": session_metadata.get(session_id, {}).get(
                                    "first_message", ""
                                ),
                            }
                        )
                    except (OSError, json.JSONDecodeError, MemoryError) as e:
                        logger.error(f"Error parsing Gemini chat file {chat_file}: {e}")
                        continue

        return sessions

    def parse_messages(self, file_path: Path, from_offset: int = 0) -> list[dict]:
        """
        Parse messages from Gemini JSON or JSONL.

        Handles both:
        - Raw JSON format from provider (single JSON object with messages array)
        - JSONL format from vault (one JSON object per line)
        from_offset is byte offset (like Claude/Codex).
        """
        messages = []
        try:
            with open(file_path, "rb") as f:
                f.seek(from_offset)
                content = f.read().decode("utf-8")

            if content.strip().startswith("[") or content.strip().startswith("{"):
                try:
                    data = json.loads(content)
                    if isinstance(data, dict) and "messages" in data:
                        for msg in data.get("messages", []):
                            role = msg.get("role")
                            if role not in ("user", "model"):
                                continue
                            messages.append(
                                {
                                    "message_id": None,
                                    "role": "assistant" if role == "model" else "user",
                                    "content": msg.get("content", ""),
                                    "timestamp": msg.get("timestamp"),
                                    "byte_offset": 0,
                                }
                            )
                        return messages
                except (json.JSONDecodeError, ValueError) as e:
                    logger.error(f"Error parsing Gemini JSON content from {file_path}: {e}")

            with open(file_path, "rb") as f:
                f.seek(from_offset)
                for line in f:
                    if not line.strip():
                        continue
                    offset = f.tell() - len(line)
                    data = json.loads(line)
                    role = data.get("role")
                    if role not in ("user", "assistant"):
                        continue

                    messages.append(
                        {
                            "message_id": None,
                            "role": role,
                            "content": data.get("content", ""),
                            "timestamp": data.get("timestamp"),
                            "byte_offset": offset,
                        }
                    )
        except (OSError, json.JSONDecodeError) as e:
            logger.error(f"Error parsing Gemini messages from {file_path}: {e}")
        return messages

    def spawn(self, identity: str, task: str | None = None) -> str:
        """Spawn Gemini agent."""
        if task:
            result = subprocess.run(
                [
                    "gemini",
                    "-p",
                    task,
                    "--yolo",
                ],
                capture_output=True,
                text=True,
            )
            return result.stdout

        from space.os.spawn.api import spawn_agent

        spawn_agent(identity)
        return ""

    def ping(self, identity: str) -> bool:
        """Check if Gemini agent is alive."""
        try:
            from space.os.spawn import api as spawn_api

            return spawn_api.get_agent(identity) is not None
        except Exception as e:
            logger.error(f"Error pinging Gemini agent {identity}: {e}")
            return False

    def list_agents(self) -> list[str]:
        """List all active agents."""
        try:
            from space.os.spawn import api as spawn_api

            return spawn_api.list_agents()
        except Exception as e:
            logger.error(f"Error listing Gemini agents: {e}")
            return []
</file>

<file path="space/lib/display.py">
import json
from datetime import datetime

import typer

from space.lib.format import format_duration
from space.os import memory


def _safe_datetime(value):
    """Best-effort conversion to datetime from iso strings or epoch seconds."""
    if value is None:
        return None
    if isinstance(value, (int, float)):
        return datetime.fromtimestamp(value)
    if isinstance(value, str):
        try:
            return datetime.fromisoformat(value)
        except ValueError:
            if value.endswith("Z"):
                try:
                    return datetime.fromisoformat(value[:-1] + "+00:00")
                except ValueError:
                    return None
    return None


def fmt_entry_header(entry, agent_identity: str = None) -> str:
    mark_archived = " [ARCHIVED]" if entry.archived_at else ""
    mark_core = " ★" if entry.core else ""
    name = agent_identity or ""
    if name:
        name = f" by {name}"
    return f"[{entry.memory_id[-8:]}] {entry.topic}{name}{mark_archived}{mark_core}"


def fmt_entry_msg(msg: str, max_len: int = 100) -> str:
    if len(msg) > max_len:
        return msg[:max_len] + "..."
    return msg


def show_memory_entry(entry, ctx_obj, related=None):
    from space.os import spawn

    agent = spawn.get_agent(entry.agent_id)
    agent_identity = agent.identity if agent else None
    typer.echo(fmt_entry_header(entry, agent_identity))
    typer.echo(f"Created: {entry.timestamp}\n")
    typer.echo(f"{entry.message}\n")

    if related:
        typer.echo("─" * 60)
        typer.echo(f"Related nodes ({len(related)}):\n")
        for rel_entry, overlap in related:
            typer.echo(fmt_entry_header(rel_entry))
            typer.echo(f"  {overlap} keywords")
            typer.echo(f"  {fmt_entry_msg(rel_entry.message)}\n")


def display_context(timeline, current_state, lattice_docs, canon_docs):
    if timeline:
        typer.echo("## EVOLUTION (last 10)\n")
        for entry in timeline:
            ts_dt = _safe_datetime(entry.get("timestamp"))
            ts = ts_dt.strftime("%Y-%m-%d %H:%M") if ts_dt else str(entry.get("timestamp"))
            typ = entry["type"]
            identity_str = entry["identity"] or "system"
            data = entry["data"][:100] if entry["data"] else ""
            typer.echo(f"[{ts}] {typ} ({identity_str})")
            typer.echo(f"  {data}\n")

    if current_state["memory"] or current_state["knowledge"] or current_state["bridge"]:
        typer.echo("\n## CURRENT STATE\n")

        if current_state["memory"]:
            typer.echo(f"memory: {len(current_state['memory'])}")
            for r in current_state["memory"][:5]:
                typer.echo(f"  {r['topic']}: {r['message'][:80]}")
            typer.echo()

        if current_state["knowledge"]:
            typer.echo(f"knowledge: {len(current_state['knowledge'])}")
            for r in current_state["knowledge"][:5]:
                typer.echo(f"  {r['domain']}: {r['content'][:80]}")
            typer.echo()

        if current_state["bridge"]:
            typer.echo(f"bridge: {len(current_state['bridge'])}")
            for r in current_state["bridge"][:5]:
                typer.echo(f"  [{r['channel']}] {r['content'][:80]}")
            typer.echo()

    if lattice_docs:
        typer.echo("\n## LATTICE DOCS\n")
        for heading, content in lattice_docs.items():
            typer.echo(f"### {heading}\n")
            preview = "\n".join(content.split("\n")[:5])
            typer.echo(f"{preview}...\n")

    if canon_docs:
        typer.echo("\n## CANON DOCS\n")
        for filename, content in canon_docs.items():
            typer.echo(f"### {filename}\n")
            preview = "\n".join(content.split("\n")[:5])
            typer.echo(f"{preview}...\n")


def show_context(identity: str):
    from space.os import knowledge, spawn

    agent = spawn.get_agent(identity)
    if not agent:
        typer.echo(f"\nNo agent found for identity: {identity}")
        return
    agent_id = agent.agent_id

    knowledge_entries = knowledge.query_by_agent(agent_id)
    if knowledge_entries:
        domains = {e.domain for e in knowledge_entries}
        typer.echo(
            f"\nKNOWLEDGE: {len(knowledge_entries)} entries across {', '.join(sorted(domains))}"
        )


def show_wake_summary(identity: str, quiet_output: bool, spawn_count: int):
    from space.os import bridge, spawn

    agent = spawn.get_agent(identity)
    self_desc = agent.description if agent else None
    typer.echo(f"⚡ You are {identity}.")
    if self_desc:
        typer.echo(f"Self: {self_desc}")
    typer.echo()

    agent_id = agent.agent_id if agent else None

    if agent_id:
        last_journal = memory.list_entries(identity, topic="journal", limit=1)

        typer.echo(f"🔄 Spawn #{spawn_count}")
        if last_journal:
            last_sleep_ts = last_journal[0].created_at
            last_sleep_dt = _safe_datetime(last_sleep_ts)
            if last_sleep_dt:
                last_sleep_duration = format_duration(
                    (datetime.now() - last_sleep_dt).total_seconds()
                )
                typer.echo(f"Last session {last_sleep_duration} ago")

        journals = memory.list_entries(identity, topic="journal")
        if journals:
            typer.echo("📝 Last session:")
            typer.echo(f"  {journals[-1].message}")
            if len(journals) > 1:
                typer.echo()
                typer.echo("Previous sessions:")
                for s in reversed(journals[-3:-1]):
                    typer.echo(f"  [{s.timestamp}] {s.message}")
            typer.echo()

        core_entries = memory.list_entries(identity, filter="core")
        if core_entries:
            typer.echo("CORE MEMORIES:")
            for e in core_entries[:5]:
                typer.echo(f"  [{e.memory_id[-8:]}] {e.message}")
            typer.echo()

        recent = memory.list_entries(identity, filter="recent:7", limit=30)
        non_journal = [e for e in recent if e.topic != "journal" and not e.core][:3]
        if non_journal:
            typer.echo("RECENT (7d):")
            for e in non_journal:
                ts_dt = _safe_datetime(e.created_at)
                ts = ts_dt.strftime("%m-%d %H:%M") if ts_dt else str(e.created_at)
                typer.echo(f"  [{ts}] {e.topic}: {e.message}")
            typer.echo()

        sent_msgs = bridge.get_sender_history(identity, limit=5)
        if sent_msgs:
            typer.echo("💬 **Last sent:**")
            channel_names = {}
            for msg in sent_msgs:
                if msg.channel_id not in channel_names:
                    channel_obj = bridge.get_channel(msg.channel_id)
                    channel_names[msg.channel_id] = channel_obj.name if channel_obj else None
                channel = channel_names[msg.channel_id]
                ts_dt = _safe_datetime(msg.created_at)
                ts = ts_dt.strftime("%m-%d %H:%M") if ts_dt else msg.created_at
                first_line = msg.content.split("\n")[0]
                preview = first_line[:50] + "..." if len(first_line) > 50 else first_line
                typer.echo(f"  [{ts}] #{channel}: {preview}")
            typer.echo()

        typer.echo(
            "📖 Read MANUAL.md for full instruction set on memory, bridge, knowledge, canon."
        )


def show_smart_memory(identity: str, json_output: bool, quiet_output: bool):
    from dataclasses import asdict

    from space.os import memory, spawn

    agent = spawn.get_agent(identity)
    self_desc = agent.description if agent else None
    journals = memory.list_entries(identity, topic="journal")
    core_entries = memory.list_entries(identity, filter="core")
    recent_entries = memory.list_entries(identity, filter="recent:7", limit=20)

    if json_output:
        payload = {
            "identity": identity,
            "description": self_desc,
            "sessions": [asdict(s) for s in journals],
            "core": [asdict(e) for e in core_entries],
            "recent": [asdict(e) for e in recent_entries],
        }
        typer.echo(json.dumps(payload, indent=2))
        return

    if quiet_output:
        return

    typer.echo(f"You are {identity}.")
    if self_desc:
        typer.echo(f"Self: {self_desc}")
    typer.echo()

    if journals:
        typer.echo("📝 Last session:")
        typer.echo(f"  {journals[-1].message}")
        if len(journals) > 1:
            typer.echo()
            typer.echo("Previous sessions:")
            for s in reversed(journals[-3:-1]):
                preview = s.message[:200] + "..." if len(s.message) > 200 else s.message
                typer.echo(f"  [{s.timestamp}] {preview}")
        typer.echo()

    if core_entries:
        typer.echo("CORE MEMORIES:")
        for e in core_entries:
            lines = [line.strip() for line in e.message.split("\n") if line.strip()]
            first = lines[0][:120] if lines else ""
            second = lines[1][:120] if len(lines) > 1 else ""
            if second:
                typer.echo(f"[{e.uuid[-8:]}] {first}")
                typer.echo(f"  {second}")
            else:
                typer.echo(f"[{e.uuid[-8:]}] {first}")
        typer.echo()

    if recent_entries:
        typer.echo("RECENT (7d):")
        current_topic = None
        for e in recent_entries:
            if e.core or e.topic == "journal":
                continue
            if e.topic != current_topic:
                if current_topic is not None:
                    typer.echo()
                typer.echo(f"# {e.topic}")
                current_topic = e.topic
            lines = [line.strip() for line in e.message.split("\n") if line.strip()]
            first = lines[0][:120] if lines else ""
            second = lines[1][:120] if len(lines) > 1 else ""
            if second:
                typer.echo(f"[{e.uuid[-8:]}] [{e.timestamp}] {first}")
                typer.echo(f"  {second}")
            else:
                typer.echo(f"[{e.uuid[-8:]}] [{e.timestamp}] {first}")
        typer.echo()

    show_context(identity)
</file>

<file path="pyproject.toml">
[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.poetry]
name = "space-os"
version = "0.1.0"
description = "Infrastructure primitives enabling autonomous agent coordination with constitutional identity"
authors = ["Tyson Chan <tyson.chan@proton.me>"]
readme = "README.md"
packages = [
    { include = "space" }
]
include = [
    "space/os/spawn/constitutions/*.md"
]

[tool.poetry.dependencies]
python = "^3.12"
typer = "^0.17.4"
pyyaml = "^6.0"
prompt-toolkit = "^3.0.47"

[tool.poetry.scripts]
bridge = "space.os.bridge.cli:app"
spawn = "space.os.spawn.cli:app"
memory = "space.os.memory.cli:app"
knowledge = "space.os.knowledge.cli:app"
context = "space.apps.context.cli:app"
space = "space.cli:main"
sleep = "space.os.spawn.cli.sleep:main"

[tool.poetry.group.dev.dependencies]
ruff = "^0.13.0"
pytest = "^8.4.1"
pytest-asyncio = "^1.1.0"
pytest-timeout = "^2.4.0"
pytest-cov = "^5.0.0"
pytest-mock = "^3.12.0"

[tool.ruff]
line-length = 100
target-version = "py310"

[tool.ruff.format]
quote-style = "double"
indent-style = "space"

[tool.ruff.lint]
select = [
    "E", "W", "F", "I", "N", "UP", "B", "C4", "SIM", "RET",
]
ignore = [
    "SIM108", "SIM117", "E501", "F821",
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ["F821"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
pythonpath = ["."]
addopts = [
    "--strict-markers",
    "--strict-config",
    "-ra"
]
markers = [
    "asyncio: Async tests using pytest-asyncio",
    "unit: Unit tests (can run in parallel)",
    "integration: Integration tests (may need isolation)",
    "contract: API contract validation tests",
    "schema: Schema validation tests",
    "slow: Slow tests that should run last",
    "timeout: Test timeout marker for pytest-timeout"
]

[tool.pytest_asyncio]
asyncio_mode = "auto"
</file>

<file path="tests/conftest.py">
import contextlib
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from space.lib import paths
from space.os import bridge, spawn
from space.os import db as unified_db
from space.os.spawn import defaults as spawn_defaults


def _configure_paths(monkeypatch: pytest.MonkeyPatch, workspace: Path, test_name: str) -> None:
    data_dir = workspace / test_name / ".space" / "data"
    data_dir.mkdir(parents=True, exist_ok=True)

    db_path = data_dir / "space.db"
    if db_path.exists():
        db_path.unlink()

    monkeypatch.setattr(paths, "space_root", lambda: workspace)
    monkeypatch.setattr(paths, "dot_space", lambda: workspace / ".space")
    monkeypatch.setattr(paths, "space_data", lambda: data_dir)


@pytest.fixture
def test_space(monkeypatch, tmp_path, request):
    workspace = tmp_path / "workspace"
    workspace.mkdir()
    (workspace / "AGENTS.md").write_text("test workspace")

    data_dir = workspace / ".space" / "data"
    data_dir.mkdir(parents=True, exist_ok=True)

    db_path = data_dir / "space.db"
    if db_path.exists():
        db_path.unlink()

    monkeypatch.setattr(paths, "space_root", lambda: workspace)
    monkeypatch.setattr(paths, "dot_space", lambda: workspace / ".space")
    monkeypatch.setattr(paths, "space_data", lambda: data_dir)

    unified_db.register()
    with unified_db.connect():
        pass

    # Create dummy constitution files for default agents
    from space.os.spawn import defaults as spawn_defaults

    for identity in spawn_defaults.DEFAULT_AGENT_MODELS:
        constitution_path = workspace / f"{identity}.md"
        constitution_path.write_text(
            f"# {identity} Constitution\n\nThis is a dummy constitution for {identity}."
        )

    # Register default channels for tests

    bridge.api.channels.resolve_channel("ch-1")
    bridge.api.channels.resolve_channel("ch-spawn-test-123")
    yield workspace


@pytest.fixture
def mock_db():
    """Mock db.ensure context manager for unit tests."""
    mock_conn = MagicMock()
    with patch("space.lib.store.ensure") as mock_ensure:
        mock_ensure.return_value.__enter__.return_value = mock_conn
        mock_ensure.return_value.__exit__.return_value = None
        yield mock_conn


@pytest.fixture
def mock_resolve_channel():
    """Standardize channel resolution patching for bridge-focused tests."""

    def _resolve(identifier):
        channel_id = getattr(identifier, "channel_id", identifier)
        if channel_id is None:
            channel_id = ""
        return MagicMock(channel_id=channel_id)

    with patch("space.os.bridge.api.channels.resolve_channel") as mock:
        mock.side_effect = _resolve
        yield mock


class AgentHandle(str):
    """String-like handle exposing agent metadata for tests."""

    def __new__(cls, identity: str, agent_id: str, model: str, constitution: str | None):
        obj = str.__new__(cls, identity)
        obj.agent_id = agent_id
        obj.model = model
        obj.constitution = constitution
        return obj


def dump_agents_table():
    with unified_db.connect() as conn:
        print("DEBUG: Contents of agents table:")
        for row in conn.execute("SELECT agent_id, identity FROM agents").fetchall():
            print(f"  agent_id: {row['agent_id']}, identity: {row['identity']}")


@pytest.fixture
def default_agents(test_space):
    """Registers a set of default agents for tests and returns their identities."""

    handles: dict[str, AgentHandle] = {}
    for identity in spawn_defaults.DEFAULT_AGENT_MODELS:
        with contextlib.suppress(ValueError):
            model = spawn_defaults.canonical_model(identity)
            spawn.register_agent(identity, model, f"{identity}.md")
        agent = spawn.get_agent(identity)
        assert agent is not None
        handles[identity] = AgentHandle(
            agent.identity, agent.agent_id, agent.model, agent.constitution
        )
    return handles
</file>

<file path="space/cli.py">
import click
import typer
from typer.core import TyperGroup

from space.apps import backup, canon, chats, context, council, daemons, health, init, stats
from space.os.spawn import api


class SpawnGroup(TyperGroup):
    """Typer group that dynamically spawns tasks for agent names."""

    def get_command(self, ctx, cmd_name):
        """Get command by name, or spawn agent if not found."""
        cmd = super().get_command(ctx, cmd_name)
        if cmd is not None:
            return cmd

        agent = api.get_agent(cmd_name)
        if agent is None:
            return None

        @click.command(name=cmd_name)
        @click.argument("task_input", required=False, nargs=-1)
        def spawn_agent(task_input):
            input_list = list(task_input) if task_input else []
            api.spawn_agent(agent.identity, extra_args=input_list)

        return spawn_agent


app = typer.Typer(invoke_without_command=True, no_args_is_help=False, cls=SpawnGroup)


@app.callback(invoke_without_command=True)
def common_options_callback(
    ctx: typer.Context,
    identity: str = typer.Option(None, "--as", help="Agent identity to use."),
    json_output: bool = typer.Option(False, "--json", "-j", help="Output in JSON format."),
    quiet_output: bool = typer.Option(
        False, "--quiet", "-q", help="Suppress non-essential output."
    ),
):
    """Adds common CLI options (identity, json_output, quiet_output) to a Typer app."""
    from space.lib import output

    output.set_flags(ctx, json_output, quiet_output)

    if ctx.obj is None:
        ctx.obj = {}

    ctx.obj["identity"] = identity
    ctx.obj["json"] = json_output
    ctx.obj["quiet"] = quiet_output

    if ctx.invoked_subcommand is None:
        if identity:
            # The 'launch' module has been removed, so this functionality is disabled.
            # Please refer to the updated documentation for alternatives.
            pass
        else:
            typer.echo(
                "space-os: Agent orchestration system.\n"
                "\n"
                "Commands: space backup|health|init|canon|chats|context|council|daemons|stats"
            )


app.add_typer(init.app, name="init")
app.add_typer(backup.app, name="backup")
app.add_typer(health.app, name="health")

app.add_typer(canon.app, name="canon")
app.add_typer(chats.app, name="chats")
app.add_typer(context.app, name="context")
app.add_typer(council.app, name="council")
app.add_typer(daemons.app, name="daemons")
app.add_typer(stats.app, name="stats")


def main() -> None:
    """Entry point for poetry script."""
    try:
        app()
    except SystemExit:
        raise
    except BaseException as e:
        raise SystemExit(1) from e
</file>

</files>
