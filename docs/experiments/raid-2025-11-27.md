# Raid 2025-11-27: Autonomous 8h Test Results

**Date:** 2025-11-27 23:42-23:52  
**Duration:** 10 minutes (intended: 8 hours)  
**Outcome:** Partial success - 3 commits delivered, all spawns timed out  

---

## Executive Summary

**What worked:**
- Coordination substrate stable (no spawn cascades, no session bleed)
- 3 legitimate performance optimizations committed
- All 307 tests passing post-raid
- EXIT RULES and handoff protocol functional

**What failed:**
- All spawns hit 10min timeout (2×300s retries)
- No session linking → spawn logs unavailable for diagnosis
- 8h autonomous operation not achievable with current substrate

---

## Results

### Spawns

| Agent | Model | Status | Duration | Outcome |
|-------|-------|--------|----------|---------|
| Sentinel | gpt-5.1-codex-low | completed | 556s (9.3min) | Barely finished before timeout |
| Prime | claude-sonnet-4-5 | failed | 600s (10min) | Timeout on retry |
| Hailot | claude-haiku-4-5 | failed | 600s (10min) | Timeout on retry |
| Sonnelot | claude-sonnet-4-5 | failed | 600s (10min) | Timeout on retry |

**Total spawns:** 4 (expected: 4 initial, feared: 40+ cascade)  
**Spawn deduplication:** ✓ Working (no cascades)  
**Constitutional isolation:** ✓ Working (no capability leaks)

### Commits Delivered

**1. `9e6046e` - Explicit column selection + batch updates**
- Replaced SELECT * with explicit columns (14 queries)
- Batched agent updates (3 queries → 1)
- Module-level logging optimization

**2. `acdd538` - N+1 elimination**
- Bridge search: batch agent lookup (O(n) → O(1))
- Workspace stats: cached agent_identities
- Added 003_performance_indexes.sql migration

**3. `a912b0c` - FTS5 migration**
- Bridge message search: LIKE → FTS5 (O(n) → O(log n))
- Knowledge search: FTS5 consistency
- 004_spawn_tree_indexes.sql, 005_bridge_messages_fts.sql
- 11 files changed (+218/-89 lines)

**Performance claims:**
- Spawn tree queries: 10-100x faster (indexed parent_spawn_id)
- Message/memory search: O(log n) FTS vs O(n) LIKE scans
- Bridge search: ~10-100x on 1000+ message channels

**Quality validation:**
- CI: 307/307 tests passing
- No breaking changes
- Follows existing code patterns
- Conservative claims (no hallucinated speedups)

---

## Critical Findings

### Finding 1: Timeout is 300s × 2 retries = 600s total

**Code:** `space/os/spawn/api/launch.py:21`
```python
SPAWN_TIMEOUT = 300  # 5 minutes
max_retries = 1      # Line 77: spawn_ephemeral default
# Total: 300s attempt 1 + 300s retry = 600s
```

**Evidence:** All 3 failed spawns ended at exactly 600s  
**Impact:** Hard limit of 10min for any spawn, regardless of task complexity

**Question:** Is 300s timeout too aggressive for optimization tasks requiring codebase analysis?

---

### Finding 2: Session linking broken for first spawn in channel

**Root cause:** Chicken-and-egg problem in `resolve_session_id()`

**Flow:**
1. New channel created → no previous spawns
2. `resolve_session_id(resume=None, channel_id="raid-2025-11-27")` called
3. Line 173-180: Looks for previous spawns with `session_id` in channel
4. No previous spawns found → returns `None`
5. Spawn executes, provider creates session file
6. `_link_session(spawn, session_id=None)` → doesn't link (line 265-268)
7. Session file orphaned in provider directory

**Code location:** `space/os/sessions/api/operations.py:171-180`

```python
if not resume:
    if channel_id:
        channel_spawns = spawns.get_channel_spawns(channel_id, agent_id=agent_id, limit=50)
        for spawn in channel_spawns:
            if not spawn.session_id:
                continue
            validated = validate_session(spawn.session_id)
            if validated:
                return validated
        return None  # ← First spawn returns None here
```

**Why autodiscovery was disabled:**

Session autodiscovery via `_discover_recent_session()` was removed to fix BUG-011 (session bleed). Original issue: timestamp-based matching caused concurrent spawns to share sessions → constitutional collapse.

**Session files exist but unlinked:**

```
Sentinel: ~/.codex/sessions/2025/11/27/rollout-2025-11-27T23-42-35-*.jsonl (527 lines)
Prime:    ~/.claude/projects/-Users-teebz--space-spawns-prime/601c4fd3-*.jsonl (119 lines)
Hailot:   ~/.claude/projects/-Users-teebz--space-spawns-hailot/cf69bee9-*.jsonl (169 lines)
Sonnelot: ~/.claude/projects/-Users-teebz--space-spawns-sonnelot/c9ffc7cb-*.jsonl (153 lines)
```

**Impact:**
- Cannot use `spawn logs <spawn_id>` for debugging
- Session files not ingested to `~/.space/sessions/{provider}/`
- No spawn context available for failure analysis
- Blocks session-based continuity for channel respawns

---

### Finding 3: Failure mode was timeout, not context exhaustion

**Initial hypothesis:** Agents would hit 200k token context limit → verbose analysis → exhaustion  
**Actual:** All 3 failures hit hard 600s timeout while still actively working

**Evidence from Prime's session (last events at 12:52:34):**
- Still reading files (`Read base.py`)
- Still analyzing provider structure
- No stop_reason or error in session logs
- Process killed mid-turn by timeout

**Conclusion:** 10min insufficient for "analyze codebase, identify optimizations, implement and test" scope.

---

## Questions for Resolution

### Q1: What was the duplication issue with post-spawn session discovery?

User mentioned: "we had this before. but it was linking poorly and we were getting duplication"

**Need to understand:**
- What got duplicated? (spawns? sessions? transcripts?)
- Was it timestamp collision? (multiple spawns created at same second)
- Was it file discovery race? (multiple agents finding same session file)

**Proposed fix needs to avoid:**
- Whatever duplication issue occurred previously
- Session bleed (BUG-011) - multiple spawns sharing same session
- Timestamp-based matching that caused constitutional collapse

### Q2: Should timeout be extended or is 5min intentional?

**Current:** 300s per attempt (600s with retry)  
**Raid task:** "Analyze space-os, identify optimizations, implement and test"

**Options:**
1. Keep 300s → forces agents to work in smaller chunks
2. Extend to 600s (10min) → single attempt, no retry doubling
3. Dynamic timeout based on task scope
4. No timeout → rely on spawn staleness detection

**Trade-off:** Longer timeout = more rope for agents to hang themselves with

### Q3: How to safely enable session linking for first spawn in channel?

**Requirements:**
- Link session created during spawn window
- Avoid timestamp collision (BUG-011 root cause)
- No duplication (previous issue)
- Work for concurrent spawns in different channels

**Proposed approaches:**

**A) Post-spawn discovery with spawn_id matching:**
```python
# After spawn completes/fails:
# 1. Discover sessions created between spawn.created_at and spawn.ended_at
# 2. Match by CWD (identity dir) to ensure right session
# 3. Link if unique match found
```

**B) Provider-level session_id extraction:**
```python
# Parse provider stdout for session_id after execution
# Claude Code prints session_id in output
# Codex includes session_id in response metadata
```

**C) Explicit session_id return from spawn:**
```python
# Modify _run_ephemeral to return session_id
# Extract from provider stdout/logs before linking
```

**D) Accept NULL sessions for first spawn:**
```python
# First spawn has no session logs
# Second spawn in channel can resume/continue
# Trade-off: lose first spawn observability
```

---

## Substrate Validation

**CONFIRMED STABLE:**
- ✓ Spawn deduplication (pending+running checks, no time window needed)
- ✓ Constitutional isolation (no session sharing)
- ✓ EXIT RULES enforcement
- ✓ Handoff protocol (agents coordinated across 3 commits)
- ✓ Error containment (failures didn't cascade)

**BLOCKERS FOR 8H AUTONOMY:**
1. Session linking (can't debug failures without logs)
2. Timeout tuning (10min insufficient for optimization tasks)
3. Compact protocol (no context management for long-running operations)

---

## Next Steps

1. **Resolve session linking:**
   - Understand previous duplication issue
   - Design safe post-spawn discovery
   - Test with concurrent spawns

2. **Timeout analysis:**
   - Review if 300s is intentional constraint
   - Consider task-based timeout scaling
   - Test if removing retry (600s single attempt) helps

3. **Manual compact protocol:**
   - Design `!compact` control command
   - Session rotation on compact
   - Context budget tracking

4. **Retry raid:**
   - Fix session linking
   - Adjust timeout if needed
   - Target 30min+ coordinated work with compact support

---

## Appendix: Session File Locations

### Claude Code Sessions
Stored in project-specific directories under `~/.claude/projects/`:

```
~/.claude/projects/-Users-teebz--space-spawns-{identity}/{session_id}.jsonl
```

**Pattern:** `-Users-teebz--space-spawns-{identity}` (CWD path with slashes replaced by hyphens)

### Codex Sessions
Stored hierarchically by date under `~/.codex/sessions/`:

```
~/.codex/sessions/{year}/{month}/{day}/rollout-{timestamp}-{session_id}.jsonl
```

**Pattern:** `rollout-YYYY-MM-DDTHH-MM-SS-{uuid}.jsonl`

### Session Discovery Challenge

**Claude:** Requires CWD matching (identity dir path → project dir name mapping)  
**Codex:** Requires timestamp proximity (spawn.created_at → session file mtime)  

Both require fuzzy matching → risk of collision → why autodiscovery was disabled.
